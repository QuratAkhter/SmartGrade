Question,Response_Number,Response,Score,Answer
What is data science?,R1,"Data science is a field that combines statistics, computer science, and domain expertise to extract insights and knowledge from data. It involves various steps like data collection, cleaning, analysis, and visualization. The insights gained are used to support decision-making in businesses and research.",1.0,"Data science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from data."
What are the key steps in the data science process?,R1,"The key steps in the data science process include collecting data, cleaning it to remove errors or missing values, exploring the data through visualization, building predictive models using machine learning, and finally interpreting the results to make informed decisions. This process helps in solving complex business problems effectively.",0.95,"The key steps typically include problem definition, data collection, data preparation, exploratory data analysis, modeling, evaluation, and deployment."
What is the difference between supervised and unsupervised learning?,R1,"Supervised learning uses labeled data to train models, meaning the algorithm learns from input-output pairs. Unsupervised learning, on the other hand, works with data that has no labels and tries to find hidden patterns or groupings on its own. Both are key areas of machine learning.",1.0,"Supervised learning involves training a model on labeled data, where the algorithm learns the relationship between input features and target labels. Unsupervised learning deals with unlabeled data and aims to find hidden patterns or structures within the data."
Explain the bias-variance tradeoff.,R1,The bias-variance tradeoff is a concept in machine learning that refers to the balance between two sources of error. Bias is the error due to overly simplistic models that underfit data. Variance is the error due to models that are too complex and overfit. A good model finds a balance between both.,1.0,The bias-variance tradeoff is the balance between the error introduced by bias (underfitting) and the error introduced by variance (overfitting) when building a machine learning model.
What is feature engineering?,R1,"Feature engineering is the process of creating new input variables or modifying existing ones to improve a model’s performance. It includes techniques like encoding categorical variables, scaling features, and combining or extracting information from raw data to make it more useful for machine learning algorithms.",1.0,"Feature engineering is the process of selecting, transforming, and creating features from raw data to improve model performance."
How do you handle missing data in a dataset?,R1,"Handling missing data depends on the type and amount of missingness. Common methods include removing rows or columns with too many missing values, filling them using mean, median, or mode, and using advanced techniques like interpolation or predictive modeling for imputation.",1.0,"Missing data can be handled by imputation techniques such as mean, median, or mode imputation, using predictive models to estimate missing values, or by deleting rows or columns with missing values, depending on the dataset size and nature."
What is cross-validation?,R1,"Cross-validation is a model evaluation technique used to assess how well a model generalizes to unseen data. It involves splitting the dataset into multiple folds, training the model on some folds, and testing it on the remaining ones. This helps reduce overfitting and gives a more reliable performance estimate.",1.0,Cross-validation is a technique used to assess the performance of a predictive model by splitting the data into multiple subsets.
Explain the concept of overfitting.,R1,"Overfitting happens when a machine learning model learns the training data too well, including its noise and outliers. As a result, it performs very well on training data but poorly on unseen data. It fails to generalize because it memorized patterns that don’t apply broadly.",1.0,"Overfitting occurs when a model learns the training data too well, capturing noise or random fluctuations in the data instead of the underlying pattern."
What are some common algorithms used in supervised learning?,R1,"Some common supervised learning algorithms include linear regression, logistic regression, decision trees, random forests, support vector machines (SVM), and k-nearest neighbors (KNN). These algorithms learn from labeled data to make predictions on new, unseen data.",1.0,"Common algorithms include linear regression, logistic regression, decision trees, random forests, support vector machines, and neural networks."
What is dimensionality reduction?,R1,"Dimensionality reduction is the process of reducing the number of input features in a dataset while retaining as much information as possible. Techniques like PCA (Principal Component Analysis) and t-SNE are commonly used to simplify data, improve model performance, and reduce overfitting.",1.0,Dimensionality reduction is the process of reducing the number of input variables in a dataset by transforming them into a lower-dimensional space while preserving important information.
What is regularization in machine learning?,R1,Regularization is a technique used in machine learning to prevent overfitting by adding a penalty term to the loss function. This discourages the model from fitting the noise in the training data. Common methods include L1 (Lasso) and L2 (Ridge) regularization.,1.0,Regularization is a technique used to prevent overfitting by adding a penalty term to the model's objective function.
What is ensemble learning?,R1,"Ensemble learning is a machine learning technique that combines predictions from multiple models to produce a more accurate and robust result. Common methods include bagging, boosting, and stacking. It helps reduce errors and improves model performance.",1.0,Ensemble learning is a technique that combines multiple machine learning models to improve performance and robustness.
What is the ROC curve?,R1,"The ROC (Receiver Operating Characteristic) curve is a graphical representation of a classification model’s performance. It plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold settings. The closer the curve is to the top-left, the better the model.",1.0,The receiver operating characteristic (ROC) curve is a graphical plot that illustrates the performance of a binary classification model across different threshold settings.
What is AUC-ROC?,R1,"AUC-ROC stands for Area Under the Receiver Operating Characteristic Curve. It is a performance metric for classification models that tells how well the model distinguishes between classes. AUC close to 1 indicates a strong model, while 0.5 indicates random guessing.",1.0,The area under the ROC curve (AUC-ROC) is a metric used to evaluate the performance of a binary classification model.
What is the difference between classification and regression?,R1,"Classification and regression are types of supervised learning. Classification predicts discrete labels or categories like 'spam' or 'not spam', while regression predicts continuous values like price or temperature. Both use labeled data to train models, but their outputs differ.",1.0,"Classification is a supervised learning task where the goal is to predict categorical labels or classes, while regression is a supervised learning task where the goal is to predict continuous numerical values."
What is clustering?,R1,Clustering is an unsupervised learning technique used to group similar data points together based on certain features. The goal is to identify natural groupings in data without using labeled outputs. A common clustering algorithm is K-means.,1.0,Clustering is an unsupervised learning technique that involves grouping similar data points together based on their features or characteristics.
What is the curse of dimensionality?,R1,"The curse of dimensionality refers to the problems that arise when analyzing data in high-dimensional spaces. As dimensions increase, data points become sparse, making it harder for models to find patterns and leading to poor performance.",1.0,The curse of dimensionality refers to the phenomenon where the performance of certain algorithms deteriorates as the number of features or dimensions in the data increases.
What is the difference between precision and recall?,R1,"Precision is the ratio of correctly predicted positive observations to total predicted positives. Recall is the ratio of correctly predicted positives to all actual positives. Precision focuses on accuracy of positive predictions, while recall focuses on capturing all true positives.",1.0,"Precision measures the proportion of true positive predictions among all positive predictions made by a classifier, while recall measures the proportion of true positive predictions among all actual positive instances in the data."
What is the F1 score?,R1,The F1 score is the harmonic mean of precision and recall. It balances the two metrics and is especially useful when there is an uneven class distribution. A high F1 score indicates a good balance between precision and recall.,1.0,The F1 score is the harmonic mean of precision and recall and provides a single metric to evaluate the performance of a classifier.
What is the bias of an estimator?,R1,The bias of an estimator is the difference between the expected value of the estimator and the true value of the parameter being estimated. A high bias means the estimator consistently misses the mark in the same direction.,1.0,The bias of an estimator measures the difference between the expected value of the estimator and the true value of the parameter being estimated.
What is the variance of an estimator?,R1,"The variance of an estimator refers to how much the estimator's predictions vary when different samples from the same population are used. High variance means the estimator is sensitive to small changes in the data, leading to inconsistent predictions.",1.0,The variance of an estimator measures the spread or variability of the estimator's values around its expected value.
What is the Central Limit Theorem?,R1,"The Central Limit Theorem states that the sampling distribution of the sample mean will approximate a normal distribution as the sample size becomes large, regardless of the original population’s distribution. This is fundamental in statistical inference.",1.0,The Central Limit Theorem states that the sampling distribution of the sample mean approaches a normal distribution as the sample size increases.
What is regularization in neural networks?,R1,"Regularization in neural networks is a technique used to reduce overfitting by adding a penalty to the loss function. This prevents the model from becoming too complex and helps it generalize better on unseen data. Common methods include L1, L2, and dropout.",1.0,Regularization in neural networks involves adding penalty terms to the loss function to prevent overfitting.
What is batch normalization?,R1,"Batch normalization is a technique used to normalize the inputs of each layer in a neural network. It helps stabilize and speed up training by ensuring that the input to each layer has a consistent distribution, usually zero mean and unit variance.",1.0,Batch normalization is a technique used in neural networks to improve training stability and speed by normalizing the activations of each layer.
What is transfer learning?,R1,"Transfer learning is a machine learning technique where a model developed for one task is reused as the starting point for a model on a second task. It saves time and resources, especially when data is limited for the new task.",1.0,Transfer learning is a machine learning technique where a model trained on one task is reused as the starting point for a model on a related task.
What is natural language processing (NLP)?,R1,"Natural Language Processing (NLP) is a field of artificial intelligence that enables computers to understand, interpret, and generate human language. It includes tasks like language translation, sentiment analysis, and speech recognition.",1.0,Natural language processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human languages.
What is word embedding?,R1,"Word embedding is a technique in NLP that maps words into continuous vector space where semantically similar words are close together. Popular methods include Word2Vec, GloVe, and FastText.",1.0,Word embedding is a technique used to represent words as dense vectors in a continuous vector space.
What is sentiment analysis?,R1,"Sentiment analysis is a Natural Language Processing technique used to determine the emotional tone behind a piece of text. It classifies text as positive, negative, or neutral and is often used in social media monitoring and customer feedback analysis.",1.0,Sentiment analysis is a natural language processing task that involves determining the sentiment or opinion expressed in a piece of text.
What is deep learning?,R1,"Deep learning is a subset of machine learning that uses neural networks with many layers to learn complex patterns from large datasets. It is commonly used in image recognition, natural language processing, and autonomous systems.",1.0,Deep learning is a subset of machine learning that focuses on artificial neural networks with multiple layers.
What is a convolutional neural network (CNN)?,R1,"A convolutional neural network (CNN) is a type of deep learning model particularly effective for processing image data. It uses convolutional layers to automatically detect patterns like edges, textures, and shapes in images.",1.0,"A convolutional neural network (CNN) is a type of deep neural network that is particularly well-suited for processing structured grid-like data, such as images."
What is a recurrent neural network (RNN)?,R1,"A recurrent neural network (RNN) is a type of neural network designed to handle sequential data by using loops that allow information to persist. It’s commonly used in tasks like language modeling, speech recognition, and time series prediction.",1.0,A recurrent neural network (RNN) is a type of neural network designed to process sequential data by maintaining a state or memory of previous inputs.
What is reinforcement learning?,R1,Reinforcement learning is a type of machine learning where an agent learns to make decisions by interacting with an environment and receiving rewards or penalties. It aims to maximize cumulative reward over time.,1.0,Reinforcement learning is a machine learning paradigm where an agent learns to make decisions by interacting with an environment to maximize cumulative rewards.
What is Q-learning?,R1,Q-learning is a type of reinforcement learning algorithm where an agent learns the value of taking a certain action in a given state. It updates a Q-table to find the optimal policy for maximizing long-term rewards.,1.0,Q-learning is a model-free reinforcement learning algorithm used to learn the optimal action-selection policy for a Markov decision process (MDP).
What is the exploration-exploitation tradeoff in reinforcement learning?,R1,The exploration-exploitation tradeoff in reinforcement learning refers to the dilemma of choosing between trying new actions to discover their rewards (exploration) and using known actions that give high rewards (exploitation). Balancing both is key to learning an optimal policy.,1.0,The exploration-exploitation tradeoff refers to the dilemma faced by reinforcement learning agents when deciding whether to explore new actions or exploit known actions to maximize rewards.
What is deep reinforcement learning?,R1,"Deep reinforcement learning combines deep learning and reinforcement learning. It uses neural networks to approximate value functions or policies, allowing agents to make decisions in environments with high-dimensional inputs like images.",1.0,Deep reinforcement learning combines reinforcement learning with deep learning techniques to handle high-dimensional input spaces and learn complex policies directly from raw sensory input.
What is a Markov decision process (MDP)?,R1,"A Markov Decision Process (MDP) is a mathematical framework used in reinforcement learning to describe an environment in terms of states, actions, rewards, and transitions. It assumes the Markov property, meaning the next state depends only on the current state and action.",1.0,A Markov decision process (MDP) is a mathematical framework used to model sequential decision-making problems in which an agent interacts with an environment.
What is cross-entropy loss?,R1,Cross-entropy loss is a commonly used loss function in classification tasks. It measures the difference between the predicted probability distribution and the actual label. Lower values mean the prediction is close to the true label.,1.0,"Cross-entropy loss, also known as log loss, is a loss function used in classification tasks to quantify the difference between the predicted probability distribution and the true probability distribution of the classes."
What is the softmax function?,R1,"The softmax function is used in the output layer of a neural network for multi-class classification. It converts raw logits into probabilities that sum to 1, making it easier to interpret which class the model thinks is most likely.",1.0,The softmax function is a mathematical function that converts a vector of arbitrary real values into a vector of probabilities.
What is the difference between a generative model and a discriminative model?,R1,"A generative model learns the joint probability distribution of input features and labels, allowing it to generate new data. A discriminative model learns the decision boundary between classes by modeling the conditional probability.",1.0,"A generative model learns the joint probability distribution of the input features and the target labels, while a discriminative model learns the conditional probability distribution of the target labels given the input features."
What is autoencoder?,R1,An autoencoder is a type of neural network used for unsupervised learning. It compresses input data into a smaller representation (encoding) and then reconstructs it back to the original form (decoding). It’s often used for dimensionality reduction and anomaly detection.,1.0,An autoencoder is a type of neural network architecture that is trained to reconstruct its input data at the output layer.
What is the difference between bagging and boosting?,R1,"Bagging and boosting are ensemble learning techniques. Bagging trains multiple models independently in parallel and averages their predictions. Boosting trains models sequentially, each trying to correct the errors of the previous one.",1.0,"Bagging (Bootstrap Aggregating) is an ensemble method that builds multiple models independently and combines their predictions through averaging or voting, while boosting sequentially builds models by giving more weight to misclassified instances."
What is hyperparameter tuning?,R1,"Hyperparameter tuning is the process of finding the best set of hyperparameters for a machine learning model to improve its performance. It’s usually done using methods like grid search, random search, or Bayesian optimization.",1.0,Hyperparameter tuning is the process of finding the optimal hyperparameters for a machine learning model to improve its performance.
What is the curse of dimensionality in the context of feature selection?,R1,"The curse of dimensionality refers to problems that arise when the number of features in a dataset becomes very large. It can make models overfit, increase computational cost, and reduce model performance due to sparse data in high-dimensional space.",1.0,"The curse of dimensionality in feature selection refers to the challenges and limitations posed by high-dimensional data, such as increased computational complexity and difficulty in finding meaningful features."
What is the difference between L1 and L2 regularization?,R1,"L1 regularization adds the absolute value of the weights to the loss function, leading to sparse models by driving some weights to zero. L2 adds the squared value of the weights, penalizing large weights more smoothly without making them exactly zero.",1.0,"L1 regularization (Lasso) adds a penalty term proportional to the absolute value of the coefficients, encouraging sparsity and feature selection, while L2 regularization (Ridge) adds a penalty term proportional to the square of the coefficients, penalizing large coefficients."
What is the purpose of cross-validation in feature selection?,R1,Cross-validation in feature selection helps evaluate how well the selected features perform on unseen data. It prevents overfitting by ensuring that the feature set generalizes well to different data splits.,1.0,"Cross-validation in feature selection is used to evaluate the performance of a model with different subsets of features, helping to identify the most informative features and reduce the risk of overfitting."
What is the role of bias in model evaluation?,R1,"In model evaluation, bias refers to the error introduced by approximating a real-world problem with a simplified model. High bias can lead to underfitting, meaning the model cannot capture the underlying patterns well.",1.0,"Bias in model evaluation refers to systematic errors or inaccuracies in the predictions made by a model, which may arise from simplifying assumptions or limitations in the model architecture."
What is the importance of interpretability in machine learning models?,R1,"Interpretability in machine learning is important because it allows humans to understand how and why a model makes its decisions. This is essential in high-stakes areas like healthcare, finance, and law where transparency and trust are critical.",1.0,"Interpretability in machine learning models is important for understanding how predictions are made, gaining insights into the underlying relationships in the data, and building trust with stakeholders."
What is the difference between k-means and hierarchical clustering?,R1,"K-means clustering partitions data into k fixed clusters by minimizing intra-cluster distance. Hierarchical clustering builds a tree of clusters using either a bottom-up or top-down approach, and does not require specifying the number of clusters in advance.",1.0,"K-means clustering partitions the data into k clusters by minimizing the within-cluster sum of squares, while hierarchical clustering builds a tree-like hierarchy of clusters by recursively merging or splitting clusters based on proximity."
What is the Elbow Method used for in k-means clustering?,R1,The Elbow Method is used to determine the optimal number of clusters in k-means clustering. It involves plotting the within-cluster sum of squares (WCSS) against the number of clusters and finding the 'elbow' point where the rate of decrease sharply changes.,1.0,"The Elbow Method is used to determine the optimal number of clusters (k) in k-means clustering by plotting the within-cluster sum of squares against the number of clusters and identifying the ""elbow"" point where the rate of decrease in inertia slows down."
What is the role of regularization in reducing model complexity?,R1,"Regularization helps reduce model complexity by penalizing large weights in the model. This prevents the model from fitting noise in the training data, reducing overfitting and improving generalization.",1.0,"Regularization in machine learning reduces model complexity by adding penalty terms to the loss function, encouraging simpler models with smaller coefficients or fewer features."
What is the difference between gradient descent and stochastic gradient descent?,R1,"Gradient Descent uses the entire training dataset to compute the gradient and update model parameters, while Stochastic Gradient Descent (SGD) updates the parameters using only one sample at a time, which makes it faster but noisier.",1.0,"Gradient descent updates model parameters using the average gradient of the entire training dataset, while stochastic gradient descent updates parameters using the gradient of a single randomly selected data point or mini-batch."
What is the tradeoff between bias and variance in model selection?,R1,The tradeoff between bias and variance involves balancing error from overly simplistic models (high bias) and error from models that are too complex and sensitive to training data (high variance). Finding the right balance improves generalization on new data.,1.0,The bias-variance tradeoff in model selection involves balancing the tradeoff between bias (underfitting) and variance (overfitting) by selecting models with an appropriate level of complexity based on the dataset and task.
What is the role of regularization in reducing overfitting?,R1,"Regularization adds a penalty to the loss function to discourage complex models, which helps reduce overfitting by simplifying the model and improving generalization.",1.0,"Regularization in machine learning reduces overfitting by penalizing overly complex models, encouraging simpler models that generalize better to unseen data."
What are some techniques for handling imbalanced datasets in classification tasks?,R1,"Techniques for handling imbalanced datasets include oversampling the minority class, undersampling the majority class, using synthetic data generation like SMOTE, and applying class weighting during model training.",1.0,"Techniques for handling imbalanced datasets include resampling methods such as oversampling the minority class or undersampling the majority class, using cost-sensitive learning algorithms, and generating synthetic samples with techniques like SMOTE."
What is the purpose of A/B testing in data science?,R1,A/B testing is used to compare two versions of a product or feature to determine which one performs better based on user behavior or other metrics.,1.0,"A/B testing is used to compare two or more versions of a product or intervention to determine which one performs better based on predefined metrics, such as conversion rate or user engagement."
What is the difference between correlation and causation?,R1,"Correlation means two variables move together, but causation means one variable directly causes the change in another. Correlation does not imply causation.",1.0,"Correlation measures the strength and direction of the linear relationship between two variables, while causation indicates that changes in one variable directly cause changes in another variable."
What is the role of feature scaling in machine learning?,R1,"Feature scaling standardizes the range of independent variables or features of data, which helps many machine learning algorithms converge faster and perform better.",1.0,"Feature scaling in machine learning ensures that all features have the same scale or range, preventing certain features from dominating others during model training. Common scaling techniques include min-max scaling and standardization."
What is the difference between classification and clustering?,R1,"Classification assigns predefined labels to data points based on training, while clustering groups data points based on similarity without predefined labels.",1.0,"Classification is a supervised learning task where the goal is to predict categorical labels or classes, while clustering is an unsupervised learning task where the goal is to group similar data points together based on their features."
What is the purpose of Principal Component Analysis (PCA) in dimensionality reduction?,R1,"PCA reduces the number of features in a dataset by transforming the data into new uncorrelated variables called principal components, which capture the most variance.",1.0,"Principal Component Analysis (PCA) is used to reduce the dimensionality of a dataset by transforming the original features into a new set of orthogonal principal components, which capture the maximum variance in the data."
What is the role of regularization in neural networks?,R1,"Regularization in neural networks helps prevent overfitting by adding a penalty to the loss function, encouraging simpler models that generalize better to new data.",1.0,"Regularization in neural networks helps prevent overfitting by adding penalty terms to the loss function, encouraging simpler models with smaller weights or fewer connections."
What is dropout regularization in neural networks?,R1,"Dropout regularization randomly disables a fraction of neurons during training, which helps prevent overfitting by forcing the network to learn redundant representations.",1.0,"Dropout regularization is a technique used in neural networks to reduce overfitting by randomly dropping out a fraction of neurons during training, forcing the network to learn more robust representations."
What is the difference between a generative adversarial network (GAN) and a variational autoencoder (VAE)?,R1,"A GAN consists of two neural networks, a generator and a discriminator, competing against each other to create realistic data, while a VAE is a probabilistic model that learns to encode and decode data using latent variables.",1.0,"Generative adversarial networks (GANs) consist of two neural networks (generator and discriminator) that compete with each other to generate realistic data samples, while variational autoencoders (VAEs) learn a probabilistic model of the data's latent space and generate new samples by sampling from this distribution."
What is the difference between batch gradient descent and mini-batch gradient descent?,R1,"Batch gradient descent computes the gradient using the entire training dataset for each update, while mini-batch gradient descent uses small batches of data, combining the advantages of batch and stochastic methods.",1.0,"Batch gradient descent computes the gradient of the cost function using the entire training dataset, while mini-batch gradient descent computes the gradient using a randomly selected subset (mini-batch) of the training data."
What is the role of activation functions in neural networks?,R1,"Activation functions introduce non-linearity into neural networks, enabling them to learn complex patterns and relationships beyond simple linear transformations.",1.0,"Activation functions introduce non-linearity into neural networks, allowing them to learn complex mappings between inputs and outputs. Common activation functions include sigmoid, tanh, ReLU, and softmax."
What is the difference between generative and discriminative models in machine learning?,R1,"Generative models learn the joint probability distribution of inputs and outputs and can generate new data, while discriminative models learn the decision boundary between classes without generating data.",1.0,"Generative models learn the joint probability distribution of the input features and the target labels, while discriminative models learn the conditional probability distribution of the target labels given the input features."
What is the purpose of dropout regularization in neural networks?,R1,"Dropout regularization helps prevent overfitting in neural networks by randomly disabling a fraction of neurons during training, forcing the network to learn more robust features.",1.0,"Dropout regularization is a technique used in neural networks to prevent overfitting by randomly dropping out a fraction of neurons during training, forcing the network to learn more robust representations."
What is the role of LSTMs in sequence modeling tasks?,R1,"LSTMs are designed to handle long-term dependencies in sequence data by using gates to control the flow of information, which helps avoid the vanishing gradient problem common in traditional RNNs.",1.0,"Long Short-Term Memory (LSTM) networks are a type of recurrent neural network (RNN) designed to capture long-range dependencies and temporal dynamics in sequential data, making them well-suited for tasks such as language modeling and time series prediction."
What is the difference between precision and recall in classification tasks?,R1,"Precision measures the proportion of true positive predictions out of all positive predictions, while recall measures the proportion of true positives out of all actual positives.",1.0,"Precision measures the proportion of true positive predictions among all positive predictions made by a classifier, while recall measures the proportion of true positive predictions among all actual positive instances in the data."
"What is the F1 score, and why is it useful in model evaluation?",R1,"The F1 score is the harmonic mean of precision and recall, providing a balance between the two metrics especially useful when classes are imbalanced.",1.0,"The F1 score is the harmonic mean of precision and recall and provides a single metric to evaluate the performance of a classifier, balancing both precision and recall. It is particularly useful for imbalanced datasets where one class is much more prevalent than the others."
What is the purpose of word embedding in natural language processing (NLP)?,R1,"Word embeddings are a core concept in NLP, transforming words into dense numerical vectors. This process allows computers to understand and process human language by representing words in a continuous vector space where words with similar meanings are located closer together. This capability is essential for various NLP tasks, including sentiment analysis, machine translation, and information retrieval, as it enables models to capture the semantic and contextual relationships between words effectively, leading to improved performance and understanding.",1.0,"Word embedding is a technique used to represent words as dense vectors in a continuous vector space, capturing semantic relationships between words and enabling neural networks to process textual data more effectively."
What is the difference between word2vec and GloVe in word embedding?,R1,"Word2Vec learns word embeddings by predicting surrounding words using a local context window, while GloVe uses global word co-occurrence statistics from the entire corpus.",1.0,"Word2Vec is a model that learns word embeddings by predicting the surrounding words in a text corpus, while GloVe (Global Vectors for Word Representation) learns word embeddings by factorizing the co-occurrence matrix of words in the corpus."
What is the purpose of sentiment analysis in natural language processing (NLP)?,R1,"Sentiment analysis is used in NLP to determine whether a piece of text expresses a positive, negative, or neutral opinion.",1.0,"Sentiment analysis is a natural language processing task that involves determining the sentiment or opinion expressed in a piece of text, such as positive, negative, or neutral. It is commonly used to analyze customer reviews, social media posts, and survey responses."
What is the difference between shallow learning and deep learning?,R1,"Shallow learning models typically have only one or two layers, while deep learning models have many layers, allowing them to learn complex patterns.",1.0,"Shallow learning refers to traditional machine learning algorithms with a small number of layers or features, while deep learning involves neural networks with multiple layers that can learn hierarchical representations of the data."
What is the role of dropout regularization in neural networks?,R1,"Dropout helps prevent overfitting by randomly disabling some neurons during training, so the model doesn’t rely too much on specific paths.",1.0,"Dropout regularization is a technique used in neural networks to prevent overfitting by randomly dropping out a fraction of neurons during training, forcing the network to learn more robust representations."
What is the difference between precision and accuracy in model evaluation?,R1,"Precision measures how many of the predicted positives are actually correct, while accuracy measures how many total predictions were correct overall.",1.0,"Precision measures the proportion of true positive predictions among all positive predictions made by a classifier, while accuracy measures the proportion of correct predictions (both true positives and true negatives) among all predictions made by the classifier."
What is the difference between k-fold cross-validation and leave-one-out cross-validation?,R1,"In k-fold cross-validation, the dataset is split into k parts and each part gets a turn as the test set. In leave-one-out, each test set has only one sample.",1.0,"K-fold cross-validation divides the data into k subsets and iteratively trains the model k times, each time using a different subset as the validation set, while leave-one-out cross-validation trains the model k times, each time using all but one data point as the training set and the remaining point as the validation set."
What is the Naive Bayes classifier and how does it work?,R1,Naive Bayes is a probabilistic classifier that applies Bayes’ Theorem with the assumption that features are independent given the class label.,1.0,"The Naive Bayes classifier is a probabilistic model based on Bayes' theorem with the ""naive"" assumption of feature independence. It calculates the probability of a class given the input features using the product of conditional probabilities."
What is the difference between correlation and covariance?,R1,"Covariance measures the direction of the relationship between two variables, while correlation measures both the strength and direction.",1.0,"Covariance measures the degree to which two variables change together, while correlation measures both the strength and direction of the linear relationship between two variables."
What is the purpose of a confusion matrix in classification tasks?,R1,"A confusion matrix helps visualize how well a classification model is performing by showing true positives, false positives, true negatives, and false negatives.",1.0,"A confusion matrix is a table that visualizes the performance of a classification model by comparing predicted class labels with true class labels. It shows the number of true positives, true negatives, false positives, and false negatives."
What is the difference between stratified sampling and random sampling?,R1,"Stratified sampling divides the population into groups and samples from each, while random sampling selects samples without considering any groups.",1.0,"Stratified sampling divides the population into homogeneous subgroups (strata) and samples from each stratum proportionally, while random sampling selects individuals randomly from the entire population without regard to strata."
"What is the difference between univariate, bivariate, and multivariate analysis?",R1,"Univariate analysis involves one variable, bivariate involves two variables and their relationship, and multivariate involves more than two variables analyzed together.",1.0,"Univariate analysis examines the distribution and summary statistics of a single variable, bivariate analysis explores the relationship between two variables, and multivariate analysis involves analyzing multiple variables simultaneously."
What is the purpose of data preprocessing in machine learning?,R1,"Data preprocessing prepares raw data for a machine learning model by cleaning, normalizing, and transforming it into a usable format.",1.0,"Data preprocessing involves cleaning, transforming, and preparing raw data for machine learning algorithms. It includes tasks such as handling missing values, encoding categorical variables, and scaling features."
What is the bias of a statistical estimator?,R1,Bias is the difference between the expected value of the estimator and the true value of the parameter being estimated.,1.0,The bias of a statistical estimator measures the difference between the expected value of the estimator and the true value of the parameter being estimated. A biased estimator systematically overestimates or underestimates the true parameter value.
What is the variance of a statistical estimator?,R1,Variance of an estimator measures how much the estimator's value would vary across different samples drawn from the same population.,1.0,"The variance of a statistical estimator measures the spread or variability of the estimator's values around its expected value. A high variance indicates that the estimator's values are scattered widely, while a low variance indicates that the values are clustered closely together."
What is the difference between parametric and non-parametric statistical tests?,R1,"Parametric tests assume underlying distributions like normality, while non-parametric tests do not make such assumptions.",1.0,"Parametric statistical tests assume that the data follows a specific probability distribution (e.g., normal distribution), while non-parametric tests make no such assumptions and are based on ranks or frequencies."
What is the purpose of feature scaling in machine learning?,R1,"Feature scaling ensures that all features contribute equally by bringing them to a similar range, which is important for distance-based algorithms.",1.0,"Feature scaling ensures that all features have the same scale or range, preventing certain features from dominating others during model training. Common scaling techniques include min-max scaling and standardization."
What is the difference between correlation and causation in statistics?,R1,"Correlation means two variables move together, while causation means one variable directly affects the other.",1.0,"Correlation measures the strength and direction of the linear relationship between two variables, while causation indicates that changes in one variable directly cause changes in another variable."
What is the Central Limit Theorem and why is it important?,R1,"The Central Limit Theorem states that the sampling distribution of the sample mean approaches a normal distribution as the sample size increases, regardless of the original distribution.",1.0,"The Central Limit Theorem states that the sampling distribution of the sample mean approaches a normal distribution as the sample size increases, regardless of the shape of the population distribution. It is important because it allows us to make inferences about population parameters based on sample statistics."
What is the purpose of outlier detection in data analysis?,R1,"Outlier detection helps identify data points that are significantly different from the rest, which can affect model performance or indicate data entry errors.",1.0,"Outlier detection involves identifying observations that deviate significantly from the rest of the data. Outliers can distort statistical analyses and machine learning models, so detecting and handling them appropriately is important for accurate results."
What is the difference between supervised and unsupervised outlier detection methods?,R1,"Supervised outlier detection uses labeled data where normal and outlier classes are known, while unsupervised methods detect outliers without any labels.",1.0,"Supervised outlier detection methods require labeled data with outlier labels, while unsupervised methods detect outliers based on the characteristics of the data itself without requiring labeled examples."
What is the difference between statistical inference and predictive modeling?,R1,"Statistical inference aims to explain relationships and test hypotheses using data, while predictive modeling focuses on making accurate predictions on new data.",1.0,"Statistical inference involves drawing conclusions about a population based on a sample of data, while predictive modeling focuses on building models that make predictions about future or unseen data based on past observations."
What is the purpose of regularization in machine learning?,R1,Regularization helps prevent overfitting by adding a penalty to the model’s complexity during training.,1.0,"Regularization is a technique used to prevent overfitting by adding a penalty term to the model's objective function, discouraging overly complex models. Common regularization techniques include L1 regularization (Lasso) and L2 regularization (Ridge)."
What is the difference between batch gradient descent and stochastic gradient descent?,R1,"Batch gradient descent uses the whole dataset to compute gradients, while stochastic gradient descent updates weights using one data point at a time.",1.0,"Batch gradient descent updates model parameters using the average gradient of the entire training dataset, while stochastic gradient descent updates parameters using the gradient of a single randomly selected data point or mini-batch."
What is the difference between statistical modeling and machine learning?,R1,"Statistical modeling focuses on understanding relationships between variables, while machine learning emphasizes prediction accuracy, often at the cost of interpretability.",1.0,"Statistical modeling focuses on understanding the relationships between variables in a probabilistic framework, while machine learning emphasizes building algorithms that can learn patterns and make predictions from data without explicitly programming them."
What is the purpose of feature selection in machine learning?,R1,Feature selection helps improve model performance by keeping only the most relevant features and removing the noisy or redundant ones.,1.0,"Feature selection involves selecting the most relevant features from a dataset to improve model performance, reduce overfitting, and enhance interpretability."
What is the difference between regularization and feature selection in machine learning?,R1,"Regularization reduces model complexity by penalizing large coefficients, while feature selection removes irrelevant or redundant features from the dataset.",1.0,"Regularization penalizes overly complex models by adding a penalty term to the objective function, while feature selection explicitly selects a subset of features to include in the model based on their importance or relevance."
What is the difference between grid search and random search for hyperparameter tuning?,R1,"Grid search tries every possible combination of hyperparameters, while random search picks random combinations from the defined space.",1.0,"Grid search exhaustively searches through a specified grid of hyperparameter values, while random search samples hyperparameter values randomly from predefined distributions."
What is the purpose of cross-validation in model evaluation?,R1,Cross-validation is used to estimate how well a model will perform on unseen data by splitting the dataset into training and validation sets multiple times.,1.0,Cross-validation is a technique used to assess the performance of a predictive model by splitting the data into multiple subsets. It helps to estimate the model's generalization error and detect overfitting.
What is the difference between precision and recall in classification evaluation?,R1,"Precision measures how many of the predicted positives are actually correct, while recall measures how many of the actual positives were correctly predicted.",1.0,"Precision measures the proportion of true positive predictions among all positive predictions made by a classifier, while recall measures the proportion of true positive predictions among all actual positive instances in the data."
"What is the F1 score, and why is it useful in imbalanced datasets?",R1,"The F1 score is the harmonic mean of precision and recall, and it's useful in imbalanced datasets because it balances the trade-off between the two.",1.0,The F1 score is the harmonic mean of precision and recall and provides a single metric to evaluate the performance of a classifier. It is particularly useful for imbalanced datasets where one class is much more prevalent than the others.
What is the difference between bagging and boosting ensemble methods?,R1,"Bagging builds multiple independent models in parallel and combines them, while boosting builds models sequentially, each trying to correct the errors of the previous one.",1.0,"Bagging (Bootstrap Aggregating) builds multiple models independently and combines their predictions through averaging or voting, while boosting sequentially builds models by giving more weight to misclassified instances."
What is the purpose of principal component analysis (PCA) in dimensionality reduction?,R1,PCA reduces the number of features by transforming them into a smaller set of uncorrelated components that retain most of the data's variance.,1.0,"Principal Component Analysis (PCA) is used to reduce the dimensionality of a dataset by transforming the original features into a new set of orthogonal principal components, which capture the maximum variance in the data."
What is the difference between classification and regression in machine learning?,R1,"Classification is used when the output is a category or label, while regression is used when the output is a continuous value.",1.0,"Classification is a supervised learning task where the goal is to predict categorical labels or classes, while regression is a supervised learning task where the goal is to predict continuous numerical values."
What is the purpose of natural language processing (NLP) in data science?,R1,"NLP helps machines understand and interpret human language, allowing data scientists to analyze text data like tweets, reviews, or articles.",1.0,"Natural language processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human languages. It enables computers to understand, interpret, and generate human language data."
What is the difference between bag-of-words and word embeddings in NLP?,R1,"Bag-of-words represents text as word counts without considering order or meaning, while word embeddings capture semantic relationships between words in vector space.",1.0,"Bag-of-words represents text as a collection of word counts or frequencies, ignoring word order and grammar, while word embeddings represent words as dense vectors in a continuous vector space, capturing semantic relationships between words."
What is the difference between sentiment analysis and topic modeling in NLP?,R1,"Sentiment analysis determines the emotional tone behind text, while topic modeling identifies the main subjects or themes discussed in the text.",1.0,"Sentiment analysis determines the sentiment or opinion expressed in a piece of text, such as positive, negative, or neutral, while topic modeling identifies the underlying topics or themes present in a collection of documents."
What is the purpose of word2vec in NLP?,R1,Word2Vec is used to convert words into vector representations that capture their semantic meanings and relationships.,1.0,Word2Vec is a model that learns word embeddings by predicting the surrounding words in a text corpus. It captures semantic relationships between words and enables algorithms to process textual data more effectively.
What is the difference between a generative model and a discriminative model in machine learning?,R1,"Generative models learn the joint probability distribution P(x, y), while discriminative models learn the conditional probability P(y|x).",1.0,"A generative model learns the joint probability distribution of the input features and the target labels, while a discriminative model learns the conditional probability distribution of the target labels given the input features."
What is the difference between deep learning and traditional machine learning algorithms?,R1,"Deep learning uses neural networks with many layers to learn complex patterns, while traditional machine learning relies on manually selected features.",1.0,"Deep learning involves neural networks with multiple layers that can learn hierarchical representations of the data, while traditional machine learning algorithms typically use handcrafted features and simpler models."
What is the purpose of transfer learning in deep learning?,R1,"Transfer learning allows a model trained on one task to be reused on a different but related task, saving time and resources.",1.0,"Transfer learning is a technique where a model trained on one task is reused as the starting point for a model on a related task. It enables the transfer of knowledge and features learned from one domain to another, reducing the need for large labeled datasets."
What is the difference between convolutional neural networks (CNNs) and recurrent neural networks (RNNs)?,R1,"CNNs are used mainly for image data because they detect spatial features, while RNNs are used for sequential data like text or time series.",1.0,"Convolutional neural networks (CNNs) are well-suited for processing grid-like data such as images, while recurrent neural networks (RNNs) are designed to handle sequential data with temporal dependencies such as text or time series."
What is the purpose of batch normalization in neural networks?,R1,"Batch normalization helps stabilize and speed up training by normalizing layer inputs, reducing internal covariate shift.",1.0,Batch normalization is a technique used in neural networks to improve training stability and speed by normalizing the activations of each layer. It reduces internal covariate shift and accelerates convergence during training.
What is the difference between a feedforward neural network and a recurrent neural network?,R1,"A feedforward neural network processes inputs in one direction with no memory, while a recurrent neural network has loops that allow it to remember past information.",1.0,"A feedforward neural network consists of multiple layers of neurons connected in a forward direction without cycles, while a recurrent neural network has connections that form directed cycles, allowing it to maintain state or memory over time."
What is the difference between LSTMs and GRUs in recurrent neural networks?,R1,"LSTMs and GRUs are both types of RNNs designed to handle long-term dependencies, but LSTMs use three gates while GRUs use only two.",1.0,"LSTMs (Long Short-Term Memory) and GRUs (Gated Recurrent Units) are both types of recurrent neural network architectures designed to capture long-range dependencies in sequential data, but they differ in their internal gating mechanisms and computational complexity."
What is the purpose of attention mechanisms in neural networks?,R1,"Attention mechanisms help neural networks focus on the most relevant parts of the input when making predictions, improving performance in tasks like translation and summarization.",1.0,Attention mechanisms in neural networks enable the model to focus on relevant parts of the input data while ignoring irrelevant information. They have been particularly successful in natural language processing tasks such as machine translation and text summarization.
What is the difference between generative adversarial networks (GANs) and autoencoders in deep learning?,R1,"GANs consist of two networks—a generator and a discriminator—that compete, while autoencoders consist of an encoder and a decoder that work together to reconstruct data.",1.0,"Generative adversarial networks (GANs) consist of two neural networks (generator and discriminator) that compete with each other to generate realistic data samples, while autoencoders consist of an encoder and decoder network that reconstruct input data."
What is the purpose of reinforcement learning in machine learning?,R1,Reinforcement learning is used to train agents to make decisions by interacting with an environment and learning from rewards and punishments.,1.0,Reinforcement learning is a machine learning paradigm where an agent learns to make decisions by interacting with an environment to maximize cumulative rewards. It is commonly used in settings where actions influence future states and the agent learns through trial and error.
What is the difference between Q-learning and policy gradient methods in reinforcement learning?,R1,"Q-learning is a value-based method that learns the value of actions, while policy gradient methods directly learn the policy that maps states to actions.",1.0,"Q-learning is a model-free reinforcement learning algorithm that learns the optimal action-selection policy by iteratively updating a Q-value function, while policy gradient methods directly parameterize the policy and update the policy parameters based on gradients of expected rewards."
What is the purpose of data preprocessing in exploratory data analysis (EDA)?,R1,Data preprocessing helps clean and prepare the data so that it can be properly analyzed and modeled during EDA.,1.0,"Data preprocessing in EDA involves cleaning, transforming, and preparing raw data for analysis. It includes tasks such as handling missing values, removing outliers, and encoding categorical variables."
What are some common techniques for handling missing data in EDA?,R1,"Common techniques include removing rows with missing values, filling them with the mean or median, or using forward/backward fill.",1.0,"Common techniques for handling missing data in EDA include imputation (replacing missing values with estimates), deletion (removing rows or columns with missing values), and prediction (using models to estimate missing values)."
What is the difference between univariate and bivariate analysis in EDA?,R1,"Univariate analysis examines one variable at a time, while bivariate analysis explores the relationship between two variables.",1.0,"Univariate analysis examines the distribution and summary statistics of a single variable, while bivariate analysis explores the relationship between two variables."
What is the purpose of data visualization in EDA?,R1,"Data visualization helps to quickly identify patterns, trends, and outliers in a dataset, making EDA more intuitive.",1.0,"Data visualization in EDA helps to explore and understand the structure, patterns, and relationships in the data. It provides insights that may not be apparent from raw data or summary statistics alone."
What are some common types of data visualization used in EDA?,R1,"Common types include histograms, box plots, scatter plots, bar charts, and heatmaps for identifying patterns and distributions.",1.0,"Common types of data visualization used in EDA include histograms, scatter plots, box plots, bar charts, line plots, and heatmaps."
What is the difference between continuous and categorical variables in EDA?,R1,"Continuous variables can take any numeric value within a range, while categorical variables represent groups or categories.",1.0,"Continuous variables are numeric variables with an infinite number of possible values within a range, while categorical variables are qualitative variables with a finite number of distinct categories or levels."
What is the purpose of statistical summaries in EDA?,R1,"Statistical summaries give a quick overview of the data, showing key properties like mean, median, standard deviation, and range.",1.0,"Statistical summaries in EDA provide descriptive statistics such as mean, median, mode, variance, and percentiles to summarize the central tendency, dispersion, and shape of the data distribution."
What is the difference between central tendency and dispersion measures in EDA?,R1,"Central tendency measures like mean, median, and mode show the center of the data, while dispersion measures like range and standard deviation show how spread out the data is.",1.0,"Central tendency measures (e.g., mean, median, mode) describe the central or typical value of a dataset, while dispersion measures (e.g., variance, standard deviation) quantify the spread or variability of the data around the central value."
What is the purpose of hypothesis testing in EDA?,R1,Hypothesis testing helps determine if a pattern or difference observed in the data is statistically significant or just due to chance.,1.0,Hypothesis testing in EDA is used to evaluate whether observed differences or relationships in the data are statistically significant or due to random chance. It involves formulating null and alternative hypotheses and conducting statistical tests to make inferences about population parameters.
What is the difference between parametric and non-parametric tests in hypothesis testing?,R1,"Parametric tests assume the data follows a specific distribution like normal, while non-parametric tests make fewer assumptions about the data.",1.0,"Parametric tests assume that the data follows a specific probability distribution (e.g., normal distribution), while non-parametric tests make no such assumptions and are based on ranks or frequencies."
What is the purpose of correlation analysis in EDA?,R1,Correlation analysis helps identify the strength and direction of relationships between numerical variables in a dataset.,1.0,Correlation analysis in EDA measures the strength and direction of the linear relationship between two continuous variables. It helps to identify patterns and associations in the data.
What is the difference between Pearson correlation and Spearman correlation?,R1,"Pearson correlation measures linear relationships using actual data values, while Spearman correlation measures monotonic relationships using data ranks.",1.0,"Pearson correlation measures the linear relationship between two continuous variables, while Spearman correlation measures the monotonic relationship between variables, allowing for non-linear associations."
What is the purpose of regression analysis in EDA?,R1,Regression analysis helps explore relationships between a dependent variable and one or more independent variables during EDA.,1.0,Regression analysis in EDA is used to model the relationship between one or more independent variables (predictors) and a dependent variable (outcome). It helps to understand how changes in predictors affect the outcome variable.
What are the assumptions of linear regression in EDA?,R1,Linear regression assumes a linear relationship between independent and dependent variables.,1.0,"The assumptions of linear regression in EDA include linearity (relationship between variables is linear), independence (residuals are independent), homoscedasticity (residuals have constant variance), and normality (residuals are normally distributed)."
What is the difference between simple linear regression and multiple linear regression?,R1,"Simple linear regression involves one independent variable to predict a dependent variable. It fits a straight line to the data. On the other hand, multiple linear regression uses two or more independent variables to predict the outcome, allowing for a more complex relationship.",1.0,"Simple linear regression models the relationship between one independent variable and a dependent variable, while multiple linear regression models the relationship between multiple independent variables and a dependent variable."
What is the purpose of logistic regression in EDA?,R1,Logistic regression is used during EDA to understand how input variables relate to a binary target variable. It helps in analyzing the relationship between predictors and the likelihood of an outcome like 'yes' or 'no'.,1.0,Logistic regression in EDA is used to model the relationship between one or more independent variables (predictors) and a binary outcome variable (0 or 1). It is commonly used for classification tasks.
What are the assumptions of logistic regression in EDA?,R1,"One key assumption of logistic regression is that the outcome variable should be binary. It’s designed to predict probabilities between 0 and 1, so it’s not suitable for continuous target variables.",1.0,"The assumptions of logistic regression in EDA include linearity (relationship between predictors and log-odds is linear), independence (observations are independent), and absence of multicollinearity (predictors are not highly correlated)."
What is the purpose of outlier detection in EDA?,R1,"Outlier detection in EDA helps identify data points that deviate significantly from the rest of the dataset. These unusual values can skew analysis, affect model accuracy, and lead to misleading insights if not addressed properly.",1.0,"Outlier detection in EDA involves identifying observations that deviate significantly from the rest of the data. Outliers can distort statistical analyses and machine learning models, so detecting and handling them appropriately is important for accurate results."
What are some common techniques for detecting outliers in EDA?,R1,"One common technique for detecting outliers is using box plots. Any data point outside the whiskers is considered a potential outlier, especially those beyond 1.5 times the interquartile range.",1.0,"Common techniques for detecting outliers in EDA include graphical methods (e.g., scatter plots, box plots), statistical methods (e.g., z-score, modified z-score), and proximity-based methods (e.g., nearest neighbors)."
What is the purpose of box plots in EDA?,R1,"Box plots help summarize the distribution of a dataset. They show the median, quartiles, and potential outliers, making it easier to detect skewness or spread in the data.",1.0,"Box plots in EDA provide a visual summary of the distribution, central tendency, and spread of a dataset. They show the median, quartiles, and potential outliers of the data distribution."
What is the purpose of histograms in EDA?,R1,"Histograms help visualize the frequency distribution of a numerical variable. They show how data is spread across different intervals, making it easier to detect skewness or modality.",1.0,Histograms in EDA display the frequency distribution of a continuous variable by dividing the data into intervals (bins) and plotting the number of observations in each interval. They provide insights into the shape and spread of the data distribution.
What is the purpose of scatter plots in EDA?,R1,"Scatter plots are used in EDA to visualize the relationship between two numerical variables. They help identify patterns, trends, and correlations between variables.",1.0,"Scatter plots in EDA visualize the relationship between two continuous variables by plotting each data point as a point on a graph. They help to identify patterns, trends, and correlations in the data."
What is the purpose of bar charts in EDA?,R1,Bar charts are useful in EDA for visualizing the frequency or count of categories in a categorical variable. They help in understanding how data is distributed across different groups.,1.0,Bar charts in EDA represent categorical data by displaying the frequency or proportion of observations in each category as bars of varying heights. They are useful for comparing the distribution of categorical variables.
What is the purpose of line plots in EDA?,R1,"Line plots are useful in EDA to visualize trends and changes over time for continuous variables. They show how a variable evolves, making them ideal for time-series analysis.",1.0,Line plots in EDA visualize the trend or pattern of a variable over time or another continuous dimension by connecting data points with lines. They are commonly used for time series analysis and trend identification.
What is the purpose of heatmaps in EDA?,R1,Heatmaps are used in EDA to visualize correlations between numerical features. They provide a color-coded matrix that makes it easy to spot strong or weak relationships.,1.0,Heatmaps in EDA visualize the magnitude of a variable or the strength of a relationship between variables using colors. They are useful for identifying patterns and correlations in large datasets.
What is the purpose of correlation matrices in EDA?,R1,Correlation matrices are used in EDA to show how strongly pairs of numerical variables are related. Values close to 1 or -1 indicate strong positive or negative correlations.,1.0,Correlation matrices in EDA display the pairwise correlations between variables in a tabular format or heatmap. They help to identify patterns and associations among variables.
What is the difference between correlation and causation in EDA?,R1,"Correlation means that two variables move together, but causation means one variable directly causes the other to change. EDA can detect correlation, but not always causation.",1.0,"Correlation measures the strength and direction of the linear relationship between two variables, while causation indicates that changes in one variable directly cause changes in another variable. Correlation does not imply causation."
What is the purpose of statistical tests in EDA?,R1,"Statistical tests in EDA help determine whether patterns in data are significant or just due to random chance. For example, they can confirm if differences between groups are meaningful.",1.0,Statistical tests in EDA are used to assess the significance of observed differences or relationships in the data. They help to make inferences about population parameters and draw conclusions from sample data.
What is the difference between parametric and non-parametric tests in EDA?,R1,"Parametric tests assume a specific distribution like normality, while non-parametric tests don’t rely on such assumptions and are more flexible with various data types.",1.0,"Parametric tests assume that the data follows a specific probability distribution (e.g., normal distribution), while non-parametric tests make no such assumptions and are based on ranks or frequencies."
What is the purpose of ANOVA in EDA?,R1,ANOVA is used in EDA to determine whether there are statistically significant differences between the means of three or more groups.,1.0,Analysis of Variance (ANOVA) in EDA is used to compare the means of two or more groups to determine whether they are significantly different. It is commonly used for comparing means across multiple categories or treatments.
What is the difference between one-way ANOVA and two-way ANOVA in EDA?,R1,"One-way ANOVA tests differences between group means based on one factor, while two-way ANOVA evaluates the effect of two factors and their interaction on the outcome.",1.0,"One-way ANOVA compares the means of two or more groups based on a single categorical factor, while two-way ANOVA considers the effects of two categorical factors (main effects and interaction) on the outcome variable."
What is the purpose of chi-squared test in EDA?,R1,The chi-squared test in EDA is used to check if there is a significant association between two categorical variables by comparing observed and expected frequencies.,1.0,The chi-squared test in EDA is used to assess the independence between two categorical variables by comparing the observed frequencies to the expected frequencies under the null hypothesis of independence.
What is the purpose of t-tests in EDA?,R1,"T-tests are used in EDA to compare the means of two groups to see if they are significantly different from each other, helping understand group differences.",1.0,T-tests in EDA are used to compare the means of two independent samples or the mean of a sample to a known value. They help to determine whether observed differences in means are statistically significant.
What is the difference between independent samples t-test and paired samples t-test in EDA?,R1,"Independent samples t-test compares the means of two different groups, while paired samples t-test compares means from the same group at two different times or conditions.",1.0,"Independent samples t-test compares the means of two independent groups, while paired samples t-test compares the means of two related groups (e.g., before and after treatment) based on matched pairs of observations."
What is the purpose of non-parametric tests in EDA?,R1,"Non-parametric tests in EDA are used when the data doesn’t meet the assumptions of parametric tests, like normal distribution or equal variances, making them more flexible.",1.0,"Non-parametric tests in EDA are used when the data does not meet the assumptions of parametric tests (e.g., normal distribution, homogeneity of variance). They provide robust alternatives for analyzing non-normally distributed or skewed data."
What is the difference between Wilcoxon signed-rank test and Mann-Whitney U test in EDA?,R1,"The Wilcoxon signed-rank test is used for comparing two related samples or paired data, while the Mann-Whitney U test compares two independent samples without assuming normal distribution.",1.0,"Wilcoxon signed-rank test is a non-parametric test used to compare the medians of two related groups, while Mann-Whitney U test compares the medians of two independent groups."
What is the purpose of exploratory factor analysis (EFA) in EDA?,R1,"Exploratory Factor Analysis (EFA) helps identify underlying latent factors or constructs that explain the correlations among observed variables, reducing dimensionality in EDA.",1.0,Exploratory factor analysis (EFA) in EDA is used to identify underlying factors or latent variables that explain the common variance among observed variables. It helps to uncover the underlying structure of a dataset.
What is the difference between EFA and principal component analysis (PCA) in EDA?,R1,"EFA aims to identify underlying latent factors that explain correlations among variables, while PCA focuses on reducing dimensionality by transforming variables into uncorrelated principal components.",1.0,"EFA and PCA are both dimensionality reduction techniques used in EDA, but EFA aims to identify underlying factors that explain the common variance among observed variables, while PCA aims to capture the maximum variance in the data with orthogonal principal components."
What is the purpose of cluster analysis in EDA?,R1,"Cluster analysis in EDA groups similar data points together based on feature similarity, helping to identify natural groupings or patterns in the data.",1.0,Cluster analysis in EDA is used to identify groups or clusters of similar observations based on their characteristics or features. It helps to uncover natural groupings in the data and identify patterns or relationships.
What are some common algorithms used for cluster analysis in EDA?,R1,"Common algorithms for cluster analysis include K-means, hierarchical clustering, and DBSCAN, which group data based on similarity and distance metrics.",1.0,"Common algorithms used for cluster analysis in EDA include K-means clustering, hierarchical clustering, DBSCAN (Density-Based Spatial Clustering of Applications with Noise), and Gaussian mixture models."
What is the difference between K-means clustering and hierarchical clustering in EDA?,R1,"K-means clustering partitions data into a fixed number of clusters by minimizing variance, while hierarchical clustering builds a tree of clusters by either merging or splitting them.",1.0,"K-means clustering partitions the data into K clusters by minimizing the within-cluster sum of squares, while hierarchical clustering builds a tree-like hierarchy of clusters by recursively merging or splitting clusters based on proximity."
What is the purpose of dendrogram in hierarchical clustering in EDA?,R1,"A dendrogram visually represents the hierarchical relationships between clusters, showing how clusters are merged or split at different distances.",1.0,A dendrogram in hierarchical clustering visualizes the hierarchical relationships between clusters by displaying the merging order and distance between clusters. It helps to identify the optimal number of clusters and interpret the clustering results.
What is the purpose of silhouette analysis in cluster validation in EDA?,R1,"Silhouette analysis measures how similar an object is to its own cluster compared to other clusters, helping validate the quality of clustering.",1.0,"Silhouette analysis in cluster validation assesses the quality of clustering results by calculating silhouette coefficients for each data point, which measure the cohesion within clusters and separation between clusters. Higher silhouette coefficients indicate better-defined clusters."
What is the purpose of dimensionality reduction techniques in EDA?,R1,"Dimensionality reduction techniques reduce the number of features in a dataset while preserving important information, making data easier to visualize and analyze during EDA.",1.0,Dimensionality reduction techniques in EDA are used to reduce the number of features or variables in a dataset while preserving the most important information. They help to overcome the curse of dimensionality and improve computational efficiency.
What are some common dimensionality reduction techniques used in EDA?,R1,"Common dimensionality reduction techniques include Principal Component Analysis (PCA), t-Distributed Stochastic Neighbor Embedding (t-SNE), and Linear Discriminant Analysis (LDA).",1.0,"Common dimensionality reduction techniques used in EDA include principal component analysis (PCA), factor analysis, t-distributed stochastic neighbor embedding (t-SNE), and linear discriminant analysis (LDA)."
What is the purpose of PCA in EDA?,R1,"PCA reduces the dimensionality of a dataset by transforming variables into principal components that capture the most variance, making it easier to visualize and analyze data in EDA.",1.0,Principal Component Analysis (PCA) in EDA is used to reduce the dimensionality of a dataset by transforming the original features into a new set of orthogonal principal components. It helps to identify the underlying structure and patterns in high-dimensional data.
What is the difference between PCA and factor analysis in EDA?,R1,"PCA is a technique that transforms variables into uncorrelated principal components to reduce dimensionality, while factor analysis models latent factors that explain correlations among variables.",1.0,"PCA and factor analysis are both dimensionality reduction techniques used in EDA, but PCA aims to capture the maximum variance in the data with orthogonal principal components, while factor analysis aims to identify underlying factors that explain the common variance among observed variables."
What is the purpose of t-SNE in EDA?,R1,"t-SNE is used to visualize high-dimensional data by reducing it to two or three dimensions while preserving local structures, helping to identify clusters or patterns in EDA.",1.0,t-Distributed Stochastic Neighbor Embedding (t-SNE) in EDA is a technique used for visualizing high-dimensional data in a lower-dimensional space while preserving the local structure and relationships between data points. It is commonly used for visualizing clusters and patterns in complex datasets.
What is the difference between linear discriminant analysis (LDA) and PCA in EDA?,R1,"LDA is a supervised technique that finds a linear combination of features that best separate classes, while PCA is unsupervised and focuses on maximizing variance without considering class labels.",1.0,"Linear Discriminant Analysis (LDA) and PCA are both dimensionality reduction techniques used in EDA, but LDA aims to maximize the separation between classes or groups while reducing dimensionality, whereas PCA focuses on capturing the maximum variance in the data regardless of class labels."
What is the purpose of feature selection in EDA?,R1,"Feature selection helps identify the most important variables in a dataset, reducing dimensionality and improving model performance by removing irrelevant or redundant features.",1.0,"Feature selection in EDA involves selecting the most relevant features from a dataset to improve model performance, reduce overfitting, and enhance interpretability. It helps to identify the subset of features that contribute the most to predicting the target variable."
What are some common feature selection techniques used in EDA?,R1,"Common feature selection techniques include filter methods like correlation and chi-square tests, wrapper methods like recursive feature elimination, and embedded methods such as Lasso regression.",1.0,"Common feature selection techniques used in EDA include filter methods (e.g., correlation analysis, information gain), wrapper methods (e.g., recursive feature elimination, forward/backward selection), and embedded methods (e.g., LASSO, decision trees)."
What is the purpose of ensemble methods in EDA?,R1,Ensemble methods combine predictions from multiple models to improve overall accuracy and reduce overfitting during exploratory data analysis.,1.0,"Ensemble methods in EDA combine multiple models or algorithms to improve predictive performance, robustness, and generalization. They help to reduce overfitting and capture diverse patterns in the data."
What are some common ensemble methods used in EDA?,R1,"Common ensemble methods include bagging, boosting, and stacking. Bagging reduces variance by training multiple models on random subsets of data.",1.0,"Common ensemble methods used in EDA include bagging (Bootstrap Aggregating), boosting (e.g., AdaBoost, Gradient Boosting), and stacking (meta-learning). They combine predictions from multiple base models to make more accurate and reliable predictions."
What is the purpose of model evaluation in EDA?,R1,"Model evaluation in EDA helps to assess how well a model fits the data and predicts outcomes, ensuring its reliability before deployment.",1.0,Model evaluation in EDA involves assessing the performance of predictive models using various metrics and techniques. It helps to determine the effectiveness of the models in capturing patterns and making accurate predictions on unseen data.
What are some common metrics used for model evaluation in EDA?,R1,"Common metrics for model evaluation include accuracy, precision, recall, F1 score, and ROC-AUC, which help assess classification models’ performance.",1.0,"Common metrics used for model evaluation in EDA include accuracy, precision, recall, F1 score, ROC-AUC score, mean squared error (MSE), and R-squared (R2) score. They provide insights into different aspects of model performance."
What is the difference between cross-validation and holdout validation in model evaluation in EDA?,R1,"Cross-validation splits the data into multiple folds and rotates the validation set to ensure all data points are tested, while holdout validation splits the dataset just once into training and test sets.",1.0,"Cross-validation in model evaluation splits the data into multiple subsets (folds) for training and testing, while holdout validation splits the data into two disjoint sets (training and testing). Cross-validation provides more reliable estimates of model performance but is computationally more expensive."
What is the purpose of hyperparameter tuning in model evaluation in EDA?,R1,"Hyperparameter tuning helps improve model performance by finding the optimal values for parameters like learning rate, depth, and regularization strength.",1.0,Hyperparameter tuning in model evaluation involves optimizing the hyperparameters of machine learning algorithms to improve model performance and generalization. It helps to find the best combination of hyperparameters for a given dataset and problem.
What are some common techniques for hyperparameter tuning in EDA?,R1,Grid Search is a common method that exhaustively tries all combinations of specified hyperparameter values.,1.0,"Common techniques for hyperparameter tuning in EDA include grid search, random search, Bayesian optimization, and genetic algorithms. They systematically search the hyperparameter space to find the optimal values for model performance."
What is the purpose of model interpretation in EDA?,R1,"Model interpretation helps understand how different features impact the model’s predictions, which is valuable during EDA to uncover hidden relationships in the data.",1.0,"Model interpretation in EDA involves understanding and explaining how predictive models make decisions and predictions. It helps to gain insights into the underlying patterns, relationships, and features that contribute to the model's predictions."
What are some common techniques for model interpretation in EDA?,R1,"SHAP values are commonly used for model interpretation because they explain the contribution of each feature to the prediction, both locally and globally.",1.0,"Common techniques for model interpretation in EDA include feature importance analysis (e.g., permutation importance, SHAP values), partial dependence plots, individual conditional expectation (ICE) plots, and model-agnostic interpretation methods (e.g., LIME, SHAP)."
What is the purpose of model deployment in EDA?,R1,"Model deployment is not a typical step in EDA. EDA is focused on understanding data, while deployment involves putting a trained model into production.",1.0,Model deployment in EDA involves deploying predictive models into production environments to make real-time predictions on new data. It allows organizations to leverage the insights and predictions generated from data analysis for decision-making and business applications.
What are some common techniques for model deployment in EDA?,R1,"Model deployment isn’t usually part of EDA, but if needed, tools like Flask or FastAPI can wrap models into APIs for lightweight use after EDA insights are gathered.",0.9,"Common techniques for model deployment in EDA include deploying models as web services or APIs, embedding models into applications or dashboards, and integrating models with existing business processes or workflows. They enable seamless integration of predictive models into operational systems and workflows."
What are the common types of regularization used in machine learning?,R1,"The most common types of regularization are L1 (Lasso) and L2 (Ridge). L1 helps with feature selection by shrinking some weights to zero, while L2 penalizes large weights.",1.0,"Common types of regularization used in machine learning include L1 regularization (Lasso), L2 regularization (Ridge), and elastic net regularization (combination of L1 and L2 penalties)."
What is the bias-variance trade-off in machine learning?,R1,The bias-variance trade-off is about balancing two types of errors: bias (error from wrong assumptions) and variance (error from sensitivity to small fluctuations in training data).,1.0,"The bias-variance trade-off refers to the balance between bias (error due to overly simplistic assumptions) and variance (error due to sensitivity to fluctuations in the training data) in machine learning models. Reducing bias typically increases variance, and vice versa."
What are the common types of cross-validation techniques used in machine learning?,R1,"Common cross-validation techniques include k-fold cross-validation, stratified k-fold, leave-one-out (LOOCV), and repeated k-fold.",1.0,"Common types of cross-validation techniques used in machine learning include k-fold cross-validation, stratified k-fold cross-validation, leave-one-out cross-validation, and nested cross-validation."
What is the purpose of hyperparameter tuning in machine learning?,R1,Hyperparameter tuning is used to find the best combination of parameters like learning rate or tree depth to improve a model’s performance.,1.0,Hyperparameter tuning in machine learning involves optimizing the hyperparameters of models to improve their performance and generalization. It helps to find the best combination of hyperparameters for a given dataset and problem.
What are some common techniques for hyperparameter tuning in machine learning?,R1,Grid Search and Random Search are two of the most common hyperparameter tuning techniques. They help explore parameter combinations to find the best fit.,1.0,"Common techniques for hyperparameter tuning in machine learning include grid search, random search, Bayesian optimization, genetic algorithms, and gradient-based optimization methods. They systematically search the hyperparameter space to find optimal values."
What is the difference between supervised and unsupervised learning in machine learning?,R1,"Supervised learning uses labeled data to train the model, while unsupervised learning uses data without labels to find patterns or groupings.",1.0,"Supervised learning involves learning from labeled data with input-output pairs, where the model predicts the output variable based on input features. Unsupervised learning involves learning from unlabeled data to discover hidden patterns or structures in the data."
What are some common dimensionality reduction techniques used in machine learning?,R1,"Principal Component Analysis (PCA), t-SNE, and Linear Discriminant Analysis (LDA) are some commonly used dimensionality reduction techniques.",1.0,"Common dimensionality reduction techniques used in machine learning include principal component analysis (PCA), linear discriminant analysis (LDA), t-distributed stochastic neighbor embedding (t-SNE), and autoencoders."
What is the purpose of model interpretation in machine learning?,R1,"Model interpretation helps us understand how and why a model makes certain predictions, which is important for trust and transparency.",1.0,"Model interpretation in machine learning involves understanding and explaining how predictive models make decisions and predictions. It helps to gain insights into the underlying patterns, relationships, and features that contribute to the model's predictions."
What are some common techniques for model interpretation in machine learning?,R1,SHAP and LIME are two popular techniques used to explain model predictions by showing feature contributions.,1.0,"Common techniques for model interpretation in machine learning include feature importance analysis (e.g., permutation importance, SHAP values), partial dependence plots, individual conditional expectation (ICE) plots, and model-agnostic interpretation methods (e.g., LIME, SHAP)."
What is the purpose of model deployment in machine learning?,R1,"Model deployment is the process of making a trained machine learning model available for real-world use, such as predicting on live data.",1.0,Model deployment in machine learning involves deploying predictive models into production environments to make real-time predictions on new data. It allows organizations to leverage the insights and predictions generated from data analysis for decision-making and business applications.
What are some common techniques for model deployment in machine learning?,R1,"Common deployment techniques include using REST APIs, containerization with Docker, and cloud services like AWS, Azure, or GCP.",1.0,"Common techniques for model deployment in machine learning include deploying models as web services or APIs, embedding models into applications or dashboards, and integrating models with existing business processes or workflows. They enable seamless integration of predictive models into operational systems and workflows."
What is the purpose of transfer learning in machine learning?,R1,"Transfer learning helps a model trained on one task apply its knowledge to a different but related task, reducing training time and data needs.",1.0,"Transfer learning in machine learning is a technique where a model trained on one task is reused as the starting point for a model on a related task. It enables the transfer of knowledge and features learned from one domain to another, reducing the need for large labeled datasets."
What are some common applications of transfer learning in machine learning?,R1,"Transfer learning is widely used in image classification tasks, like detecting objects or animals using pre-trained CNN models like ResNet.",1.0,"Common applications of transfer learning in machine learning include image classification, natural language processing (NLP), and speech recognition. Pre-trained models such as BERT, ResNet, and GPT are often used as starting points for transfer learning tasks."
What is the purpose of feature engineering in machine learning?,R1,Feature engineering helps improve model performance by creating new relevant features or transforming existing ones to better represent the underlying patterns.,1.0,"Feature engineering involves selecting, transforming, or creating new features from the raw data to improve the performance of machine learning models by making them more expressive, informative, and suitable for the task at hand."
What are some common techniques for feature engineering?,R1,"Some common techniques include one-hot encoding, normalization, and feature scaling to prepare the data for machine learning models.",1.0,"Common techniques include one-hot encoding for categorical variables, scaling numerical features, creating interaction terms, binning or discretizing continuous variables, and extracting features from text or images using techniques like TF-IDF or CNNs."
What is the difference between batch processing and real-time processing?,R1,"Batch processing handles large volumes of data at once, usually on a schedule, while real-time processing deals with data instantly as it arrives.",1.0,"Batch processing involves processing data in large, discrete batches or groups, typically on a scheduled or periodic basis, while real-time processing involves handling data as soon as it is generated or received, often requiring low-latency and immediate responses."
What is transfer learning in NLP?,R1,Transfer learning in NLP involves using a model trained on a large text corpus and fine-tuning it for a specific task like sentiment analysis or translation.,1.0,"Transfer learning in NLP involves leveraging pre-trained language models, such as BERT or GPT, trained on large corpora of text data, and fine-tuning them on specific tasks or domains with smaller datasets to achieve better performance and faster convergence."
What is named entity recognition (NER)?,R1,"Named Entity Recognition (NER) is a subtask of NLP that identifies and classifies proper nouns in text into predefined categories like person names, organizations, and locations.",1.0,"Named entity recognition is a subtask of information extraction that involves identifying and classifying named entities (e.g., persons, organizations, locations) mentioned in text documents, which is useful for tasks such as entity linking and relation extraction."
What are some challenges in working with unstructured data?,R1,"Unstructured data like text or images lacks a predefined format, making it hard to store, process, and analyze efficiently.",1.0,"Challenges include extracting meaningful information from unstructured text, handling noisy or inconsistent data, dealing with large volumes of data, and ensuring privacy and security when working with sensitive information."
What is collaborative filtering in recommendation systems?,R1,Collaborative filtering is a method used in recommendation systems that makes suggestions based on the preferences of similar users.,1.0,"Collaborative filtering is a technique used in recommendation systems to generate personalized recommendations by analyzing similarities and patterns in users' behavior or preferences, often based on user-item interactions or user-user similarity."
What are some evaluation metrics used in recommendation systems?,R1,"Common evaluation metrics in recommendation systems include precision, recall, F1-score, and Mean Average Precision (MAP).",1.0,"Evaluation metrics include precision, recall, F1 score, Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and ranking-based metrics like Mean Average Precision (MAP) and Normalized Discounted Cumulative Gain (NDCG)."
What is the Markov Decision Process (MDP)?,R1,A Markov Decision Process (MDP) is a mathematical framework used to model decision making where outcomes are partly random and partly under the control of a decision maker.,1.0,"The Markov Decision Process is a mathematical framework used to model sequential decision-making problems, where an agent takes actions in an environment to maximize cumulative rewards, while satisfying the Markov property (future states depend only on the current state and action)."
What is policy gradient in reinforcement learning?,R1,Policy gradient is a reinforcement learning method where the policy is directly optimized by computing gradients of expected rewards.,1.0,"Policy gradient methods are a class of reinforcement learning algorithms that directly optimize the policy (the agent's strategy or behavior) by estimating gradients of expected rewards with respect to the policy parameters, typically using techniques like stochastic gradient ascent."
What is data augmentation in deep learning?,R1,"Data augmentation is a technique in deep learning where training data is artificially increased using transformations like rotation, flipping, and scaling.",1.0,"Data augmentation is a technique used to artificially increase the size and diversity of training datasets by applying transformations such as rotation, translation, scaling, cropping, or flipping to input data, which can improve model generalization and robustness."
What is sequence-to-sequence learning?,R1,"Sequence-to-sequence learning is a technique where an input sequence is transformed into an output sequence, commonly used in tasks like machine translation.",1.0,"Sequence-to-sequence learning is a type of model architecture used for tasks involving input and output sequences of variable lengths, such as machine translation, summarization, and speech recognition, typically implemented using recurrent neural networks (RNNs) or transformers."
What is attention mechanism in deep learning?,R1,"The attention mechanism allows a model to focus on the most relevant parts of the input when producing each part of the output, improving tasks like translation.",1.0,"Attention mechanism is a mechanism used in neural networks to selectively focus on relevant parts of the input data while processing sequences, enabling the model to learn to weigh different input elements dynamically and attend to the most informative parts."
What are some common optimization algorithms used in deep learning?,R1,"Some common optimization algorithms in deep learning include SGD, Adam, RMSprop, and Adagrad, which help minimize the loss function efficiently.",1.0,"Common optimization algorithms include stochastic gradient descent (SGD), Adam, RMSprop, and Adagrad, each with different update rules and learning rate adaptation strategies to optimize the model parameters efficiently."
What is semi-supervised learning?,R1,Semi-supervised learning is a machine learning approach that uses a small amount of labeled data along with a large amount of unlabeled data to improve learning accuracy.,1.0,"Semi-supervised learning is a machine learning paradigm that combines labeled and unlabeled data to improve model performance, often by leveraging the inherent structure or relationships in the data to semi-supervised learning algorithms."
What is self-supervised learning?,R1,"Self-supervised learning is a type of machine learning where the model generates its own labels from the input data, often by predicting parts of the data from other parts.",1.0,"Self-supervised learning is a form of unsupervised learning where the model learns to predict or generate certain properties or features of the input data itself, without requiring external labels or annotations, often used as a pretraining step for downstream tasks."
What is the Bellman equation in reinforcement learning?,R1,"The Bellman equation expresses the relationship between the value of a state and the values of its successor states, helping compute optimal policies.",1.0,The Bellman equation is a fundamental equation in dynamic programming and reinforcement learning that expresses the value of a state or state-action pair in terms of the expected immediate reward and the value of the next state or next state-action pair.
What is the difference between on-policy and off-policy learning in reinforcement learning?,R1,"On-policy learning learns the value of the policy being carried out by the agent, while off-policy learning learns the value of a different policy than the one being executed.",1.0,"On-policy learning involves learning the value or policy while following the current policy, while off-policy learning involves learning the value or policy while following a different behavior policy, often leading to more efficient exploration and better sample efficiency."
What are the main components of the data science process?,R1,"The main components include data collection, data cleaning, exploratory data analysis, modeling, and deployment.",1.0,"The main components include data collection, data cleaning and preprocessing, exploratory data analysis, modeling, evaluation, and deployment."
What is overfitting in machine learning?,R1,"Overfitting happens when a model learns the training data too well, including noise, causing poor performance on new data.",1.0,Overfitting occurs when a model learns the details and noise in the training data to the extent that it negatively impacts its performance on unseen data.
Explain the curse of dimensionality.,R1,"The curse of dimensionality refers to various phenomena that arise when analyzing data in high-dimensional spaces, like data sparsity and increased computational cost.",1.0,"The curse of dimensionality refers to the increased difficulty of analyzing and processing data as the number of features or dimensions increases, leading to sparsity and computational challenges."
What is the purpose of cross-validation?,R1,Cross-validation is used to evaluate a model’s performance by training and testing it on different subsets of data to reduce overfitting.,1.0,"Cross-validation is used to assess the generalization performance of a model by partitioning the data into multiple subsets, training the model on some subsets, and evaluating it on the remaining subsets."
How does regularization prevent overfitting in neural networks?,R1,"Regularization adds a penalty to large weights during training, which discourages complex models and reduces overfitting.",1.0,"Regularization techniques such as L1 and L2 regularization add penalty terms to the neural network's loss function, which discourages overly complex weight configurations and helps prevent overfitting."
What is the purpose of activation functions in neural networks?,R1,"Activation functions introduce non-linearity to neural networks, allowing them to learn complex patterns.",1.0,"Activation functions introduce nonlinearity to neural networks, allowing them to learn complex patterns and relationships in the data by transforming the input signal from each neuron into an output signal."
What is the purpose of dimensionality reduction techniques?,R1,Dimensionality reduction techniques help simplify data by reducing the number of features while preserving important information.,1.0,"Dimensionality reduction techniques are used to reduce the number of features or dimensions in a dataset while preserving its important information, which can help improve the performance and efficiency of machine learning algorithms."
Explain the difference between PCA and t-SNE.,R1,"PCA is a linear dimensionality reduction technique that projects data onto principal components, while t-SNE is a non-linear method focusing on preserving local structures.",1.0,"PCA (Principal Component Analysis) is a linear dimensionality reduction technique that seeks to maximize variance, while t-SNE (t-Distributed Stochastic Neighbor Embedding) is a nonlinear technique that focuses on preserving local relationships between data points."
What is K-means clustering?,R1,K-means clustering is an unsupervised learning algorithm that partitions data into K distinct clusters based on feature similarity.,1.0,"K-means clustering is an unsupervised learning algorithm used to partition a dataset into K clusters based on similarities in the data points' features, with the goal of minimizing the within-cluster sum of squares."
What is the elbow method used for in K-means clustering?,R1,The elbow method helps determine the optimal number of clusters (K) by plotting the within-cluster sum of squares and finding the point where the decrease slows down.,1.0,"The elbow method is used to determine the optimal number of clusters (K) in K-means clustering by plotting the within-cluster sum of squares against the number of clusters and selecting the point where the rate of decrease sharply changes (the ""elbow"" point)."
What is outlier detection?,R1,Outlier detection is the process of identifying data points that differ significantly from the majority of the data.,1.0,"Outlier detection is the process of identifying data points or observations that deviate significantly from the rest of the dataset, which may indicate errors, anomalies, or interesting patterns."
What are the advantages of deep learning over traditional machine learning algorithms?,R1,"Deep learning models can automatically extract features from raw data, whereas traditional machine learning often requires manual feature engineering, which can be time-consuming and less effective.",1.0,"Deep learning models, particularly neural networks, can automatically learn hierarchical representations of data, handle large and complex datasets, and often achieve state-of-the-art performance in tasks such as image recognition and natural language processing."
What is backpropagation?,R1,"Backpropagation is the algorithm used in training neural networks. It calculates the gradient of the loss function with respect to each weight by applying the chain rule, allowing the model to learn.",1.0,Backpropagation is a supervised learning algorithm used to train neural networks by iteratively adjusting the weights of connections between neurons in the network based on the error between predicted and actual outputs.
How do you evaluate the performance of a regression model?,R1,"We evaluate a regression model using metrics like Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and R-squared, which measure how close predictions are to the actual values.",1.0,"Performance metrics for regression models include Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), R-squared (coefficient of determination), and others."
What is the purpose of the A/B test in data science?,R1,"A/B testing is used to compare two versions of a variable, like a website design or marketing strategy, to see which one performs better based on a defined metric.",1.0,"A/B testing is a statistical hypothesis testing method used to compare two or more versions of a product, webpage, or other elements to determine which one performs better in terms of predefined metrics."
What is Bayesian inference?,R1,Bayesian inference is a method of statistical inference in which Bayes' theorem is used to update the probability of a hypothesis as more evidence becomes available.,1.0,"Bayesian inference is a statistical approach that uses Bayes' theorem to update the probability of a hypothesis or belief based on new evidence or observations, incorporating prior knowledge and uncertainty into the analysis."
What is the purpose of hypothesis testing?,R1,Hypothesis testing is used to determine whether there is enough evidence in a sample of data to support a particular belief or hypothesis about a population.,1.0,"Hypothesis testing is used to make inferences or decisions about a population parameter based on sample data, by testing a null hypothesis against an alternative hypothesis using statistical methods."
What is regularization in linear regression?,R1,"Regularization in linear regression is a technique to prevent overfitting by adding a penalty term to the loss function, which discourages complex models.",1.0,"Regularization in linear regression involves adding a penalty term to the ordinary least squares (OLS) loss function to prevent overfitting, with common regularization techniques including Ridge regression (L2 regularization) and Lasso regression (L1 regularization)."
What are the assumptions of linear regression?,R1,"Linear regression assumes a linear relationship between the independent and dependent variables, meaning the effect of predictors is additive and proportional.",1.0,"The assumptions of linear regression include linearity between the independent and dependent variables, homoscedasticity (constant variance of errors), independence of errors, and normality of error terms."
What is the difference between Type I and Type II errors?,R1,"Type I error happens when we reject a true null hypothesis, while Type II error happens when we fail to reject a false null hypothesis.",1.0,"Type I error occurs when a true null hypothesis is rejected (false positive), while Type II error occurs when a false null hypothesis is not rejected (false negative)."
What is the p-value in hypothesis testing?,R1,"The p-value measures the probability of observing the sample results, or something more extreme, assuming the null hypothesis is true.",1.0,"The p-value is the probability of observing a test statistic as extreme as, or more extreme than, the one observed, assuming that the null hypothesis is true. It is used to assess the strength of evidence against the null hypothesis."
What is the bias-variance tradeoff?,R1,The bias-variance tradeoff is the balance between a model's ability to accurately capture patterns (low bias) and its sensitivity to noise in the training data (low variance).,1.0,"The bias-variance tradeoff refers to the compromise between bias (error due to overly simplistic assumptions) and variance (error due to sensitivity to fluctuations in the training data) in machine learning models, where reducing one typically increases the other."
What is the difference between variance and standard deviation?,R1,"Variance measures the average squared deviation from the mean, while standard deviation is the square root of variance.",1.0,"Variance measures the average squared deviation from the mean of a set of values, while standard deviation measures the average deviation from the mean, providing a measure of the dispersion or spread of the data."
What is the difference between a decision tree and a random forest?,R1,"A decision tree is a single model that splits data based on features, while a random forest is an ensemble of many decision trees combined to improve accuracy.",1.0,"A decision tree is a simple, interpretable tree-like structure that recursively splits the data based on features, while a random forest is an ensemble of decision trees that aggregates their predictions to improve performance and reduce overfitting."
What is batch normalization in neural networks?,R1,"Batch normalization normalizes the inputs of each layer so that they have a mean of zero and variance of one, which helps stabilize and speed up training.",1.0,"Batch normalization is a technique used to normalize the activations of each layer in a neural network by adjusting and scaling them to have zero mean and unit variance, which can accelerate training and improve model performance."
What is the purpose of a confusion matrix in classification?,R1,A confusion matrix shows how many predictions were correct and incorrect by comparing predicted and actual classes.,1.0,"A confusion matrix is a table that summarizes the performance of a classification model by comparing predicted class labels with actual class labels, showing the counts of true positives, true negatives, false positives, and false negatives."
What is the Kullback-Leibler (KL) divergence?,R1,"KL divergence measures how one probability distribution diverges from a second, expected probability distribution.",1.0,"The Kullback-Leibler divergence is a measure of the difference between two probability distributions, used in information theory and statistics to quantify the amount of information lost when one distribution is used to approximate another."
What is the difference between batch normalization and layer normalization in neural networks?,R1,"Batch normalization normalizes across the batch dimension, while layer normalization normalizes across the features of a single data point.",1.0,"Batch normalization normalizes the activations of each layer across the batch dimension, while layer normalization normalizes the activations across the feature dimension, making it more suitable for recurrent neural networks and other architectures with variable-length sequences."
What is unsupervised learning?,R1,Unsupervised learning is a type of machine learning where the model finds patterns in data without labeled outcomes.,1.0,"Unsupervised learning is a type of machine learning where the model is trained on unlabeled data, and it seeks to find patterns or structures in the data without explicit supervision."
What are the main types of unsupervised learning techniques?,R1,"The main types are clustering and dimensionality reduction, like k-means and PCA.",1.0,"The main types include clustering, dimensionality reduction, and association rule learning."
What are the common clustering algorithms?,R1,"Common clustering algorithms include K-Means, DBSCAN, and Hierarchical Clustering.",1.0,"Common clustering algorithms include K-means clustering, hierarchical clustering, DBSCAN, and Gaussian mixture models (GMM)."
What is hierarchical clustering?,R1,Hierarchical clustering is a method of cluster analysis that builds a hierarchy of clusters using a tree-like structure called a dendrogram.,1.0,Hierarchical clustering is an algorithm that creates a hierarchy of clusters by recursively merging or splitting clusters based on their similarity or dissimilarity.
What is DBSCAN clustering?,R1,DBSCAN is a density-based clustering algorithm that groups together points that are closely packed and labels points in low-density regions as outliers.,1.0,"DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a density-based clustering algorithm that groups together data points that are closely packed, while marking outliers as noise."
What are the common dimensionality reduction techniques?,R1,"Some common dimensionality reduction techniques are PCA, t-SNE, and LDA.",1.0,"Common techniques include Principal Component Analysis (PCA), t-Distributed Stochastic Neighbor Embedding (t-SNE), and Singular Value Decomposition (SVD)."
What is PCA (Principal Component Analysis)?,R1,PCA is a dimensionality reduction technique that transforms data into a set of linearly uncorrelated components called principal components.,1.0,PCA is a linear dimensionality reduction technique that transforms the original features into a lower-dimensional space while preserving the maximum variance in the data.
What is t-SNE (t-Distributed Stochastic Neighbor Embedding)?,R1,t-SNE is a dimensionality reduction technique mainly used for visualizing high-dimensional data in 2 or 3 dimensions.,1.0,"t-SNE is a nonlinear dimensionality reduction technique that focuses on preserving local relationships between data points in a lower-dimensional space, often used for visualizing high-dimensional data."
What is Singular Value Decomposition (SVD)?,R1,"SVD is a matrix factorization technique used to break down a matrix into three matrices: U, Σ, and Vᵀ.",1.0,"SVD is a matrix factorization technique used to decompose a matrix into the product of three matrices, which can be used for dimensionality reduction, data compression, and matrix approximation."
What is association rule learning?,R1,Association rule learning is a method in data mining to find interesting relationships or patterns between variables in large datasets.,1.0,"Association rule learning is a type of unsupervised learning used to discover interesting associations or relationships between variables in large datasets, commonly used in market basket analysis and recommendation systems."
What are the common association rule learning algorithms?,R1,Apriori and FP-Growth are the most common algorithms used for association rule learning.,1.0,Common algorithms include Apriori and FP-Growth.
What is the Apriori algorithm?,R1,Apriori is an algorithm used in association rule learning to find frequent itemsets and generate association rules from transactional datasets.,1.0,"The Apriori algorithm is a classic algorithm for mining frequent itemsets and generating association rules, based on the principle of the Apriori property and candidate generation."
What is FP-Growth?,R1,FP-Growth is a fast algorithm used in association rule learning to find frequent itemsets without generating candidates like Apriori does.,1.0,"FP-Growth (Frequent Pattern Growth) is an efficient algorithm for mining frequent itemsets and generating association rules without candidate generation, using a compact data structure called FP-tree."
What is anomaly detection?,R1,Anomaly detection is the process of identifying data points that deviate significantly from the majority of data.,1.0,"Anomaly detection, also known as outlier detection, is the process of identifying data points or observations that deviate significantly from the rest of the dataset, which may indicate errors, anomalies, or interesting patterns."
What are the common anomaly detection techniques?,R1,"Common techniques include statistical methods, clustering, classification, and deep learning models.",1.0,"Common techniques include statistical methods, density-based methods, distance-based methods, and machine learning-based methods such as Isolation Forest and One-Class SVM."
What is Isolation Forest?,R1,Isolation Forest is an anomaly detection algorithm that works by isolating data points using random decision trees.,1.0,"Isolation Forest is an unsupervised learning algorithm for anomaly detection that isolates anomalies by recursively partitioning the dataset into subsets using random splits, making anomalies easier to isolate than normal data points."
What is One-Class SVM?,R1,One-Class SVM is an unsupervised machine learning algorithm used for anomaly detection by learning the boundary of normal data.,1.0,One-Class Support Vector Machine (SVM) is a machine learning algorithm for anomaly detection that learns a boundary around normal data points in the feature space and identifies anomalies as data points lying outside this boundary.
What is the purpose of density estimation?,R1,Density estimation is used to estimate the probability distribution of a dataset based on observed data points.,1.0,"Density estimation is the process of estimating the probability density function of a random variable from a set of data points, often used in clustering, anomaly detection, and generative modeling."
What are the common density estimation techniques?,R1,Common techniques include parametric methods like Gaussian Mixture Models and non-parametric methods like Kernel Density Estimation.,1.0,"Common techniques include histogram-based methods, kernel density estimation (KDE), Gaussian mixture models (GMM), and Parzen window estimation."
What is the Gaussian Mixture Model (GMM)?,R1,Gaussian Mixture Model is a probabilistic model that assumes data is generated from a mixture of several Gaussian distributions.,1.0,"Gaussian Mixture Model (GMM) is a probabilistic model used for density estimation and clustering, assuming that the data is generated from a mixture of several Gaussian distributions."
What is the Expectation-Maximization (EM) algorithm?,R1,The EM algorithm is an iterative method to find maximum likelihood estimates of parameters in models with latent variables.,1.0,"The Expectation-Maximization (EM) algorithm is an iterative optimization algorithm used to estimate the parameters of probabilistic models with latent variables, such as Gaussian mixture models (GMM), by iteratively computing the expected values of the latent variables and maximizing the likelihood function."
What is the difference between generative and discriminative models?,R1,"Generative models learn the joint probability distribution P(X, Y) and can generate new data, while discriminative models learn the conditional probability P(Y|X) and focus on classification.",1.0,"Generative models learn the joint probability distribution of the input features and the output labels, while discriminative models learn the conditional probability distribution of the output labels given the input features."
What is the purpose of generative modeling?,R1,"Generative modeling aims to learn the underlying data distribution to generate new, realistic samples similar to the training data.",1.0,"Generative modeling is used to model the underlying structure of the data and generate new samples from the learned distribution, often used in tasks such as image generation, text generation, and data augmentation."
What is data science?,R2,Data science is about using tools like Excel and SQL to store and organize data. It’s mostly about databases and making tables that help keep records. I think it’s important for organizing data in companies and hospitals.,0.3,"Data science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from data."
What are the key steps in the data science process?,R2,"First, you collect some data and then directly make graphs or dashboards using Excel or PowerPoint. After that, you can start coding in Python or maybe run SQL queries. I don’t think cleaning or modeling the data is that important unless the data looks messy.",0.3,"The key steps typically include problem definition, data collection, data preparation, exploratory data analysis, modeling, evaluation, and deployment."
What is the difference between supervised and unsupervised learning?,R2,"In supervised learning, the system is told what the correct answers are in advance. In unsupervised learning, it just guesses based on the information it sees. This makes supervised learning more accurate in most cases, especially when predicting specific outcomes like exam scores.",0.8,"Supervised learning involves training a model on labeled data, where the algorithm learns the relationship between input features and target labels. Unsupervised learning deals with unlabeled data and aims to find hidden patterns or structures within the data."
Explain the bias-variance tradeoff.,R2,Bias happens when a model makes strong assumptions and ignores patterns in the data. Variance occurs when a model is too sensitive to small changes in the training data. Managing this tradeoff helps in building models that generalize well on new data.,0.95,The bias-variance tradeoff is the balance between the error introduced by bias (underfitting) and the error introduced by variance (overfitting) when building a machine learning model.
What is feature engineering?,R2,"In feature engineering, we clean the dataset and remove unwanted rows or columns. It’s mostly about deleting missing values and renaming columns so the data looks neat. Once cleaned, the features are ready for training a machine learning model.",0.3,"Feature engineering is the process of selecting, transforming, and creating features from raw data to improve model performance."
How do you handle missing data in a dataset?,R2,"If data is missing, the easiest way is to delete the entire dataset and start with a new one. Missing data can cause errors, so it’s better to remove everything and collect data again.",0.1,"Missing data can be handled by imputation techniques such as mean, median, or mode imputation, using predictive models to estimate missing values, or by deleting rows or columns with missing values, depending on the dataset size and nature."
What is cross-validation?,R2,"I think cross-validation is when you validate your data by checking it twice. First, you clean it and then you check it again to make sure everything is correct. It’s like proofreading your data before you use it for training.",0.25,Cross-validation is a technique used to assess the performance of a predictive model by splitting the data into multiple subsets.
Explain the concept of overfitting.,R2,Overfitting means that your model gives 100% accuracy all the time. It is the best case because it means your model is perfect and you don’t need to make any changes to it unless the data changes.,0.2,"Overfitting occurs when a model learns the training data too well, capturing noise or random fluctuations in the data instead of the underlying pattern."
What are some common algorithms used in supervised learning?,R2,"Supervised learning uses algorithms like clustering and dimensionality reduction. These methods help the model learn from patterns in the input data without needing the actual labels, making them useful for classification problems.",0.2,"Common algorithms include linear regression, logistic regression, decision trees, random forests, support vector machines, and neural networks."
What is dimensionality reduction?,R2,"Dimensionality reduction means reducing the size of your dataset by deleting rows that don’t seem important. This helps the model train faster and gives better results, especially when the data is very large.",0.4,Dimensionality reduction is the process of reducing the number of input variables in a dataset by transforming them into a lower-dimensional space while preserving important information.
What is regularization in machine learning?,R2,Regularization means restarting the training process whenever the model performs badly. You keep training it with smaller batches until it starts performing better on both training and test data. That’s how you control the accuracy.,0.2,Regularization is a technique used to prevent overfitting by adding a penalty term to the model's objective function.
What is ensemble learning?,R2,I think ensemble learning is about teaching the model in groups. You divide your dataset into parts and assign each to a different model. The model with the best accuracy is selected in the end.,0.5,Ensemble learning is a technique that combines multiple machine learning models to improve performance and robustness.
What is the ROC curve?,R2,The ROC curve is used to show how many correct and incorrect predictions a model makes. It plots training accuracy against testing accuracy to check if the model is overfitting.,0.3,The receiver operating characteristic (ROC) curve is a graphical plot that illustrates the performance of a binary classification model across different threshold settings.
What is AUC-ROC?,R2,AUC-ROC is used in regression problems to check how close the predicted values are to the actual values. A higher AUC means better prediction performance in numerical data.,0.2,The area under the ROC curve (AUC-ROC) is a metric used to evaluate the performance of a binary classification model.
What is the difference between classification and regression?,R2,"Classification is about putting data into charts and regression is about drawing lines through graphs. Classification usually uses bar charts, and regression uses line graphs to represent trends in data.",0.3,"Classification is a supervised learning task where the goal is to predict categorical labels or classes, while regression is a supervised learning task where the goal is to predict continuous numerical values."
What is clustering?,R2,Clustering is when we sort the data in ascending or descending order. It helps organize the dataset before using it in machine learning models. It’s mostly done using sorting functions in Excel or Python.,0.3,Clustering is an unsupervised learning technique that involves grouping similar data points together based on their features or characteristics.
What is the curse of dimensionality?,R2,"Curse of dimensionality is when too many rows in a dataset make it difficult for the model to train. More rows mean more training time, which causes overfitting and poor accuracy.",0.3,The curse of dimensionality refers to the phenomenon where the performance of certain algorithms deteriorates as the number of features or dimensions in the data increases.
What is the difference between precision and recall?,R2,"Precision and recall are both about checking model accuracy. Precision means how fast the model runs, and recall means how many results it gives back. They are mostly used for regression problems.",0.2,"Precision measures the proportion of true positive predictions among all positive predictions made by a classifier, while recall measures the proportion of true positive predictions among all actual positive instances in the data."
What is the F1 score?,R2,"F1 score tells us how fast a machine learning model trains. The lower the F1 score, the quicker the model finishes training. It’s used to speed up algorithms, especially in large datasets.",0.2,The F1 score is the harmonic mean of precision and recall and provides a single metric to evaluate the performance of a classifier.
What is the bias of an estimator?,R2,"Bias of an estimator means the algorithm is unfair or prejudiced. If a model is biased, it means it is treating some classes better than others, like humans show bias.",0.3,The bias of an estimator measures the difference between the expected value of the estimator and the true value of the parameter being estimated.
What is the variance of an estimator?,R2,"Variance of an estimator means how far the predicted values are from the actual values. If predictions are accurate, variance is high. If predictions are wrong, variance is low.",0.3,The variance of an estimator measures the spread or variability of the estimator's values around its expected value.
What is the Central Limit Theorem?,R2,"The Central Limit Theorem means that all data becomes normally distributed when plotted on a graph. No matter how it starts, it ends up as a bell curve after enough training.",0.3,The Central Limit Theorem states that the sampling distribution of the sample mean approaches a normal distribution as the sample size increases.
What is regularization in neural networks?,R2,Regularization means you stop training the neural network once the accuracy reaches a certain level. It helps the model stay simple by preventing further learning beyond a fixed threshold.,0.3,Regularization in neural networks involves adding penalty terms to the loss function to prevent overfitting.
What is batch normalization?,R2,Batch normalization is when we increase the batch size during training to normalize the accuracy. It helps the model perform better because larger batches mean better results.,0.3,Batch normalization is a technique used in neural networks to improve training stability and speed by normalizing the activations of each layer.
What is transfer learning?,R2,Transfer learning is when a model transfers files between different datasets. It helps models share memory and reduce the time required to train on new data by copying data structures.,0.2,Transfer learning is a machine learning technique where a model trained on one task is reused as the starting point for a model on a related task.
What is natural language processing (NLP)?,R2,NLP is a type of computer programming language used to build chatbots and virtual assistants. It is designed specifically for making computers understand only English language commands.,0.3,Natural language processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human languages.
What is word embedding?,R2,Word embedding means converting entire sentences into one number. This number is then used by the machine to understand the meaning of the paragraph.,0.3,Word embedding is a technique used to represent words as dense vectors in a continuous vector space.
What is sentiment analysis?,R2,Sentiment analysis is a method for identifying whether a paragraph is formal or informal. It helps software choose better grammar rules based on the type of sentence.,0.3,Sentiment analysis is a natural language processing task that involves determining the sentiment or opinion expressed in a piece of text.
What is deep learning?,R2,"Deep learning is when you train a model for a long time until it becomes deep. The deeper it gets, the more accurate the model becomes for any kind of data.",0.3,Deep learning is a subset of machine learning that focuses on artificial neural networks with multiple layers.
What is a convolutional neural network (CNN)?,R2,"CNN stands for Central Neural Network, and it is mainly used in brain simulations. It copies the structure of the brain to make more natural decisions in AI systems.",0.2,"A convolutional neural network (CNN) is a type of deep neural network that is particularly well-suited for processing structured grid-like data, such as images."
What is a recurrent neural network (RNN)?,R2,"RNN is a model that remembers all previous inputs and outputs them at once. It’s used to store data like a database, so we don’t need to train models again.",0.3,A recurrent neural network (RNN) is a type of neural network designed to process sequential data by maintaining a state or memory of previous inputs.
What is reinforcement learning?,R2,Reinforcement learning is when the model is given labels and learns directly from them. It works like supervised learning but is more accurate because it uses feedback.,0.3,Reinforcement learning is a machine learning paradigm where an agent learns to make decisions by interacting with an environment to maximize cumulative rewards.
What is Q-learning?,R2,Q-learning is a supervised learning algorithm that uses labels to train a model on how to answer quiz questions. The 'Q' stands for 'Question'.,0.2,Q-learning is a model-free reinforcement learning algorithm used to learn the optimal action-selection policy for a Markov decision process (MDP).
What is the exploration-exploitation tradeoff in reinforcement learning?,R2,"Exploration-exploitation tradeoff means the agent has to either explore new states or delete previous ones. If it explores too much, it forgets what it already learned.",0.3,The exploration-exploitation tradeoff refers to the dilemma faced by reinforcement learning agents when deciding whether to explore new actions or exploit known actions to maximize rewards.
What is deep reinforcement learning?,R2,Deep reinforcement learning is when a deep neural network is trained using supervised labels and rewards at the same time. The goal is to get the highest accuracy using labeled data.,0.3,Deep reinforcement learning combines reinforcement learning with deep learning techniques to handle high-dimensional input spaces and learn complex policies directly from raw sensory input.
What is a Markov decision process (MDP)?,R2,An MDP is a type of deep learning model that uses multiple layers to make decisions. It is similar to CNNs but used mostly for reward-based training.,0.3,A Markov decision process (MDP) is a mathematical framework used to model sequential decision-making problems in which an agent interacts with an environment.
What is cross-entropy loss?,R2,Cross-entropy loss is used when you want to compare two images. It checks how much difference exists between their pixel values and gives a loss score accordingly.,0.2,"Cross-entropy loss, also known as log loss, is a loss function used in classification tasks to quantify the difference between the predicted probability distribution and the true probability distribution of the classes."
What is the softmax function?,R2,Softmax is a function that makes the neural network softer. It reduces the number of neurons firing at once so the model becomes lighter and faster.,0.2,The softmax function is a mathematical function that converts a vector of arbitrary real values into a vector of probabilities.
What is the difference between a generative model and a discriminative model?,R2,"Generative models are used only for generating images and music, while discriminative models are used for predictions. They are not related in terms of probability.",0.3,"A generative model learns the joint probability distribution of the input features and the target labels, while a discriminative model learns the conditional probability distribution of the target labels given the input features."
What is autoencoder?,R2,Autoencoder is a tool that automatically writes code for you. It generates Python or Java code by understanding your project requirements using machine learning.,0.2,An autoencoder is a type of neural network architecture that is trained to reconstruct its input data at the output layer.
What is the difference between bagging and boosting?,R2,Bagging stands for 'bagging groceries' and boosting means making a model faster. They are not related to ensemble learning or machine learning at all.,0.2,"Bagging (Bootstrap Aggregating) is an ensemble method that builds multiple models independently and combines their predictions through averaging or voting, while boosting sequentially builds models by giving more weight to misclassified instances."
What is hyperparameter tuning?,R2,Hyperparameter tuning means fixing errors in the dataset before training the model. It ensures the input values are clean and normalized for better predictions.,0.2,Hyperparameter tuning is the process of finding the optimal hyperparameters for a machine learning model to improve its performance.
What is the curse of dimensionality in the context of feature selection?,R2,The curse of dimensionality is when the computer runs out of memory due to having too many files in the dataset. It has nothing to do with model training or features.,0.2,"The curse of dimensionality in feature selection refers to the challenges and limitations posed by high-dimensional data, such as increased computational complexity and difficulty in finding meaningful features."
What is the difference between L1 and L2 regularization?,R2,"L1 and L2 regularization are the same, just written differently. They both remove features from the dataset before training the model, which reduces overfitting.",0.2,"L1 regularization (Lasso) adds a penalty term proportional to the absolute value of the coefficients, encouraging sparsity and feature selection, while L2 regularization (Ridge) adds a penalty term proportional to the square of the coefficients, penalizing large coefficients."
What is the purpose of cross-validation in feature selection?,R2,The purpose of cross-validation in feature selection is to reduce the number of output labels. It helps in compressing the output space by testing which labels can be removed safely.,0.2,"Cross-validation in feature selection is used to evaluate the performance of a model with different subsets of features, helping to identify the most informative features and reduce the risk of overfitting."
What is the role of bias in model evaluation?,R2,"Bias in model evaluation means the data is politically or socially biased. It always makes the model racist or unfair, even if it’s trained correctly.",0.3,"Bias in model evaluation refers to systematic errors or inaccuracies in the predictions made by a model, which may arise from simplifying assumptions or limitations in the model architecture."
What is the importance of interpretability in machine learning models?,R2,"Interpretability means making the model run faster. If a model is interpretable, it will use fewer resources and predict results more quickly than a complex one.",0.3,"Interpretability in machine learning models is important for understanding how predictions are made, gaining insights into the underlying relationships in the data, and building trust with stakeholders."
What is the difference between k-means and hierarchical clustering?,R2,K-means and hierarchical clustering are just two names for the same algorithm. Both use centroids and need the same number of clusters to work properly.,0.2,"K-means clustering partitions the data into k clusters by minimizing the within-cluster sum of squares, while hierarchical clustering builds a tree-like hierarchy of clusters by recursively merging or splitting clusters based on proximity."
What is the Elbow Method used for in k-means clustering?,R2,The Elbow Method is used to find the best features for clustering. It selects the top features that give the highest accuracy using a curve-shaped graph.,0.25,"The Elbow Method is used to determine the optimal number of clusters (k) in k-means clustering by plotting the within-cluster sum of squares against the number of clusters and identifying the ""elbow"" point where the rate of decrease in inertia slows down."
What is the role of regularization in reducing model complexity?,R2,Regularization increases the complexity of the model by adding extra layers. This ensures the model can learn more patterns and provide better accuracy on the training set.,0.3,"Regularization in machine learning reduces model complexity by adding penalty terms to the loss function, encouraging simpler models with smaller coefficients or fewer features."
What is the difference between gradient descent and stochastic gradient descent?,R2,"Gradient Descent is used for linear models, and Stochastic Gradient Descent is used only for neural networks. That is their main difference.",0.3,"Gradient descent updates model parameters using the average gradient of the entire training dataset, while stochastic gradient descent updates parameters using the gradient of a single randomly selected data point or mini-batch."
What is the tradeoff between bias and variance in model selection?,R2,Bias means the model is wrong and variance means the model is right but changes randomly. The tradeoff is to choose which one is less harmful.,0.3,The bias-variance tradeoff in model selection involves balancing the tradeoff between bias (underfitting) and variance (overfitting) by selecting models with an appropriate level of complexity based on the dataset and task.
What is the role of regularization in reducing overfitting?,R2,Regularization increases the size of the training data so the model doesn’t overfit.,0.2,"Regularization in machine learning reduces overfitting by penalizing overly complex models, encouraging simpler models that generalize better to unseen data."
What are some techniques for handling imbalanced datasets in classification tasks?,R2,You can balance datasets by just removing some examples from the larger class until both classes have the same number of samples.,0.7,"Techniques for handling imbalanced datasets include resampling methods such as oversampling the minority class or undersampling the majority class, using cost-sensitive learning algorithms, and generating synthetic samples with techniques like SMOTE."
What is the purpose of A/B testing in data science?,R2,It helps in randomly dividing users into two groups to test different algorithms at the same time to see which is faster.,0.7,"A/B testing is used to compare two or more versions of a product or intervention to determine which one performs better based on predefined metrics, such as conversion rate or user engagement."
What is the difference between correlation and causation?,R2,"Correlation and causation are the same because if two things happen together, one must cause the other.",0.1,"Correlation measures the strength and direction of the linear relationship between two variables, while causation indicates that changes in one variable directly cause changes in another variable."
What is the role of feature scaling in machine learning?,R2,It reduces the size of the dataset so the model trains faster and uses less memory.,0.2,"Feature scaling in machine learning ensures that all features have the same scale or range, preventing certain features from dominating others during model training. Common scaling techniques include min-max scaling and standardization."
What is the difference between classification and clustering?,R2,"Classification is supervised learning where the model learns from labeled data, whereas clustering is unsupervised learning without labels.",0.95,"Classification is a supervised learning task where the goal is to predict categorical labels or classes, while clustering is an unsupervised learning task where the goal is to group similar data points together based on their features."
What is the purpose of Principal Component Analysis (PCA) in dimensionality reduction?,R2,PCA removes all irrelevant features from the dataset to make it smaller.,0.3,"Principal Component Analysis (PCA) is used to reduce the dimensionality of a dataset by transforming the original features into a new set of orthogonal principal components, which capture the maximum variance in the data."
What is the role of regularization in neural networks?,R2,It makes the neural network train faster by reducing the number of neurons.,0.2,"Regularization in neural networks helps prevent overfitting by adding penalty terms to the loss function, encouraging simpler models with smaller weights or fewer connections."
What is dropout regularization in neural networks?,R2,Dropout means removing all neurons from a layer to make the network smaller and faster.,0.2,"Dropout regularization is a technique used in neural networks to reduce overfitting by randomly dropping out a fraction of neurons during training, forcing the network to learn more robust representations."
What is the difference between a generative adversarial network (GAN) and a variational autoencoder (VAE)?,R2,"GANs generate data by guessing random noise, whereas VAEs compress data into smaller vectors without generating anything new.",0.4,"Generative adversarial networks (GANs) consist of two neural networks (generator and discriminator) that compete with each other to generate realistic data samples, while variational autoencoders (VAEs) learn a probabilistic model of the data's latent space and generate new samples by sampling from this distribution."
What is the difference between batch gradient descent and mini-batch gradient descent?,R2,"Batch gradient descent updates parameters after each data point, and mini-batch updates after processing the whole dataset.",0.2,"Batch gradient descent computes the gradient of the cost function using the entire training dataset, while mini-batch gradient descent computes the gradient using a randomly selected subset (mini-batch) of the training data."
What is the role of activation functions in neural networks?,R2,"They decide which neurons to turn off and which to keep active during training, acting like switches.",0.7,"Activation functions introduce non-linearity into neural networks, allowing them to learn complex mappings between inputs and outputs. Common activation functions include sigmoid, tanh, ReLU, and softmax."
What is the difference between generative and discriminative models in machine learning?,R2,"Discriminative models generate new samples from the data distribution, but generative models only classify data.",0.2,"Generative models learn the joint probability distribution of the input features and the target labels, while discriminative models learn the conditional probability distribution of the target labels given the input features."
What is the purpose of dropout regularization in neural networks?,R2,Dropout means permanently removing some neurons from the network to make it smaller and faster.,0.2,"Dropout regularization is a technique used in neural networks to prevent overfitting by randomly dropping out a fraction of neurons during training, forcing the network to learn more robust representations."
What is the role of LSTMs in sequence modeling tasks?,R2,LSTMs memorize every input in the sequence exactly and use it to predict the output.,0.3,"Long Short-Term Memory (LSTM) networks are a type of recurrent neural network (RNN) designed to capture long-range dependencies and temporal dynamics in sequential data, making them well-suited for tasks such as language modeling and time series prediction."
What is the difference between precision and recall in classification tasks?,R2,"Precision is how many correct predictions you make, and recall is how fast the model runs.",0.2,"Precision measures the proportion of true positive predictions among all positive predictions made by a classifier, while recall measures the proportion of true positive predictions among all actual positive instances in the data."
"What is the F1 score, and why is it useful in model evaluation?",R2,F1 score measures how fast a model predicts outcomes in a test dataset.,0.2,"The F1 score is the harmonic mean of precision and recall and provides a single metric to evaluate the performance of a classifier, balancing both precision and recall. It is particularly useful for imbalanced datasets where one class is much more prevalent than the others."
What is the purpose of word embedding in natural language processing (NLP)?,R2,"The purpose of word embedding is to make words longer and more complex so that only advanced computers can understand them. This process involves adding extra characters and symbols to each word, making them harder for humans to read but supposedly more efficient for machine processing. It is primarily used in highly specialized, technical applications where human readability is not a concern, focusing on machine-centric data manipulation rather than linguistic understanding.",0.1,"Word embedding is a technique used to represent words as dense vectors in a continuous vector space, capturing semantic relationships between words and enabling neural networks to process textual data more effectively."
What is the difference between word2vec and GloVe in word embedding?,R2,"GloVe and Word2Vec are basically the same because both give you vectors for words, so there's no real difference.",0.2,"Word2Vec is a model that learns word embeddings by predicting the surrounding words in a text corpus, while GloVe (Global Vectors for Word Representation) learns word embeddings by factorizing the co-occurrence matrix of words in the corpus."
What is the purpose of sentiment analysis in natural language processing (NLP)?,R2,The goal of sentiment analysis is to translate languages by identifying emotional tones in sentences.,0.2,"Sentiment analysis is a natural language processing task that involves determining the sentiment or opinion expressed in a piece of text, such as positive, negative, or neutral. It is commonly used to analyze customer reviews, social media posts, and survey responses."
What is the difference between shallow learning and deep learning?,R2,"Deep learning is used for only images and videos, while shallow learning is for everything else.",0.3,"Shallow learning refers to traditional machine learning algorithms with a small number of layers or features, while deep learning involves neural networks with multiple layers that can learn hierarchical representations of the data."
What is the role of dropout regularization in neural networks?,R2,Dropout increases training accuracy by keeping all neurons active during the process and updating them together.,0.2,"Dropout regularization is a technique used in neural networks to prevent overfitting by randomly dropping out a fraction of neurons during training, forcing the network to learn more robust representations."
What is the difference between precision and accuracy in model evaluation?,R2,Precision and accuracy are the same — both show how good a model is at predicting the right labels.,0.2,"Precision measures the proportion of true positive predictions among all positive predictions made by a classifier, while accuracy measures the proportion of correct predictions (both true positives and true negatives) among all predictions made by the classifier."
What is the difference between k-fold cross-validation and leave-one-out cross-validation?,R2,"K-fold uses multiple samples for testing, while leave-one-out tests the model on the entire dataset without training.",0.2,"K-fold cross-validation divides the data into k subsets and iteratively trains the model k times, each time using a different subset as the validation set, while leave-one-out cross-validation trains the model k times, each time using all but one data point as the training set and the remaining point as the validation set."
What is the Naive Bayes classifier and how does it work?,R2,Naive Bayes works by storing data in tables and comparing new input to find the closest match.,0.2,"The Naive Bayes classifier is a probabilistic model based on Bayes' theorem with the ""naive"" assumption of feature independence. It calculates the probability of a class given the input features using the product of conditional probabilities."
What is the difference between correlation and covariance?,R2,"Correlation and covariance are the same — they both tell us how similar two variables are, with no major difference.",0.2,"Covariance measures the degree to which two variables change together, while correlation measures both the strength and direction of the linear relationship between two variables."
What is the purpose of a confusion matrix in classification tasks?,R2,The confusion matrix is used to confuse the model by giving it difficult examples to learn from.,0.1,"A confusion matrix is a table that visualizes the performance of a classification model by comparing predicted class labels with true class labels. It shows the number of true positives, true negatives, false positives, and false negatives."
What is the difference between stratified sampling and random sampling?,R2,Random sampling is more accurate because it ensures every individual has equal representation from each group.,0.3,"Stratified sampling divides the population into homogeneous subgroups (strata) and samples from each stratum proportionally, while random sampling selects individuals randomly from the entire population without regard to strata."
"What is the difference between univariate, bivariate, and multivariate analysis?",R2,"Univariate, bivariate, and multivariate analysis all mean the same but differ in the number of models used.",0.2,"Univariate analysis examines the distribution and summary statistics of a single variable, bivariate analysis explores the relationship between two variables, and multivariate analysis involves analyzing multiple variables simultaneously."
What is the purpose of data preprocessing in machine learning?,R2,Preprocessing is only about converting text data into numbers so the model can understand it.,0.3,"Data preprocessing involves cleaning, transforming, and preparing raw data for machine learning algorithms. It includes tasks such as handling missing values, encoding categorical variables, and scaling features."
What is the bias of a statistical estimator?,R2,Bias refers to when the model favors one class over another during training.,0.3,The bias of a statistical estimator measures the difference between the expected value of the estimator and the true value of the parameter being estimated. A biased estimator systematically overestimates or underestimates the true parameter value.
What is the variance of a statistical estimator?,R2,Estimator variance shows the error rate of a model on the training data.,0.3,"The variance of a statistical estimator measures the spread or variability of the estimator's values around its expected value. A high variance indicates that the estimator's values are scattered widely, while a low variance indicates that the values are clustered closely together."
What is the difference between parametric and non-parametric statistical tests?,R2,"Parametric tests are used only for text data, while non-parametric tests are used for images.",0.2,"Parametric statistical tests assume that the data follows a specific probability distribution (e.g., normal distribution), while non-parametric tests make no such assumptions and are based on ranks or frequencies."
What is the purpose of feature scaling in machine learning?,R2,Feature scaling increases the size of features so that models can train faster.,0.2,"Feature scaling ensures that all features have the same scale or range, preventing certain features from dominating others during model training. Common scaling techniques include min-max scaling and standardization."
What is the difference between correlation and causation in statistics?,R2,Causation is just a stronger form of correlation — they both mean the same thing essentially.,0.2,"Correlation measures the strength and direction of the linear relationship between two variables, while causation indicates that changes in one variable directly cause changes in another variable."
What is the Central Limit Theorem and why is it important?,R2,It means that all data eventually becomes normally distributed if we wait long enough.,0.2,"The Central Limit Theorem states that the sampling distribution of the sample mean approaches a normal distribution as the sample size increases, regardless of the shape of the population distribution. It is important because it allows us to make inferences about population parameters based on sample statistics."
What is the purpose of outlier detection in data analysis?,R2,Outliers are useful because they always show the most important trends in data.,0.2,"Outlier detection involves identifying observations that deviate significantly from the rest of the data. Outliers can distort statistical analyses and machine learning models, so detecting and handling them appropriately is important for accurate results."
What is the difference between supervised and unsupervised outlier detection methods?,R2,"Unsupervised outlier detection requires labels, but supervised methods don’t need any labeled data.",0.2,"Supervised outlier detection methods require labeled data with outlier labels, while unsupervised methods detect outliers based on the characteristics of the data itself without requiring labeled examples."
What is the difference between statistical inference and predictive modeling?,R2,Statistical inference and predictive modeling are basically the same—they both try to find patterns in data to make predictions.,0.2,"Statistical inference involves drawing conclusions about a population based on a sample of data, while predictive modeling focuses on building models that make predictions about future or unseen data based on past observations."
What is the purpose of regularization in machine learning?,R2,Regularization makes the model memorize training data more effectively by emphasizing large weights.,0.2,"Regularization is a technique used to prevent overfitting by adding a penalty term to the model's objective function, discouraging overly complex models. Common regularization techniques include L1 regularization (Lasso) and L2 regularization (Ridge)."
What is the difference between batch gradient descent and stochastic gradient descent?,R2,"Stochastic gradient descent is better because it doesn't use any loss function, unlike batch gradient descent.",0.2,"Batch gradient descent updates model parameters using the average gradient of the entire training dataset, while stochastic gradient descent updates parameters using the gradient of a single randomly selected data point or mini-batch."
What is the difference between statistical modeling and machine learning?,R2,Machine learning and statistical modeling are the same thing; they both use equations to describe data.,0.2,"Statistical modeling focuses on understanding the relationships between variables in a probabilistic framework, while machine learning emphasizes building algorithms that can learn patterns and make predictions from data without explicitly programming them."
What is the purpose of feature selection in machine learning?,R2,The main goal of feature selection is to increase the number of features in the dataset for better predictions.,0.2,"Feature selection involves selecting the most relevant features from a dataset to improve model performance, reduce overfitting, and enhance interpretability."
What is the difference between regularization and feature selection in machine learning?,R2,Feature selection and regularization are the same because both delete unnecessary data from the dataset.,0.2,"Regularization penalizes overly complex models by adding a penalty term to the objective function, while feature selection explicitly selects a subset of features to include in the model based on their importance or relevance."
What is the difference between grid search and random search for hyperparameter tuning?,R2,Random search is more accurate than grid search because it checks all combinations in a more organized way.,0.2,"Grid search exhaustively searches through a specified grid of hyperparameter values, while random search samples hyperparameter values randomly from predefined distributions."
What is the purpose of cross-validation in model evaluation?,R2,The purpose of cross-validation is to train the model on the entire dataset multiple times to increase its accuracy artificially.,0.2,Cross-validation is a technique used to assess the performance of a predictive model by splitting the data into multiple subsets. It helps to estimate the model's generalization error and detect overfitting.
What is the difference between precision and recall in classification evaluation?,R2,Precision and recall are the same because they both calculate the number of correct predictions.,0.2,"Precision measures the proportion of true positive predictions among all positive predictions made by a classifier, while recall measures the proportion of true positive predictions among all actual positive instances in the data."
"What is the F1 score, and why is it useful in imbalanced datasets?",R2,"F1 score just adds precision and recall together, and it’s only used for balanced datasets.",0.2,The F1 score is the harmonic mean of precision and recall and provides a single metric to evaluate the performance of a classifier. It is particularly useful for imbalanced datasets where one class is much more prevalent than the others.
What is the difference between bagging and boosting ensemble methods?,R2,Boosting and bagging are the same because they both use voting to combine models and make predictions.,0.2,"Bagging (Bootstrap Aggregating) builds multiple models independently and combines their predictions through averaging or voting, while boosting sequentially builds models by giving more weight to misclassified instances."
What is the purpose of principal component analysis (PCA) in dimensionality reduction?,R2,PCA increases the number of features to make the data more complex and accurate.,0.1,"Principal Component Analysis (PCA) is used to reduce the dimensionality of a dataset by transforming the original features into a new set of orthogonal principal components, which capture the maximum variance in the data."
What is the difference between classification and regression in machine learning?,R2,Regression is just a type of classification that deals with more than two classes.,0.2,"Classification is a supervised learning task where the goal is to predict categorical labels or classes, while regression is a supervised learning task where the goal is to predict continuous numerical values."
What is the purpose of natural language processing (NLP) in data science?,R2,The main purpose of NLP is to generate random text from numbers and formulas.,0.1,"Natural language processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human languages. It enables computers to understand, interpret, and generate human language data."
What is the difference between bag-of-words and word embeddings in NLP?,R2,Word embeddings and bag-of-words are both the same because they convert words into numbers for models.,0.2,"Bag-of-words represents text as a collection of word counts or frequencies, ignoring word order and grammar, while word embeddings represent words as dense vectors in a continuous vector space, capturing semantic relationships between words."
What is the difference between sentiment analysis and topic modeling in NLP?,R2,Both sentiment analysis and topic modeling are used to find out if people liked a product or not.,0.2,"Sentiment analysis determines the sentiment or opinion expressed in a piece of text, such as positive, negative, or neutral, while topic modeling identifies the underlying topics or themes present in a collection of documents."
What is the purpose of word2vec in NLP?,R2,The purpose of Word2Vec is to translate words into multiple languages by using vectors.,0.2,Word2Vec is a model that learns word embeddings by predicting the surrounding words in a text corpus. It captures semantic relationships between words and enables algorithms to process textual data more effectively.
What is the difference between a generative model and a discriminative model in machine learning?,R2,"Discriminative models generate data like images or text, while generative models only classify inputs.",0.2,"A generative model learns the joint probability distribution of the input features and the target labels, while a discriminative model learns the conditional probability distribution of the target labels given the input features."
What is the difference between deep learning and traditional machine learning algorithms?,R2,Traditional machine learning is better than deep learning in all cases because it trains faster and is more accurate.,0.2,"Deep learning involves neural networks with multiple layers that can learn hierarchical representations of the data, while traditional machine learning algorithms typically use handcrafted features and simpler models."
What is the purpose of transfer learning in deep learning?,R2,The purpose of transfer learning is to copy and paste data between models so they become more accurate.,0.2,"Transfer learning is a technique where a model trained on one task is reused as the starting point for a model on a related task. It enables the transfer of knowledge and features learned from one domain to another, reducing the need for large labeled datasets."
What is the difference between convolutional neural networks (CNNs) and recurrent neural networks (RNNs)?,R2,RNNs are a type of CNN that uses filters instead of memory units.,0.2,"Convolutional neural networks (CNNs) are well-suited for processing grid-like data such as images, while recurrent neural networks (RNNs) are designed to handle sequential data with temporal dependencies such as text or time series."
What is the purpose of batch normalization in neural networks?,R2,The main purpose of batch normalization is to make all weights equal in the neural network.,0.2,Batch normalization is a technique used in neural networks to improve training stability and speed by normalizing the activations of each layer. It reduces internal covariate shift and accelerates convergence during training.
What is the difference between a feedforward neural network and a recurrent neural network?,R2,Feedforward networks are always better than recurrent networks because they are more accurate.,0.2,"A feedforward neural network consists of multiple layers of neurons connected in a forward direction without cycles, while a recurrent neural network has connections that form directed cycles, allowing it to maintain state or memory over time."
What is the difference between LSTMs and GRUs in recurrent neural networks?,R2,GRUs are more complex than LSTMs because they have more gates and layers.,0.2,"LSTMs (Long Short-Term Memory) and GRUs (Gated Recurrent Units) are both types of recurrent neural network architectures designed to capture long-range dependencies in sequential data, but they differ in their internal gating mechanisms and computational complexity."
What is the purpose of attention mechanisms in neural networks?,R2,Attention is used to forget all previous layers and only consider the last word in the sequence.,0.2,Attention mechanisms in neural networks enable the model to focus on relevant parts of the input data while ignoring irrelevant information. They have been particularly successful in natural language processing tasks such as machine translation and text summarization.
What is the difference between generative adversarial networks (GANs) and autoencoders in deep learning?,R2,"Autoencoders are mainly used to create fake images, while GANs are used to compress data.",0.2,"Generative adversarial networks (GANs) consist of two neural networks (generator and discriminator) that compete with each other to generate realistic data samples, while autoencoders consist of an encoder and decoder network that reconstruct input data."
What is the purpose of reinforcement learning in machine learning?,R2,The purpose of reinforcement learning is to label data so that models can learn from it just like supervised learning.,0.2,Reinforcement learning is a machine learning paradigm where an agent learns to make decisions by interacting with an environment to maximize cumulative rewards. It is commonly used in settings where actions influence future states and the agent learns through trial and error.
What is the difference between Q-learning and policy gradient methods in reinforcement learning?,R2,"Policy gradient methods are used only when Q-learning fails completely, which is very rare.",0.2,"Q-learning is a model-free reinforcement learning algorithm that learns the optimal action-selection policy by iteratively updating a Q-value function, while policy gradient methods directly parameterize the policy and update the policy parameters based on gradients of expected rewards."
What is the purpose of data preprocessing in exploratory data analysis (EDA)?,R2,"Preprocessing is done to make data look pretty for visualizations only, not for analysis.",0.2,"Data preprocessing in EDA involves cleaning, transforming, and preparing raw data for analysis. It includes tasks such as handling missing values, removing outliers, and encoding categorical variables."
What are some common techniques for handling missing data in EDA?,R2,The best way to handle missing data is to delete the entire dataset and start over.,0.1,"Common techniques for handling missing data in EDA include imputation (replacing missing values with estimates), deletion (removing rows or columns with missing values), and prediction (using models to estimate missing values)."
What is the difference between univariate and bivariate analysis in EDA?,R2,"Univariate analysis is about comparing two columns in a dataset, while bivariate is about plotting histograms only.",0.2,"Univariate analysis examines the distribution and summary statistics of a single variable, while bivariate analysis explores the relationship between two variables."
What is the purpose of data visualization in EDA?,R2,The main purpose of data visualization is to decorate the report so it looks professional.,0.1,"Data visualization in EDA helps to explore and understand the structure, patterns, and relationships in the data. It provides insights that may not be apparent from raw data or summary statistics alone."
What are some common types of data visualization used in EDA?,R2,The main type of visualization in EDA is pie charts because they show everything clearly and are always accurate.,0.2,"Common types of data visualization used in EDA include histograms, scatter plots, box plots, bar charts, line plots, and heatmaps."
What is the difference between continuous and categorical variables in EDA?,R2,"Categorical variables are always numbers, and continuous variables are always text.",0.1,"Continuous variables are numeric variables with an infinite number of possible values within a range, while categorical variables are qualitative variables with a finite number of distinct categories or levels."
What is the purpose of statistical summaries in EDA?,R2,The main purpose is to create visuals like pie charts using the summary numbers.,0.2,"Statistical summaries in EDA provide descriptive statistics such as mean, median, mode, variance, and percentiles to summarize the central tendency, dispersion, and shape of the data distribution."
What is the difference between central tendency and dispersion measures in EDA?,R2,"Central tendency means the data is moving in one direction, and dispersion means it's spinning around randomly.",0.1,"Central tendency measures (e.g., mean, median, mode) describe the central or typical value of a dataset, while dispersion measures (e.g., variance, standard deviation) quantify the spread or variability of the data around the central value."
What is the purpose of hypothesis testing in EDA?,R2,The purpose of hypothesis testing is to guess what kind of chart we should use in the EDA process.,0.2,Hypothesis testing in EDA is used to evaluate whether observed differences or relationships in the data are statistically significant or due to random chance. It involves formulating null and alternative hypotheses and conducting statistical tests to make inferences about population parameters.
What is the difference between parametric and non-parametric tests in hypothesis testing?,R2,Parametric tests work on images and non-parametric tests work on text data.,0.1,"Parametric tests assume that the data follows a specific probability distribution (e.g., normal distribution), while non-parametric tests make no such assumptions and are based on ranks or frequencies."
What is the purpose of correlation analysis in EDA?,R2,It is used to remove unrelated rows from the dataset before training the model.,0.1,Correlation analysis in EDA measures the strength and direction of the linear relationship between two continuous variables. It helps to identify patterns and associations in the data.
What is the difference between Pearson correlation and Spearman correlation?,R2,Pearson is used for text data and Spearman is for numerical data.,0.1,"Pearson correlation measures the linear relationship between two continuous variables, while Spearman correlation measures the monotonic relationship between variables, allowing for non-linear associations."
What is the purpose of regression analysis in EDA?,R2,The purpose of regression in EDA is to clean the data by removing columns with null values.,0.2,Regression analysis in EDA is used to model the relationship between one or more independent variables (predictors) and a dependent variable (outcome). It helps to understand how changes in predictors affect the outcome variable.
What are the assumptions of linear regression in EDA?,R2,One key assumption is that the variables must all be categorical and unrelated.,0.1,"The assumptions of linear regression in EDA include linearity (relationship between variables is linear), independence (residuals are independent), homoscedasticity (residuals have constant variance), and normality (residuals are normally distributed)."
What is the difference between simple linear regression and multiple linear regression?,R2,"In simple linear regression, you use one input to get one output. Multiple linear regression gives multiple outputs from one input, so it's more powerful.",0.2,"Simple linear regression models the relationship between one independent variable and a dependent variable, while multiple linear regression models the relationship between multiple independent variables and a dependent variable."
What is the purpose of logistic regression in EDA?,R2,Logistic regression is mostly used to generate histograms and bar charts in EDA. It transforms all values into visual plots automatically.,0.2,Logistic regression in EDA is used to model the relationship between one or more independent variables (predictors) and a binary outcome variable (0 or 1). It is commonly used for classification tasks.
What are the assumptions of logistic regression in EDA?,R2,"Logistic regression assumes that the data must follow a normal distribution, like in linear regression. Without normality, it doesn't work at all.",0.2,"The assumptions of logistic regression in EDA include linearity (relationship between predictors and log-odds is linear), independence (observations are independent), and absence of multicollinearity (predictors are not highly correlated)."
What is the purpose of outlier detection in EDA?,R2,The main goal of outlier detection is to remove all the highest values from the dataset. This makes the data more balanced and easier to handle.,0.2,"Outlier detection in EDA involves identifying observations that deviate significantly from the rest of the data. Outliers can distort statistical analyses and machine learning models, so detecting and handling them appropriately is important for accurate results."
What are some common techniques for detecting outliers in EDA?,R2,The best way to find outliers is to delete the smallest values from the dataset since they usually represent mistakes or missing data.,0.2,"Common techniques for detecting outliers in EDA include graphical methods (e.g., scatter plots, box plots), statistical methods (e.g., z-score, modified z-score), and proximity-based methods (e.g., nearest neighbors)."
What is the purpose of box plots in EDA?,R2,The main purpose of box plots is to sort the dataset alphabetically and highlight which entries are duplicates or missing.,0.1,"Box plots in EDA provide a visual summary of the distribution, central tendency, and spread of a dataset. They show the median, quartiles, and potential outliers of the data distribution."
What is the purpose of histograms in EDA?,R2,"Histograms are mostly used to sort the data in alphabetical order during EDA, which helps in organizing the variables properly.",0.1,Histograms in EDA display the frequency distribution of a continuous variable by dividing the data into intervals (bins) and plotting the number of observations in each interval. They provide insights into the shape and spread of the data distribution.
What is the purpose of scatter plots in EDA?,R2,The main use of scatter plots is to fill missing values in the dataset by drawing lines between known points.,0.1,"Scatter plots in EDA visualize the relationship between two continuous variables by plotting each data point as a point on a graph. They help to identify patterns, trends, and correlations in the data."
What is the purpose of bar charts in EDA?,R2,The main purpose of a bar chart is to show how numerical data changes over time. Each bar represents a different timestamp.,0.2,Bar charts in EDA represent categorical data by displaying the frequency or proportion of observations in each category as bars of varying heights. They are useful for comparing the distribution of categorical variables.
What is the purpose of line plots in EDA?,R2,The main purpose of line plots is to detect missing values in the dataset by connecting the points and spotting any breaks in the line.,0.2,Line plots in EDA visualize the trend or pattern of a variable over time or another continuous dimension by connecting data points with lines. They are commonly used for time series analysis and trend identification.
What is the purpose of heatmaps in EDA?,R2,The purpose of a heatmap is to show which data points are outliers by using different shades of red and green for high and low values.,0.2,Heatmaps in EDA visualize the magnitude of a variable or the strength of a relationship between variables using colors. They are useful for identifying patterns and correlations in large datasets.
What is the purpose of correlation matrices in EDA?,R2,The main purpose of a correlation matrix is to automatically remove outliers by identifying highly deviant values.,0.1,Correlation matrices in EDA display the pairwise correlations between variables in a tabular format or heatmap. They help to identify patterns and associations among variables.
What is the difference between correlation and causation in EDA?,R2,"In EDA, correlation and causation are the same thing — if two variables are strongly correlated, one is definitely causing the other.",0.1,"Correlation measures the strength and direction of the linear relationship between two variables, while causation indicates that changes in one variable directly cause changes in another variable. Correlation does not imply causation."
What is the purpose of statistical tests in EDA?,R2,The purpose of statistical tests in EDA is to sort the data alphabetically so that models can learn more efficiently.,0.1,Statistical tests in EDA are used to assess the significance of observed differences or relationships in the data. They help to make inferences about population parameters and draw conclusions from sample data.
What is the difference between parametric and non-parametric tests in EDA?,R2,"Parametric tests are for visualizing data, and non-parametric tests are used only for time-series datasets in EDA.",0.1,"Parametric tests assume that the data follows a specific probability distribution (e.g., normal distribution), while non-parametric tests make no such assumptions and are based on ranks or frequencies."
What is the purpose of ANOVA in EDA?,R2,The main purpose of ANOVA in EDA is to draw line graphs for time series data so we can compare trends.,0.1,Analysis of Variance (ANOVA) in EDA is used to compare the means of two or more groups to determine whether they are significantly different. It is commonly used for comparing means across multiple categories or treatments.
What is the difference between one-way ANOVA and two-way ANOVA in EDA?,R2,The main difference is that one-way ANOVA is used for numerical data and two-way ANOVA is used for categorical data in EDA.,0.1,"One-way ANOVA compares the means of two or more groups based on a single categorical factor, while two-way ANOVA considers the effects of two categorical factors (main effects and interaction) on the outcome variable."
What is the purpose of chi-squared test in EDA?,R2,Chi-squared test helps visualize data trends over time by plotting frequency distributions during EDA.,0.1,The chi-squared test in EDA is used to assess the independence between two categorical variables by comparing the observed frequencies to the expected frequencies under the null hypothesis of independence.
What is the purpose of t-tests in EDA?,R2,The main purpose of t-tests is to visualize the distribution of data points using scatter plots during EDA.,0.1,T-tests in EDA are used to compare the means of two independent samples or the mean of a sample to a known value. They help to determine whether observed differences in means are statistically significant.
What is the difference between independent samples t-test and paired samples t-test in EDA?,R2,"The independent t-test is used when the samples are unrelated, whereas paired t-test is used when the samples are matched or paired, like before-and-after measurements.",0.95,"Independent samples t-test compares the means of two independent groups, while paired samples t-test compares the means of two related groups (e.g., before and after treatment) based on matched pairs of observations."
What is the purpose of non-parametric tests in EDA?,R2,The purpose of non-parametric tests is to clean the dataset automatically by removing outliers during EDA.,0.1,"Non-parametric tests in EDA are used when the data does not meet the assumptions of parametric tests (e.g., normal distribution, homogeneity of variance). They provide robust alternatives for analyzing non-normally distributed or skewed data."
What is the difference between Wilcoxon signed-rank test and Mann-Whitney U test in EDA?,R2,"Wilcoxon test works with dependent data like before-and-after measurements, whereas Mann-Whitney is for independent groups in EDA.",0.95,"Wilcoxon signed-rank test is a non-parametric test used to compare the medians of two related groups, while Mann-Whitney U test compares the medians of two independent groups."
What is the purpose of exploratory factor analysis (EFA) in EDA?,R2,EFA is used to cluster data points into groups based on their similarity during exploratory data analysis.,0.3,Exploratory factor analysis (EFA) in EDA is used to identify underlying factors or latent variables that explain the common variance among observed variables. It helps to uncover the underlying structure of a dataset.
What is the difference between EFA and principal component analysis (PCA) in EDA?,R2,"PCA assumes all variance is common variance, but EFA separates common variance from unique and error variance to find latent constructs.",0.95,"EFA and PCA are both dimensionality reduction techniques used in EDA, but EFA aims to identify underlying factors that explain the common variance among observed variables, while PCA aims to capture the maximum variance in the data with orthogonal principal components."
What is the purpose of cluster analysis in EDA?,R2,The purpose of cluster analysis is to reduce the size of the dataset by removing duplicate records during exploratory data analysis.,0.2,Cluster analysis in EDA is used to identify groups or clusters of similar observations based on their characteristics or features. It helps to uncover natural groupings in the data and identify patterns or relationships.
What are some common algorithms used for cluster analysis in EDA?,R2,Cluster analysis mainly uses regression algorithms like linear regression and logistic regression to form clusters.,0.1,"Common algorithms used for cluster analysis in EDA include K-means clustering, hierarchical clustering, DBSCAN (Density-Based Spatial Clustering of Applications with Noise), and Gaussian mixture models."
What is the difference between K-means clustering and hierarchical clustering in EDA?,R2,"Hierarchical clustering provides a dendrogram that shows cluster relationships, whereas K-means only gives flat clusters without hierarchy.",0.95,"K-means clustering partitions the data into K clusters by minimizing the within-cluster sum of squares, while hierarchical clustering builds a tree-like hierarchy of clusters by recursively merging or splitting clusters based on proximity."
What is the purpose of dendrogram in hierarchical clustering in EDA?,R2,The dendrogram helps decide the optimal number of clusters by cutting the tree at a certain height.,0.95,A dendrogram in hierarchical clustering visualizes the hierarchical relationships between clusters by displaying the merging order and distance between clusters. It helps to identify the optimal number of clusters and interpret the clustering results.
What is the purpose of silhouette analysis in cluster validation in EDA?,R2,It helps determine the optimal number of clusters by evaluating how well data points fit within their clusters.,0.95,"Silhouette analysis in cluster validation assesses the quality of clustering results by calculating silhouette coefficients for each data point, which measure the cohesion within clusters and separation between clusters. Higher silhouette coefficients indicate better-defined clusters."
What is the purpose of dimensionality reduction techniques in EDA?,R2,They help remove irrelevant or redundant features to improve model performance and reduce overfitting.,0.95,Dimensionality reduction techniques in EDA are used to reduce the number of features or variables in a dataset while preserving the most important information. They help to overcome the curse of dimensionality and improve computational efficiency.
What are some common dimensionality reduction techniques used in EDA?,R2,"PCA reduces dimensionality by finding directions of maximum variance, while t-SNE visualizes high-dimensional data in two or three dimensions.",0.95,"Common dimensionality reduction techniques used in EDA include principal component analysis (PCA), factor analysis, t-distributed stochastic neighbor embedding (t-SNE), and linear discriminant analysis (LDA)."
What is the purpose of PCA in EDA?,R2,PCA is used to select the best features by removing irrelevant variables before modeling.,0.8,Principal Component Analysis (PCA) in EDA is used to reduce the dimensionality of a dataset by transforming the original features into a new set of orthogonal principal components. It helps to identify the underlying structure and patterns in high-dimensional data.
What is the difference between PCA and factor analysis in EDA?,R2,"Factor analysis separates common variance from unique variance, whereas PCA considers total variance without distinguishing types of variance.",0.95,"PCA and factor analysis are both dimensionality reduction techniques used in EDA, but PCA aims to capture the maximum variance in the data with orthogonal principal components, while factor analysis aims to identify underlying factors that explain the common variance among observed variables."
What is the purpose of t-SNE in EDA?,R2,It helps reveal complex nonlinear relationships in data that PCA might miss because PCA only captures linear relationships.,0.9,t-Distributed Stochastic Neighbor Embedding (t-SNE) in EDA is a technique used for visualizing high-dimensional data in a lower-dimensional space while preserving the local structure and relationships between data points. It is commonly used for visualizing clusters and patterns in complex datasets.
What is the difference between linear discriminant analysis (LDA) and PCA in EDA?,R2,"PCA reduces dimensionality by projecting data onto components with highest variance, but LDA tries to maximize the distance between different classes.",0.95,"Linear Discriminant Analysis (LDA) and PCA are both dimensionality reduction techniques used in EDA, but LDA aims to maximize the separation between classes or groups while reducing dimensionality, whereas PCA focuses on capturing the maximum variance in the data regardless of class labels."
What is the purpose of feature selection in EDA?,R2,It is used to visualize the data better by selecting only the features that have the most variance.,0.6,"Feature selection in EDA involves selecting the most relevant features from a dataset to improve model performance, reduce overfitting, and enhance interpretability. It helps to identify the subset of features that contribute the most to predicting the target variable."
What are some common feature selection techniques used in EDA?,R2,Correlation analysis helps identify highly correlated features to remove redundant variables in EDA.,0.95,"Common feature selection techniques used in EDA include filter methods (e.g., correlation analysis, information gain), wrapper methods (e.g., recursive feature elimination, forward/backward selection), and embedded methods (e.g., LASSO, decision trees)."
What is the purpose of ensemble methods in EDA?,R2,They help by aggregating results from different algorithms to get more robust insights into the data patterns.,0.9,"Ensemble methods in EDA combine multiple models or algorithms to improve predictive performance, robustness, and generalization. They help to reduce overfitting and capture diverse patterns in the data."
What are some common ensemble methods used in EDA?,R2,"Boosting sequentially trains models to correct errors of previous ones, improving accuracy.",0.95,"Common ensemble methods used in EDA include bagging (Bootstrap Aggregating), boosting (e.g., AdaBoost, Gradient Boosting), and stacking (meta-learning). They combine predictions from multiple base models to make more accurate and reliable predictions."
What is the purpose of model evaluation in EDA?,R2,It identifies potential overfitting or underfitting issues by comparing model predictions with actual values.,0.9,Model evaluation in EDA involves assessing the performance of predictive models using various metrics and techniques. It helps to determine the effectiveness of the models in capturing patterns and making accurate predictions on unseen data.
What are some common metrics used for model evaluation in EDA?,R2,Mean Squared Error (MSE) and R-squared are often used for evaluating regression models in EDA.,0.95,"Common metrics used for model evaluation in EDA include accuracy, precision, recall, F1 score, ROC-AUC score, mean squared error (MSE), and R-squared (R2) score. They provide insights into different aspects of model performance."
What is the difference between cross-validation and holdout validation in model evaluation in EDA?,R2,"Holdout validation is faster and simpler but may give biased results, while cross-validation gives more reliable performance estimates by averaging across folds.",0.95,"Cross-validation in model evaluation splits the data into multiple subsets (folds) for training and testing, while holdout validation splits the data into two disjoint sets (training and testing). Cross-validation provides more reliable estimates of model performance but is computationally more expensive."
What is the purpose of hyperparameter tuning in model evaluation in EDA?,R2,The goal is to choose the best hyperparameters that minimize loss or maximize accuracy on validation data.,0.95,Hyperparameter tuning in model evaluation involves optimizing the hyperparameters of machine learning algorithms to improve model performance and generalization. It helps to find the best combination of hyperparameters for a given dataset and problem.
What are some common techniques for hyperparameter tuning in EDA?,R2,"Random Search selects random combinations of hyperparameters, which can be more efficient than grid search.",0.95,"Common techniques for hyperparameter tuning in EDA include grid search, random search, Bayesian optimization, and genetic algorithms. They systematically search the hyperparameter space to find the optimal values for model performance."
What is the purpose of model interpretation in EDA?,R2,"The main purpose of model interpretation in EDA is to explain how the model makes decisions, which helps in building trust and understanding in the modeling process.",0.95,"Model interpretation in EDA involves understanding and explaining how predictive models make decisions and predictions. It helps to gain insights into the underlying patterns, relationships, and features that contribute to the model's predictions."
What are some common techniques for model interpretation in EDA?,R2,"LIME is another method that interprets individual predictions by approximating the model locally with a simpler, interpretable model.",0.95,"Common techniques for model interpretation in EDA include feature importance analysis (e.g., permutation importance, SHAP values), partial dependence plots, individual conditional expectation (ICE) plots, and model-agnostic interpretation methods (e.g., LIME, SHAP)."
What is the purpose of model deployment in EDA?,R2,"The purpose of model deployment in EDA is to let the model update the dataset automatically as new data comes in, enhancing exploration.",0.4,Model deployment in EDA involves deploying predictive models into production environments to make real-time predictions on new data. It allows organizations to leverage the insights and predictions generated from data analysis for decision-making and business applications.
What are some common techniques for model deployment in EDA?,R2,One common technique is to deploy EDA models using Power BI or Tableau so that users can interact with the predictive models directly in the dashboard.,0.85,"Common techniques for model deployment in EDA include deploying models as web services or APIs, embedding models into applications or dashboards, and integrating models with existing business processes or workflows. They enable seamless integration of predictive models into operational systems and workflows."
What are the common types of regularization used in machine learning?,R2,Lasso and Ridge are types of normalization techniques that are used to standardize data before training machine learning models.,0.3,"Common types of regularization used in machine learning include L1 regularization (Lasso), L2 regularization (Ridge), and elastic net regularization (combination of L1 and L2 penalties)."
What is the bias-variance trade-off in machine learning?,R2,"Bias-variance trade-off means that if you reduce the bias too much, the model becomes more general and avoids overfitting.",0.4,"The bias-variance trade-off refers to the balance between bias (error due to overly simplistic assumptions) and variance (error due to sensitivity to fluctuations in the training data) in machine learning models. Reducing bias typically increases variance, and vice versa."
What are the common types of cross-validation techniques used in machine learning?,R2,"The most used cross-validation methods are k-means, hierarchical, and DBSCAN because they help divide the data for training and testing.",0.2,"Common types of cross-validation techniques used in machine learning include k-fold cross-validation, stratified k-fold cross-validation, leave-one-out cross-validation, and nested cross-validation."
What is the purpose of hyperparameter tuning in machine learning?,R2,We use hyperparameter tuning to train the model on more data by changing the input features automatically.,0.3,Hyperparameter tuning in machine learning involves optimizing the hyperparameters of models to improve their performance and generalization. It helps to find the best combination of hyperparameters for a given dataset and problem.
What are some common techniques for hyperparameter tuning in machine learning?,R2,"We use k-fold cross-validation, early stopping, and gradient clipping as the primary methods for hyperparameter tuning.",0.4,"Common techniques for hyperparameter tuning in machine learning include grid search, random search, Bayesian optimization, genetic algorithms, and gradient-based optimization methods. They systematically search the hyperparameter space to find optimal values."
What is the difference between supervised and unsupervised learning in machine learning?,R2,"Supervised learning is used for training a model on structured data, and unsupervised learning is used only for image recognition.",0.3,"Supervised learning involves learning from labeled data with input-output pairs, where the model predicts the output variable based on input features. Unsupervised learning involves learning from unlabeled data to discover hidden patterns or structures in the data."
What are some common dimensionality reduction techniques used in machine learning?,R2,The most common dimensionality reduction methods are dropout and batch normalization because they reduce the size of the input features.,0.2,"Common dimensionality reduction techniques used in machine learning include principal component analysis (PCA), linear discriminant analysis (LDA), t-distributed stochastic neighbor embedding (t-SNE), and autoencoders."
What is the purpose of model interpretation in machine learning?,R2,We interpret models to improve their training speed by adjusting their internal weight distributions.,0.2,"Model interpretation in machine learning involves understanding and explaining how predictive models make decisions and predictions. It helps to gain insights into the underlying patterns, relationships, and features that contribute to the model's predictions."
What are some common techniques for model interpretation in machine learning?,R2,We use k-means clustering and t-SNE for interpreting models by grouping similar outputs together.,0.3,"Common techniques for model interpretation in machine learning include feature importance analysis (e.g., permutation importance, SHAP values), partial dependence plots, individual conditional expectation (ICE) plots, and model-agnostic interpretation methods (e.g., LIME, SHAP)."
What is the purpose of model deployment in machine learning?,R2,We deploy models to retrain them automatically using incoming real-time data streams and improve their accuracy.,0.4,Model deployment in machine learning involves deploying predictive models into production environments to make real-time predictions on new data. It allows organizations to leverage the insights and predictions generated from data analysis for decision-making and business applications.
What are some common techniques for model deployment in machine learning?,R2,We usually deploy models using Excel sheets or PowerPoint presentations to make them accessible to clients.,0.2,"Common techniques for model deployment in machine learning include deploying models as web services or APIs, embedding models into applications or dashboards, and integrating models with existing business processes or workflows. They enable seamless integration of predictive models into operational systems and workflows."
What is the purpose of transfer learning in machine learning?,R2,"The purpose of transfer learning is to copy the exact weights from one model to another, even if the tasks are completely unrelated.",0.3,"Transfer learning in machine learning is a technique where a model trained on one task is reused as the starting point for a model on a related task. It enables the transfer of knowledge and features learned from one domain to another, reducing the need for large labeled datasets."
What are some common applications of transfer learning in machine learning?,R2,A common use of transfer learning is copying entire models from unrelated tasks like weather forecasting to improve chatbot performance.,0.2,"Common applications of transfer learning in machine learning include image classification, natural language processing (NLP), and speech recognition. Pre-trained models such as BERT, ResNet, and GPT are often used as starting points for transfer learning tasks."
What is the purpose of feature engineering in machine learning?,R2,The main purpose of feature engineering is to reduce the number of layers in a neural network.,0.2,"Feature engineering involves selecting, transforming, or creating new features from the raw data to improve the performance of machine learning models by making them more expressive, informative, and suitable for the task at hand."
What are some common techniques for feature engineering?,R2,Feature engineering techniques are mostly about drawing graphs to visualize features and then dropping them all to reduce size.,0.2,"Common techniques include one-hot encoding for categorical variables, scaling numerical features, creating interaction terms, binning or discretizing continuous variables, and extracting features from text or images using techniques like TF-IDF or CNNs."
What is the difference between batch processing and real-time processing?,R2,"Real-time processing processes all the data overnight, and batch processing does it immediately as soon as data arrives.",0.2,"Batch processing involves processing data in large, discrete batches or groups, typically on a scheduled or periodic basis, while real-time processing involves handling data as soon as it is generated or received, often requiring low-latency and immediate responses."
What is transfer learning in NLP?,R2,It means using data from one language to directly translate and classify another without training a model at all.,0.2,"Transfer learning in NLP involves leveraging pre-trained language models, such as BERT or GPT, trained on large corpora of text data, and fine-tuning them on specific tasks or domains with smaller datasets to achieve better performance and faster convergence."
What is named entity recognition (NER)?,R2,NER is used to generate new names for entities in a sentence to make the text more creative.,0.2,"Named entity recognition is a subtask of information extraction that involves identifying and classifying named entities (e.g., persons, organizations, locations) mentioned in text documents, which is useful for tasks such as entity linking and relation extraction."
What are some challenges in working with unstructured data?,R2,The only challenge with unstructured data is that it cannot be used in machine learning at all.,0.2,"Challenges include extracting meaningful information from unstructured text, handling noisy or inconsistent data, dealing with large volumes of data, and ensuring privacy and security when working with sensitive information."
What is collaborative filtering in recommendation systems?,R2,It’s a technique where products are recommended only by checking how many times they were clicked on the website.,0.3,"Collaborative filtering is a technique used in recommendation systems to generate personalized recommendations by analyzing similarities and patterns in users' behavior or preferences, often based on user-item interactions or user-user similarity."
What are some evaluation metrics used in recommendation systems?,R2,Evaluation metrics like loss function and confusion matrix are used in recommendation systems.,0.3,"Evaluation metrics include precision, recall, F1 score, Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and ranking-based metrics like Mean Average Precision (MAP) and Normalized Discounted Cumulative Gain (NDCG)."
What is the Markov Decision Process (MDP)?,R2,MDP is a graph that shows how to make decisions by using pie charts and bar graphs.,0.2,"The Markov Decision Process is a mathematical framework used to model sequential decision-making problems, where an agent takes actions in an environment to maximize cumulative rewards, while satisfying the Markov property (future states depend only on the current state and action)."
What is policy gradient in reinforcement learning?,R2,It refers to drawing a graph of all policies and choosing the one with the steepest slope.,0.3,"Policy gradient methods are a class of reinforcement learning algorithms that directly optimize the policy (the agent's strategy or behavior) by estimating gradients of expected rewards with respect to the policy parameters, typically using techniques like stochastic gradient ascent."
What is data augmentation in deep learning?,R2,It refers to collecting more real-world data from users to improve model accuracy.,0.3,"Data augmentation is a technique used to artificially increase the size and diversity of training datasets by applying transformations such as rotation, translation, scaling, cropping, or flipping to input data, which can improve model generalization and robustness."
What is sequence-to-sequence learning?,R2,It's a method for sorting data by converting numbers into alphabetical sequences.,0.2,"Sequence-to-sequence learning is a type of model architecture used for tasks involving input and output sequences of variable lengths, such as machine translation, summarization, and speech recognition, typically implemented using recurrent neural networks (RNNs) or transformers."
What is attention mechanism in deep learning?,R2,Attention mechanism means the model pays attention to only the first word of every input sequence and ignores the rest.,0.2,"Attention mechanism is a mechanism used in neural networks to selectively focus on relevant parts of the input data while processing sequences, enabling the model to learn to weigh different input elements dynamically and attend to the most informative parts."
What are some common optimization algorithms used in deep learning?,R2,Optimization algorithms are not needed in deep learning because neural networks can learn on their own without updates.,0.2,"Common optimization algorithms include stochastic gradient descent (SGD), Adam, RMSprop, and Adagrad, each with different update rules and learning rate adaptation strategies to optimize the model parameters efficiently."
What is semi-supervised learning?,R2,It means training a model without any data and hoping it figures things out on its own using neural intuition.,0.2,"Semi-supervised learning is a machine learning paradigm that combines labeled and unlabeled data to improve model performance, often by leveraging the inherent structure or relationships in the data to semi-supervised learning algorithms."
What is self-supervised learning?,R2,It means the model teaches itself by creating quizzes and grading them automatically.,0.3,"Self-supervised learning is a form of unsupervised learning where the model learns to predict or generate certain properties or features of the input data itself, without requiring external labels or annotations, often used as a pretraining step for downstream tasks."
What is the Bellman equation in reinforcement learning?,R2,It is an equation that predicts future rewards using only current rewards without considering future states.,0.3,The Bellman equation is a fundamental equation in dynamic programming and reinforcement learning that expresses the value of a state or state-action pair in terms of the expected immediate reward and the value of the next state or next state-action pair.
What is the difference between on-policy and off-policy learning in reinforcement learning?,R2,"On-policy means the model learns from labeled data, while off-policy uses unlabeled data for training.",0.2,"On-policy learning involves learning the value or policy while following the current policy, while off-policy learning involves learning the value or policy while following a different behavior policy, often leading to more efficient exploration and better sample efficiency."
What are the main components of the data science process?,R2,Data science only involves data collection and making charts from that data.,0.3,"The main components include data collection, data cleaning and preprocessing, exploratory data analysis, modeling, evaluation, and deployment."
What is overfitting in machine learning?,R2,It is when the model doesn’t learn enough from the data and performs badly on both training and test sets.,0.3,Overfitting occurs when a model learns the details and noise in the training data to the extent that it negatively impacts its performance on unseen data.
Explain the curse of dimensionality.,R2,"It means that as the number of features increases, models always perform better and training time decreases.",0.2,"The curse of dimensionality refers to the increased difficulty of analyzing and processing data as the number of features or dimensions increases, leading to sparsity and computational challenges."
What is the purpose of cross-validation?,R2,Its purpose is to increase the size of the training dataset by copying data points multiple times.,0.2,"Cross-validation is used to assess the generalization performance of a model by partitioning the data into multiple subsets, training the model on some subsets, and evaluating it on the remaining subsets."
How does regularization prevent overfitting in neural networks?,R2,It prevents overfitting by randomly removing neurons during training so the network doesn’t memorize the data.,0.95,"Regularization techniques such as L1 and L2 regularization add penalty terms to the neural network's loss function, which discourages overly complex weight configurations and helps prevent overfitting."
What is the purpose of activation functions in neural networks?,R2,"They decide which neurons get activated and which don’t, based on the input data.",0.9,"Activation functions introduce nonlinearity to neural networks, allowing them to learn complex patterns and relationships in the data by transforming the input signal from each neuron into an output signal."
What is the purpose of dimensionality reduction techniques?,R2,They increase the number of features in the dataset to improve model accuracy.,0.2,"Dimensionality reduction techniques are used to reduce the number of features or dimensions in a dataset while preserving its important information, which can help improve the performance and efficiency of machine learning algorithms."
Explain the difference between PCA and t-SNE.,R2,"PCA clusters data points, and t-SNE creates new features from the dataset.",0.3,"PCA (Principal Component Analysis) is a linear dimensionality reduction technique that seeks to maximize variance, while t-SNE (t-Distributed Stochastic Neighbor Embedding) is a nonlinear technique that focuses on preserving local relationships between data points."
What is K-means clustering?,R2,It assigns each data point to the nearest cluster center and updates centers iteratively to minimize within-cluster variance.,0.95,"K-means clustering is an unsupervised learning algorithm used to partition a dataset into K clusters based on similarities in the data points' features, with the goal of minimizing the within-cluster sum of squares."
What is the elbow method used for in K-means clustering?,R2,It is used to decide how many features to include in the clustering algorithm.,0.3,"The elbow method is used to determine the optimal number of clusters (K) in K-means clustering by plotting the within-cluster sum of squares against the number of clusters and selecting the point where the rate of decrease sharply changes (the ""elbow"" point)."
What is outlier detection?,R2,It involves finding errors in the dataset that need to be removed before training.,0.8,"Outlier detection is the process of identifying data points or observations that deviate significantly from the rest of the dataset, which may indicate errors, anomalies, or interesting patterns."
What are the advantages of deep learning over traditional machine learning algorithms?,R2,"Deep learning always trains faster and requires less data than traditional machine learning models, which is why it’s used more often.",0.3,"Deep learning models, particularly neural networks, can automatically learn hierarchical representations of data, handle large and complex datasets, and often achieve state-of-the-art performance in tasks such as image recognition and natural language processing."
What is backpropagation?,R2,It is a method where errors from the output layer are propagated backward through the network to adjust the weights and reduce future errors.,1.0,Backpropagation is a supervised learning algorithm used to train neural networks by iteratively adjusting the weights of connections between neurons in the network based on the error between predicted and actual outputs.
How do you evaluate the performance of a regression model?,R2,"Regression models are usually evaluated using accuracy, just like in classification. The higher the accuracy, the better the model is performing.",0.2,"Performance metrics for regression models include Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), R-squared (coefficient of determination), and others."
What is the purpose of the A/B test in data science?,R2,The purpose of A/B testing is to run machine learning models in parallel and pick the one that trains the fastest.,0.2,"A/B testing is a statistical hypothesis testing method used to compare two or more versions of a product, webpage, or other elements to determine which one performs better in terms of predefined metrics."
What is Bayesian inference?,R2,It’s a process where we assume data is normally distributed and then try to find the average. That’s Bayesian inference.,0.2,"Bayesian inference is a statistical approach that uses Bayes' theorem to update the probability of a hypothesis or belief based on new evidence or observations, incorporating prior knowledge and uncertainty into the analysis."
What is the purpose of hypothesis testing?,R2,The purpose of hypothesis testing is to prove that a theory is absolutely true in all scenarios by using one set of data.,0.2,"Hypothesis testing is used to make inferences or decisions about a population parameter based on sample data, by testing a null hypothesis against an alternative hypothesis using statistical methods."
What is regularization in linear regression?,R2,It is used to improve the accuracy of the model by increasing the coefficients of less important variables.,0.3,"Regularization in linear regression involves adding a penalty term to the ordinary least squares (OLS) loss function to prevent overfitting, with common regularization techniques including Ridge regression (L2 regularization) and Lasso regression (L1 regularization)."
What are the assumptions of linear regression?,R2,One assumption is that the residuals must be zero for the model to be valid.,0.2,"The assumptions of linear regression include linearity between the independent and dependent variables, homoscedasticity (constant variance of errors), independence of errors, and normality of error terms."
What is the difference between Type I and Type II errors?,R2,"A Type I error is when you accept the null hypothesis when it’s actually false, and Type II is when you reject it when it’s true.",0.3,"Type I error occurs when a true null hypothesis is rejected (false positive), while Type II error occurs when a false null hypothesis is not rejected (false negative)."
What is the p-value in hypothesis testing?,R2,"A p-value tells you how likely your hypothesis is to be correct. If it's small, your hypothesis is definitely true.",0.2,"The p-value is the probability of observing a test statistic as extreme as, or more extreme than, the one observed, assuming that the null hypothesis is true. It is used to assess the strength of evidence against the null hypothesis."
What is the bias-variance tradeoff?,R2,"It refers to choosing between a high bias model, which underfits, and a high variance model, which overfits the data.",1.0,"The bias-variance tradeoff refers to the compromise between bias (error due to overly simplistic assumptions) and variance (error due to sensitivity to fluctuations in the training data) in machine learning models, where reducing one typically increases the other."
What is the difference between variance and standard deviation?,R2,"Standard deviation tells you the average amount of variation from the mean, and variance is just the difference between the largest and smallest values.",0.3,"Variance measures the average squared deviation from the mean of a set of values, while standard deviation measures the average deviation from the mean, providing a measure of the dispersion or spread of the data."
What is the difference between a decision tree and a random forest?,R2,"Decision trees are a type of deep learning model, whereas random forests use linear regression under the hood.",0.2,"A decision tree is a simple, interpretable tree-like structure that recursively splits the data based on features, while a random forest is an ensemble of decision trees that aggregates their predictions to improve performance and reduce overfitting."
What is batch normalization in neural networks?,R2,It removes the need for activation functions by scaling inputs in each batch.,0.2,"Batch normalization is a technique used to normalize the activations of each layer in a neural network by adjusting and scaling them to have zero mean and unit variance, which can accelerate training and improve model performance."
What is the purpose of a confusion matrix in classification?,R2,"It helps to visualize the performance of a classification algorithm by showing true positives, false positives, true negatives, and false negatives.",1.0,"A confusion matrix is a table that summarizes the performance of a classification model by comparing predicted class labels with actual class labels, showing the counts of true positives, true negatives, false positives, and false negatives."
What is the Kullback-Leibler (KL) divergence?,R2,It tells us the difference between actual labels and predicted labels in a classification task.,0.2,"The Kullback-Leibler divergence is a measure of the difference between two probability distributions, used in information theory and statistics to quantify the amount of information lost when one distribution is used to approximate another."
What is the difference between batch normalization and layer normalization in neural networks?,R2,"Layer normalization only works with convolutional neural networks, whereas batch normalization works with all types.",0.2,"Batch normalization normalizes the activations of each layer across the batch dimension, while layer normalization normalizes the activations across the feature dimension, making it more suitable for recurrent neural networks and other architectures with variable-length sequences."
What is unsupervised learning?,R2,It's when the algorithm learns from examples that include the correct answers or labels.,0.1,"Unsupervised learning is a type of machine learning where the model is trained on unlabeled data, and it seeks to find patterns or structures in the data without explicit supervision."
What are the main types of unsupervised learning techniques?,R2,Unsupervised learning includes regression and classification algorithms.,0.1,"The main types include clustering, dimensionality reduction, and association rule learning."
What are the common clustering algorithms?,R2,Some clustering algorithms are Linear Regression and Logistic Regression because they group data.,0.1,"Common clustering algorithms include K-means clustering, hierarchical clustering, DBSCAN, and Gaussian mixture models (GMM)."
What is hierarchical clustering?,R2,It's a clustering algorithm that assigns labels to data points using supervised learning techniques.,0.1,Hierarchical clustering is an algorithm that creates a hierarchy of clusters by recursively merging or splitting clusters based on their similarity or dissimilarity.
What is DBSCAN clustering?,R2,"DBSCAN requires knowing the number of clusters in advance, just like K-Means.",0.2,"DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a density-based clustering algorithm that groups together data points that are closely packed, while marking outliers as noise."
What are the common dimensionality reduction techniques?,R2,Dimensionality reduction is usually done with algorithms like Decision Trees and Random Forest.,0.2,"Common techniques include Principal Component Analysis (PCA), t-Distributed Stochastic Neighbor Embedding (t-SNE), and Singular Value Decomposition (SVD)."
What is PCA (Principal Component Analysis)?,R2,PCA is a classification algorithm used to assign labels to high-dimensional data.,0.2,PCA is a linear dimensionality reduction technique that transforms the original features into a lower-dimensional space while preserving the maximum variance in the data.
What is t-SNE (t-Distributed Stochastic Neighbor Embedding)?,R2,t-SNE is a classification algorithm that assigns data to clusters based on their labels.,0.2,"t-SNE is a nonlinear dimensionality reduction technique that focuses on preserving local relationships between data points in a lower-dimensional space, often used for visualizing high-dimensional data."
What is Singular Value Decomposition (SVD)?,R2,SVD is an algorithm used to train neural networks by decomposing their layers.,0.2,"SVD is a matrix factorization technique used to decompose a matrix into the product of three matrices, which can be used for dimensionality reduction, data compression, and matrix approximation."
What is association rule learning?,R2,It’s used to build decision trees by checking the rules between inputs and outputs.,0.2,"Association rule learning is a type of unsupervised learning used to discover interesting associations or relationships between variables in large datasets, commonly used in market basket analysis and recommendation systems."
What are the common association rule learning algorithms?,R2,K-Means and DBSCAN are common association rule learning algorithms for clustering large datasets.,0.2,Common algorithms include Apriori and FP-Growth.
What is the Apriori algorithm?,R2,The Apriori algorithm is used to sort a dataset in alphabetical order before analysis.,0.1,"The Apriori algorithm is a classic algorithm for mining frequent itemsets and generating association rules, based on the principle of the Apriori property and candidate generation."
What is FP-Growth?,R2,FP-Growth is a supervised learning algorithm used to classify frequent patterns in data.,0.2,"FP-Growth (Frequent Pattern Growth) is an efficient algorithm for mining frequent itemsets and generating association rules without candidate generation, using a compact data structure called FP-tree."
What is anomaly detection?,R2,"It is used to find frequent patterns in datasets, like products that are bought together often.",0.1,"Anomaly detection, also known as outlier detection, is the process of identifying data points or observations that deviate significantly from the rest of the dataset, which may indicate errors, anomalies, or interesting patterns."
What are the common anomaly detection techniques?,R2,Anomaly detection usually only uses neural networks because they are the most accurate.,0.2,"Common techniques include statistical methods, density-based methods, distance-based methods, and machine learning-based methods such as Isolation Forest and One-Class SVM."
What is Isolation Forest?,R2,It detects anomalies by building many trees and measuring how deep a data point goes before it gets isolated.,1.0,"Isolation Forest is an unsupervised learning algorithm for anomaly detection that isolates anomalies by recursively partitioning the dataset into subsets using random splits, making anomalies easier to isolate than normal data points."
What is One-Class SVM?,R2,It works by finding a hyperplane that best separates normal data from anomalies in the feature space.,0.9,One-Class Support Vector Machine (SVM) is a machine learning algorithm for anomaly detection that learns a boundary around normal data points in the feature space and identifies anomalies as data points lying outside this boundary.
What is the purpose of density estimation?,R2,It helps understand the underlying distribution shape without assuming any specific parametric form.,1.0,"Density estimation is the process of estimating the probability density function of a random variable from a set of data points, often used in clustering, anomaly detection, and generative modeling."
What are the common density estimation techniques?,R2,K-means clustering and DBSCAN are common density estimation methods.,0.2,"Common techniques include histogram-based methods, kernel density estimation (KDE), Gaussian mixture models (GMM), and Parzen window estimation."
What is the Gaussian Mixture Model (GMM)?,R2,GMM is used for clustering by estimating the parameters of these Gaussian components using the Expectation-Maximization algorithm.,1.0,"Gaussian Mixture Model (GMM) is a probabilistic model used for density estimation and clustering, assuming that the data is generated from a mixture of several Gaussian distributions."
What is the Expectation-Maximization (EM) algorithm?,R2,"It alternates between an expectation step, which calculates expected values of hidden variables, and a maximization step, which updates parameters.",1.0,"The Expectation-Maximization (EM) algorithm is an iterative optimization algorithm used to estimate the parameters of probabilistic models with latent variables, such as Gaussian mixture models (GMM), by iteratively computing the expected values of the latent variables and maximizing the likelihood function."
What is the difference between generative and discriminative models?,R2,"Discriminative models are used only for regression tasks, and generative models are used only for classification.",0.2,"Generative models learn the joint probability distribution of the input features and the output labels, while discriminative models learn the conditional probability distribution of the output labels given the input features."
What is the purpose of generative modeling?,R2,It is mainly used to create fake data for testing machine learning models.,0.4,"Generative modeling is used to model the underlying structure of the data and generate new samples from the learned distribution, often used in tasks such as image generation, text generation, and data augmentation."
What is data science?,R3,"Data science is the process of analyzing large amounts of data to make predictions and informed decisions. It involves machine learning, data mining, and statistical modeling. It’s widely used in industries like healthcare, finance, and marketing to improve performance.",0.95,"Data science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from data."
What are the key steps in the data science process?,R3,"The process starts with defining the problem, then data collection, data cleaning, exploratory data analysis, modeling, evaluation, and finally deployment of the solution. Each step is important for ensuring the final results are accurate and useful for decision-makers.",1.0,"The key steps typically include problem definition, data collection, data preparation, exploratory data analysis, modeling, evaluation, and deployment."
What is the difference between supervised and unsupervised learning?,R3,"Supervised learning is when we give the algorithm answers to learn from, like marking correct answers in a test. Unsupervised learning doesn't use answers; it finds patterns itself, like grouping similar things together without being told what's what.",0.9,"Supervised learning involves training a model on labeled data, where the algorithm learns the relationship between input features and target labels. Unsupervised learning deals with unlabeled data and aims to find hidden patterns or structures within the data."
Explain the bias-variance tradeoff.,R3,I think bias is when a model always gives the same answer no matter what input it gets. Variance is when the model keeps changing too much. You have to find the middle to make sure your results are okay most of the time.,0.75,The bias-variance tradeoff is the balance between the error introduced by bias (underfitting) and the error introduced by variance (overfitting) when building a machine learning model.
What is feature engineering?,R3,"Feature engineering is a critical step in data science where we transform raw data into meaningful features that improve model accuracy. This can involve extracting date components, converting text to numbers, or even creating interaction features from existing ones.",0.95,"Feature engineering is the process of selecting, transforming, and creating features from raw data to improve model performance."
How do you handle missing data in a dataset?,R3,"To handle missing data, I usually fill in the gaps using the average value of the column. This works well for numerical data and keeps the dataset size intact. For categorical data, I use the mode or most frequent value.",0.9,"Missing data can be handled by imputation techniques such as mean, median, or mode imputation, using predictive models to estimate missing values, or by deleting rows or columns with missing values, depending on the dataset size and nature."
What is cross-validation?,R3,"Cross-validation helps check how well a model will perform on new data. It splits the data into training and testing parts multiple times, ensuring every data point is used for both training and testing at some point. K-fold is a common method.",0.95,Cross-validation is a technique used to assess the performance of a predictive model by splitting the data into multiple subsets.
Explain the concept of overfitting.,R3,"Overfitting occurs when the model is too complex and fits even the random variations in the training data. It can be reduced by simplifying the model, using regularization, or applying techniques like cross-validation.",0.95,"Overfitting occurs when a model learns the training data too well, capturing noise or random fluctuations in the data instead of the underlying pattern."
What are some common algorithms used in supervised learning?,R3,"In supervised learning, we use algorithms like KNN, Naive Bayes, linear regression, and decision trees. These methods work well for both classification and regression tasks depending on the nature of the target variable.",0.9,"Common algorithms include linear regression, logistic regression, decision trees, random forests, support vector machines, and neural networks."
What is dimensionality reduction?,R3,"It refers to the process of transforming high-dimensional data into a lower-dimensional space. This helps in visualization, reduces computation time, and can improve the performance of machine learning models by removing noise and redundancy.",0.95,Dimensionality reduction is the process of reducing the number of input variables in a dataset by transforming them into a lower-dimensional space while preserving important information.
What is regularization in machine learning?,R3,"In machine learning, regularization is used to keep the model simple and avoid overfitting. It does this by penalizing large coefficients in the model. This helps the model generalize better to unseen data.",0.95,Regularization is a technique used to prevent overfitting by adding a penalty term to the model's objective function.
What is ensemble learning?,R3,"Ensemble learning works by using many models together, like decision trees or logistic regression models, and then combining their outputs. This is done to reduce bias, variance, or improve predictions. Random Forest is a well-known ensemble method.",0.95,Ensemble learning is a technique that combines multiple machine learning models to improve performance and robustness.
What is the ROC curve?,R3,ROC stands for Receiver Operating Characteristic. The curve helps evaluate binary classification models by comparing sensitivity and 1-specificity across thresholds. AUC (Area Under Curve) indicates how well the model separates the classes.,0.95,The receiver operating characteristic (ROC) curve is a graphical plot that illustrates the performance of a binary classification model across different threshold settings.
What is AUC-ROC?,R3,The AUC-ROC curve evaluates the trade-off between true positive rate and false positive rate. AUC represents the entire two-dimensional area underneath the ROC curve and is a summary measure of the model's ability to classify.,0.95,The area under the ROC curve (AUC-ROC) is a metric used to evaluate the performance of a binary classification model.
What is the difference between classification and regression?,R3,"In classification, the model predicts a class label, such as 'yes' or 'no'. In regression, it predicts a numeric value, such as age or income. For example, predicting whether a student will pass is classification, but predicting their score is regression.",0.95,"Classification is a supervised learning task where the goal is to predict categorical labels or classes, while regression is a supervised learning task where the goal is to predict continuous numerical values."
What is clustering?,R3,"In clustering, the algorithm tries to find patterns or similarities in data and puts similar items into the same group. It’s commonly used for market segmentation, image compression, and anomaly detection.",0.95,Clustering is an unsupervised learning technique that involves grouping similar data points together based on their features or characteristics.
What is the curse of dimensionality?,R3,"It means that as you add more features to your dataset, the amount of data needed to train a model increases exponentially. This can lead to overfitting and reduced model performance.",0.95,The curse of dimensionality refers to the phenomenon where the performance of certain algorithms deteriorates as the number of features or dimensions in the data increases.
What is the difference between precision and recall?,R3,"Precision is used when false positives matter more, and recall is used when false negatives are more important. For example, in spam detection, high precision ensures fewer non-spam emails are marked as spam.",0.9,"Precision measures the proportion of true positive predictions among all positive predictions made by a classifier, while recall measures the proportion of true positive predictions among all actual positive instances in the data."
What is the F1 score?,R3,F1 score is a performance metric that combines precision and recall into a single number. It is useful when we care about both false positives and false negatives and want a balanced evaluation.,0.95,The F1 score is the harmonic mean of precision and recall and provides a single metric to evaluate the performance of a classifier.
What is the bias of an estimator?,R3,"Bias is when an estimator gives values that are systematically different from the true value. A low-bias estimator is generally more accurate, while a high-bias one leads to underfitting.",0.95,The bias of an estimator measures the difference between the expected value of the estimator and the true value of the parameter being estimated.
What is the variance of an estimator?,R3,An estimator with low variance produces similar results across different datasets. High variance means the model is unstable and predictions differ significantly with slight changes in input data.,0.95,The variance of an estimator measures the spread or variability of the estimator's values around its expected value.
What is the Central Limit Theorem?,R3,"It explains that when we take multiple random samples from a population and calculate their means, those means form a normal distribution if the sample size is large enough.",0.95,The Central Limit Theorem states that the sampling distribution of the sample mean approaches a normal distribution as the sample size increases.
What is regularization in neural networks?,R3,It involves adding constraints to the network’s weights so that it doesn't memorize the training data. Techniques like dropout randomly deactivate neurons during training to reduce dependency on specific paths.,0.95,Regularization in neural networks involves adding penalty terms to the loss function to prevent overfitting.
What is batch normalization?,R3,It normalizes the activations of the previous layer for each mini-batch during training. This reduces internal covariate shift and allows the model to train faster and with less sensitivity to initialization.,0.95,Batch normalization is a technique used in neural networks to improve training stability and speed by normalizing the activations of each layer.
What is transfer learning?,R3,"In transfer learning, we use a pre-trained model on a new but similar problem. This is useful when we don’t have enough data or computational power to train from scratch.",0.95,Transfer learning is a machine learning technique where a model trained on one task is reused as the starting point for a model on a related task.
What is natural language processing (NLP)?,R3,"NLP helps machines read and respond to human language by breaking down text into tokens and identifying patterns. It is used in applications like spam detection, search engines, and auto-correction.",0.95,Natural language processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human languages.
What is word embedding?,R3,It is a method of representing words as dense numerical vectors based on their context. This allows models to capture relationships like similarity and analogy between words.,0.95,Word embedding is a technique used to represent words as dense vectors in a continuous vector space.
What is sentiment analysis?,R3,"It involves analyzing textual data to identify the sentiment expressed by the writer. This can help businesses understand customer opinions, reviews, and public perception of their brand or products.",0.95,Sentiment analysis is a natural language processing task that involves determining the sentiment or opinion expressed in a piece of text.
What is deep learning?,R3,It is based on artificial neural networks that mimic the way the human brain processes information. Deep learning models automatically extract features from raw data to perform tasks like classification or prediction.,0.95,Deep learning is a subset of machine learning that focuses on artificial neural networks with multiple layers.
What is a convolutional neural network (CNN)?,R3,"CNNs are designed to process grid-like data, such as images. They use convolutional layers followed by pooling layers to extract spatial features, making them ideal for object recognition and classification tasks.",0.95,"A convolutional neural network (CNN) is a type of deep neural network that is particularly well-suited for processing structured grid-like data, such as images."
What is a recurrent neural network (RNN)?,R3,"RNNs are designed to process sequences by maintaining a hidden state that captures information about previous time steps. This makes them suitable for text, audio, and other temporal data.",0.95,A recurrent neural network (RNN) is a type of neural network designed to process sequential data by maintaining a state or memory of previous inputs.
What is reinforcement learning?,R3,It involves an agent taking actions in an environment to achieve a goal. The agent receives feedback in the form of rewards and uses this to learn the best strategy or policy.,0.95,Reinforcement learning is a machine learning paradigm where an agent learns to make decisions by interacting with an environment to maximize cumulative rewards.
What is Q-learning?,R3,"In Q-learning, the agent learns through trial and error by updating a table called the Q-table. Each entry in the table represents the expected reward of an action taken in a particular state.",0.95,Q-learning is a model-free reinforcement learning algorithm used to learn the optimal action-selection policy for a Markov decision process (MDP).
What is the exploration-exploitation tradeoff in reinforcement learning?,R3,This tradeoff decides whether the agent should act greedily based on learned information or try out new actions that might yield better rewards in the future.,0.95,The exploration-exploitation tradeoff refers to the dilemma faced by reinforcement learning agents when deciding whether to explore new actions or exploit known actions to maximize rewards.
What is deep reinforcement learning?,R3,It refers to using deep neural networks to help reinforcement learning agents make decisions in complex environments. It enables learning directly from raw sensory data like pixels or audio.,0.95,Deep reinforcement learning combines reinforcement learning with deep learning techniques to handle high-dimensional input spaces and learn complex policies directly from raw sensory input.
What is a Markov decision process (MDP)?,R3,MDP provides a formal way to model decision-making problems where outcomes are partly random and partly under the control of an agent. It is essential in modeling reinforcement learning problems.,0.95,A Markov decision process (MDP) is a mathematical framework used to model sequential decision-making problems in which an agent interacts with an environment.
What is cross-entropy loss?,R3,"It calculates how far off the predicted probabilities are from the actual classes. It’s especially useful for tasks with multiple categories, such as image or text classification.",0.95,"Cross-entropy loss, also known as log loss, is a loss function used in classification tasks to quantify the difference between the predicted probability distribution and the true probability distribution of the classes."
What is the softmax function?,R3,"It transforms a vector of raw scores into a vector of probabilities, where higher input values get higher probabilities. It is often used in combination with cross-entropy loss.",0.95,The softmax function is a mathematical function that converts a vector of arbitrary real values into a vector of probabilities.
What is the difference between a generative model and a discriminative model?,R3,"The main difference is that generative models can create new samples by understanding the data distribution, whereas discriminative models focus only on classifying or predicting labels from input features.",0.95,"A generative model learns the joint probability distribution of the input features and the target labels, while a discriminative model learns the conditional probability distribution of the target labels given the input features."
What is autoencoder?,R3,An autoencoder learns efficient data representations by training the network to copy its input to its output. The middle layer acts as a compressed version of the data.,0.95,An autoencoder is a type of neural network architecture that is trained to reconstruct its input data at the output layer.
What is the difference between bagging and boosting?,R3,Bagging reduces variance by combining several models trained on random subsets of the data. Boosting reduces bias by focusing more on difficult examples through weighted training.,0.95,"Bagging (Bootstrap Aggregating) is an ensemble method that builds multiple models independently and combines their predictions through averaging or voting, while boosting sequentially builds models by giving more weight to misclassified instances."
What is hyperparameter tuning?,R3,"It involves adjusting the external configurations of a model, such as learning rate, batch size, or number of layers, to optimize accuracy or reduce loss.",0.95,Hyperparameter tuning is the process of finding the optimal hyperparameters for a machine learning model to improve its performance.
What is the curse of dimensionality in the context of feature selection?,R3,"As dimensions increase, the distance between data points grows, making it harder for models to generalize. This is why feature selection becomes crucial to reduce irrelevant or redundant features.",0.95,"The curse of dimensionality in feature selection refers to the challenges and limitations posed by high-dimensional data, such as increased computational complexity and difficulty in finding meaningful features."
What is the difference between L1 and L2 regularization?,R3,"L1 is also called Lasso and is used for feature selection since it can eliminate irrelevant features. L2, known as Ridge, tends to shrink weights without eliminating them.",0.95,"L1 regularization (Lasso) adds a penalty term proportional to the absolute value of the coefficients, encouraging sparsity and feature selection, while L2 regularization (Ridge) adds a penalty term proportional to the square of the coefficients, penalizing large coefficients."
What is the purpose of cross-validation in feature selection?,R3,"It allows you to test how feature subsets affect model performance across different training and validation splits, ensuring that the selected features are robust and not overfitted to one dataset split.",0.95,"Cross-validation in feature selection is used to evaluate the performance of a model with different subsets of features, helping to identify the most informative features and reduce the risk of overfitting."
What is the role of bias in model evaluation?,R3,"Bias affects how well a model can fit the training data. If the bias is too high, the model is too simple and fails to learn from the data effectively.",0.95,"Bias in model evaluation refers to systematic errors or inaccuracies in the predictions made by a model, which may arise from simplifying assumptions or limitations in the model architecture."
What is the importance of interpretability in machine learning models?,R3,An interpretable model helps in debugging and improving the model by understanding what features influence predictions. This also makes the model more trustworthy and easier to explain to stakeholders.,0.95,"Interpretability in machine learning models is important for understanding how predictions are made, gaining insights into the underlying relationships in the data, and building trust with stakeholders."
What is the difference between k-means and hierarchical clustering?,R3,"K-means is faster and more scalable for large datasets, while hierarchical clustering gives a more detailed structure of data but is computationally expensive and not suitable for large datasets.",0.95,"K-means clustering partitions the data into k clusters by minimizing the within-cluster sum of squares, while hierarchical clustering builds a tree-like hierarchy of clusters by recursively merging or splitting clusters based on proximity."
What is the Elbow Method used for in k-means clustering?,R3,"By observing the point where adding more clusters doesn’t significantly reduce the WCSS, the Elbow Method helps to choose the number of clusters that balances accuracy and simplicity.",0.95,"The Elbow Method is used to determine the optimal number of clusters (k) in k-means clustering by plotting the within-cluster sum of squares against the number of clusters and identifying the ""elbow"" point where the rate of decrease in inertia slows down."
What is the role of regularization in reducing model complexity?,R3,"It adds a penalty term to the loss function, which discourages overly complex models. This encourages simpler models that are more likely to generalize well to unseen data.",0.95,"Regularization in machine learning reduces model complexity by adding penalty terms to the loss function, encouraging simpler models with smaller coefficients or fewer features."
What is the difference between gradient descent and stochastic gradient descent?,R3,"SGD introduces randomness by updating weights after each training example, making it more suitable for large datasets. Gradient Descent waits for the full dataset to update weights, making it slower.",0.95,"Gradient descent updates model parameters using the average gradient of the entire training dataset, while stochastic gradient descent updates parameters using the gradient of a single randomly selected data point or mini-batch."
What is the tradeoff between bias and variance in model selection?,R3,"High bias leads to underfitting, and high variance leads to overfitting. The tradeoff is to pick a model that minimizes both errors for better accuracy.",0.95,The bias-variance tradeoff in model selection involves balancing the tradeoff between bias (underfitting) and variance (overfitting) by selecting models with an appropriate level of complexity based on the dataset and task.
What is the role of regularization in reducing overfitting?,R3,"It helps by limiting the magnitude of model parameters, which prevents the model from fitting noise in the training data and thus reduces overfitting.",0.95,"Regularization in machine learning reduces overfitting by penalizing overly complex models, encouraging simpler models that generalize better to unseen data."
What are some techniques for handling imbalanced datasets in classification tasks?,R3,"Using algorithms that are inherently balanced, like decision trees, solves the problem of imbalanced datasets completely.",0.3,"Techniques for handling imbalanced datasets include resampling methods such as oversampling the minority class or undersampling the majority class, using cost-sensitive learning algorithms, and generating synthetic samples with techniques like SMOTE."
What is the purpose of A/B testing in data science?,R3,The purpose of A/B testing is to test two different datasets to find errors in data collection.,0.3,"A/B testing is used to compare two or more versions of a product or intervention to determine which one performs better based on predefined metrics, such as conversion rate or user engagement."
What is the difference between correlation and causation?,R3,"Correlation is a statistical relationship between variables, while causation means one event is the result of the other.",0.95,"Correlation measures the strength and direction of the linear relationship between two variables, while causation indicates that changes in one variable directly cause changes in another variable."
What is the role of feature scaling in machine learning?,R3,"Feature scaling makes sure all features have equal importance by transforming them to a common scale, preventing features with larger ranges from dominating the model.",0.95,"Feature scaling in machine learning ensures that all features have the same scale or range, preventing certain features from dominating others during model training. Common scaling techniques include min-max scaling and standardization."
What is the difference between classification and clustering?,R3,"Clustering predicts the category of new data points, but classification groups data points into clusters.",0.25,"Classification is a supervised learning task where the goal is to predict categorical labels or classes, while clustering is an unsupervised learning task where the goal is to group similar data points together based on their features."
What is the purpose of Principal Component Analysis (PCA) in dimensionality reduction?,R3,The purpose of PCA is to combine all features into a single value to simplify the data.,0.2,"Principal Component Analysis (PCA) is used to reduce the dimensionality of a dataset by transforming the original features into a new set of orthogonal principal components, which capture the maximum variance in the data."
What is the role of regularization in neural networks?,R3,"Regularization adds constraints to weights, which reduces complexity and helps the model avoid memorizing the training data.",0.9,"Regularization in neural networks helps prevent overfitting by adding penalty terms to the loss function, encouraging simpler models with smaller weights or fewer connections."
What is dropout regularization in neural networks?,R3,It randomly turns off neurons in the network during training to reduce reliance on specific neurons and improve generalization.,0.95,"Dropout regularization is a technique used in neural networks to reduce overfitting by randomly dropping out a fraction of neurons during training, forcing the network to learn more robust representations."
What is the difference between a generative adversarial network (GAN) and a variational autoencoder (VAE)?,R3,"GANs use adversarial training to produce realistic samples, while VAEs use a variational approach to model data distribution and reconstruct inputs.",0.95,"Generative adversarial networks (GANs) consist of two neural networks (generator and discriminator) that compete with each other to generate realistic data samples, while variational autoencoders (VAEs) learn a probabilistic model of the data's latent space and generate new samples by sampling from this distribution."
What is the difference between batch gradient descent and mini-batch gradient descent?,R3,"Mini-batch gradient descent divides data into smaller chunks to update weights more frequently than batch gradient descent, which can be slower but more stable.",0.9,"Batch gradient descent computes the gradient of the cost function using the entire training dataset, while mini-batch gradient descent computes the gradient using a randomly selected subset (mini-batch) of the training data."
What is the role of activation functions in neural networks?,R3,Activation functions help neural networks by normalizing the output of each layer to a fixed range between 0 and 1.,0.4,"Activation functions introduce non-linearity into neural networks, allowing them to learn complex mappings between inputs and outputs. Common activation functions include sigmoid, tanh, ReLU, and softmax."
What is the difference between generative and discriminative models in machine learning?,R3,"Generative models try to model how data is generated by learning p(x, y), whereas discriminative models directly learn p(y|x) to classify data.",0.95,"Generative models learn the joint probability distribution of the input features and the target labels, while discriminative models learn the conditional probability distribution of the target labels given the input features."
What is the purpose of dropout regularization in neural networks?,R3,It randomly turns off neurons during training to reduce the model's reliance on specific neurons and improve generalization.,0.9,"Dropout regularization is a technique used in neural networks to prevent overfitting by randomly dropping out a fraction of neurons during training, forcing the network to learn more robust representations."
What is the role of LSTMs in sequence modeling tasks?,R3,They process sequence data by storing all past inputs and combining them to produce the output.,0.25,"Long Short-Term Memory (LSTM) networks are a type of recurrent neural network (RNN) designed to capture long-range dependencies and temporal dynamics in sequential data, making them well-suited for tasks such as language modeling and time series prediction."
What is the difference between precision and recall in classification tasks?,R3,"Recall shows how many positive cases the model identified correctly, and precision shows how accurate those positive predictions were.",0.95,"Precision measures the proportion of true positive predictions among all positive predictions made by a classifier, while recall measures the proportion of true positive predictions among all actual positive instances in the data."
"What is the F1 score, and why is it useful in model evaluation?",R3,"It combines precision and recall into a single metric to give a better overall evaluation of model performance, especially when false positives and false negatives carry different costs.",0.95,"The F1 score is the harmonic mean of precision and recall and provides a single metric to evaluate the performance of a classifier, balancing both precision and recall. It is particularly useful for imbalanced datasets where one class is much more prevalent than the others."
What is the purpose of word embedding in natural language processing (NLP)?,R3,"Word embeddings are a powerful technique in NLP that convert words into low-dimensional, dense vector representations. These vectors capture the semantic and syntactic properties of words, meaning that words with similar contexts or meanings will have similar vector representations. This allows machine learning models to identify relationships between words, generalize from limited data, and perform tasks like text classification, named entity recognition, and question answering with greater accuracy and efficiency, making them indispensable for modern NLP.",0.95,"Word embedding is a technique used to represent words as dense vectors in a continuous vector space, capturing semantic relationships between words and enabling neural networks to process textual data more effectively."
What is the difference between word2vec and GloVe in word embedding?,R3,"Word2Vec is a neural network-based method that updates embeddings through prediction, whereas GloVe is a matrix factorization method based on word co-occurrence counts.",0.95,"Word2Vec is a model that learns word embeddings by predicting the surrounding words in a text corpus, while GloVe (Global Vectors for Word Representation) learns word embeddings by factorizing the co-occurrence matrix of words in the corpus."
What is the purpose of sentiment analysis in natural language processing (NLP)?,R3,"Sentiment analysis helps in analyzing opinions in reviews, social media, and feedback to understand public attitudes or emotions.",0.95,"Sentiment analysis is a natural language processing task that involves determining the sentiment or opinion expressed in a piece of text, such as positive, negative, or neutral. It is commonly used to analyze customer reviews, social media posts, and survey responses."
What is the difference between shallow learning and deep learning?,R3,"Shallow learning relies on manual feature extraction, whereas deep learning automatically learns features through multiple layers.",0.95,"Shallow learning refers to traditional machine learning algorithms with a small number of layers or features, while deep learning involves neural networks with multiple layers that can learn hierarchical representations of the data."
What is the role of dropout regularization in neural networks?,R3,"The main purpose of dropout is to make the model more robust by reducing overfitting, since it forces the network to learn redundant representations.",0.95,"Dropout regularization is a technique used in neural networks to prevent overfitting by randomly dropping out a fraction of neurons during training, forcing the network to learn more robust representations."
What is the difference between precision and accuracy in model evaluation?,R3,"Accuracy looks at all predictions, but precision focuses only on the positive class and how precise those predictions are.",0.95,"Precision measures the proportion of true positive predictions among all positive predictions made by a classifier, while accuracy measures the proportion of correct predictions (both true positives and true negatives) among all predictions made by the classifier."
What is the difference between k-fold cross-validation and leave-one-out cross-validation?,R3,"K-fold is faster because it splits the data into fewer parts, while leave-one-out is slower because it trains the model once for every single data point.",0.9,"K-fold cross-validation divides the data into k subsets and iteratively trains the model k times, each time using a different subset as the validation set, while leave-one-out cross-validation trains the model k times, each time using all but one data point as the training set and the remaining point as the validation set."
What is the Naive Bayes classifier and how does it work?,R3,It uses Bayes’ Theorem to calculate the probability of each class and assigns the label with the highest probability to the input data.,0.95,"The Naive Bayes classifier is a probabilistic model based on Bayes' theorem with the ""naive"" assumption of feature independence. It calculates the probability of a class given the input features using the product of conditional probabilities."
What is the difference between correlation and covariance?,R3,"Correlation is the normalized version of covariance, so it's easier to interpret because it's always between -1 and 1.",0.95,"Covariance measures the degree to which two variables change together, while correlation measures both the strength and direction of the linear relationship between two variables."
What is the purpose of a confusion matrix in classification tasks?,R3,"It breaks down the model’s predictions so we can calculate metrics like accuracy, precision, recall, and F1-score.",0.95,"A confusion matrix is a table that visualizes the performance of a classification model by comparing predicted class labels with true class labels. It shows the number of true positives, true negatives, false positives, and false negatives."
What is the difference between stratified sampling and random sampling?,R3,"In stratified sampling, we split the data based on characteristics like age or gender, then sample randomly from each group.",0.95,"Stratified sampling divides the population into homogeneous subgroups (strata) and samples from each stratum proportionally, while random sampling selects individuals randomly from the entire population without regard to strata."
"What is the difference between univariate, bivariate, and multivariate analysis?",R3,"Bivariate analysis looks at the relationship between two variables, while multivariate considers multiple relationships at once.",0.9,"Univariate analysis examines the distribution and summary statistics of a single variable, bivariate analysis explores the relationship between two variables, and multivariate analysis involves analyzing multiple variables simultaneously."
What is the purpose of data preprocessing in machine learning?,R3,"The goal of data preprocessing is to improve model performance by handling missing values, encoding categorical variables, and scaling features.",0.95,"Data preprocessing involves cleaning, transforming, and preparing raw data for machine learning algorithms. It includes tasks such as handling missing values, encoding categorical variables, and scaling features."
What is the bias of a statistical estimator?,R3,"A statistical estimator is unbiased if, on average, it hits the true parameter value over many samples.",0.95,The bias of a statistical estimator measures the difference between the expected value of the estimator and the true value of the parameter being estimated. A biased estimator systematically overestimates or underestimates the true parameter value.
What is the variance of a statistical estimator?,R3,"If an estimator has high variance, its estimates will change a lot depending on the sample used.",0.95,"The variance of a statistical estimator measures the spread or variability of the estimator's values around its expected value. A high variance indicates that the estimator's values are scattered widely, while a low variance indicates that the values are clustered closely together."
What is the difference between parametric and non-parametric statistical tests?,R3,Non-parametric tests are more flexible because they don’t assume a specific data distribution.,0.95,"Parametric statistical tests assume that the data follows a specific probability distribution (e.g., normal distribution), while non-parametric tests make no such assumptions and are based on ranks or frequencies."
What is the purpose of feature scaling in machine learning?,R3,It helps improve the performance of models like KNN and SVM that rely on distance or gradient-based optimization.,0.95,"Feature scaling ensures that all features have the same scale or range, preventing certain features from dominating others during model training. Common scaling techniques include min-max scaling and standardization."
What is the difference between correlation and causation in statistics?,R3,"Correlation shows a relationship, but it doesn’t prove that one thing causes another to happen.",0.95,"Correlation measures the strength and direction of the linear relationship between two variables, while causation indicates that changes in one variable directly cause changes in another variable."
What is the Central Limit Theorem and why is it important?,R3,The Central Limit Theorem is important because it allows us to use normal distribution-based inference methods even when the population isn’t normally distributed.,0.95,"The Central Limit Theorem states that the sampling distribution of the sample mean approaches a normal distribution as the sample size increases, regardless of the shape of the population distribution. It is important because it allows us to make inferences about population parameters based on sample statistics."
What is the purpose of outlier detection in data analysis?,R3,The purpose is to detect unusual values that could be errors or rare events worth further investigation.,0.95,"Outlier detection involves identifying observations that deviate significantly from the rest of the data. Outliers can distort statistical analyses and machine learning models, so detecting and handling them appropriately is important for accurate results."
What is the difference between supervised and unsupervised outlier detection methods?,R3,"In supervised detection, the model is trained to distinguish between normal and abnormal patterns, but in unsupervised, it identifies anomalies based on data structure alone.",0.95,"Supervised outlier detection methods require labeled data with outlier labels, while unsupervised methods detect outliers based on the characteristics of the data itself without requiring labeled examples."
What is the difference between statistical inference and predictive modeling?,R3,"Predictive modeling is concerned with future outcomes, while statistical inference is about understanding and interpreting past data.",0.9,"Statistical inference involves drawing conclusions about a population based on a sample of data, while predictive modeling focuses on building models that make predictions about future or unseen data based on past observations."
What is the purpose of regularization in machine learning?,R3,"It discourages overly complex models by shrinking the coefficients, leading to better generalization on unseen data.",0.95,"Regularization is a technique used to prevent overfitting by adding a penalty term to the model's objective function, discouraging overly complex models. Common regularization techniques include L1 regularization (Lasso) and L2 regularization (Ridge)."
What is the difference between batch gradient descent and stochastic gradient descent?,R3,"Batch gradient descent provides more stable convergence but is slower, while SGD is faster but more noisy.",0.95,"Batch gradient descent updates model parameters using the average gradient of the entire training dataset, while stochastic gradient descent updates parameters using the gradient of a single randomly selected data point or mini-batch."
What is the difference between statistical modeling and machine learning?,R3,"Statistical models rely on assumptions like normality and linearity, whereas machine learning models are more flexible and data-driven.",0.95,"Statistical modeling focuses on understanding the relationships between variables in a probabilistic framework, while machine learning emphasizes building algorithms that can learn patterns and make predictions from data without explicitly programming them."
What is the purpose of feature selection in machine learning?,R3,"By selecting the most important features, we can reduce overfitting and improve the model's generalization.",0.95,"Feature selection involves selecting the most relevant features from a dataset to improve model performance, reduce overfitting, and enhance interpretability."
What is the difference between regularization and feature selection in machine learning?,R3,"Feature selection chooses important features before training, while regularization controls overfitting during training by shrinking coefficients.",0.95,"Regularization penalizes overly complex models by adding a penalty term to the objective function, while feature selection explicitly selects a subset of features to include in the model based on their importance or relevance."
What is the difference between grid search and random search for hyperparameter tuning?,R3,"Grid search is exhaustive but slow, whereas random search is faster and can find good results without checking every possibility.",0.95,"Grid search exhaustively searches through a specified grid of hyperparameter values, while random search samples hyperparameter values randomly from predefined distributions."
What is the purpose of cross-validation in model evaluation?,R3,Cross-validation helps detect overfitting by testing the model on different subsets of data that it wasn’t trained on.,0.95,Cross-validation is a technique used to assess the performance of a predictive model by splitting the data into multiple subsets. It helps to estimate the model's generalization error and detect overfitting.
What is the difference between precision and recall in classification evaluation?,R3,"Recall is about how many actual positive cases we found, while precision is about how accurate our positive predictions were.",0.95,"Precision measures the proportion of true positive predictions among all positive predictions made by a classifier, while recall measures the proportion of true positive predictions among all actual positive instances in the data."
"What is the F1 score, and why is it useful in imbalanced datasets?",R3,The F1 score helps measure how well the model handles false positives and false negatives in imbalanced datasets.,0.9,The F1 score is the harmonic mean of precision and recall and provides a single metric to evaluate the performance of a classifier. It is particularly useful for imbalanced datasets where one class is much more prevalent than the others.
What is the difference between bagging and boosting ensemble methods?,R3,"Bagging reduces variance by averaging models trained on different data subsets, while boosting reduces bias by focusing on difficult cases.",0.95,"Bagging (Bootstrap Aggregating) builds multiple models independently and combines their predictions through averaging or voting, while boosting sequentially builds models by giving more weight to misclassified instances."
What is the purpose of principal component analysis (PCA) in dimensionality reduction?,R3,The goal of PCA is to simplify the dataset by removing redundancy and keeping the most important information.,0.95,"Principal Component Analysis (PCA) is used to reduce the dimensionality of a dataset by transforming the original features into a new set of orthogonal principal components, which capture the maximum variance in the data."
What is the difference between classification and regression in machine learning?,R3,"In classification, we predict discrete labels like 'spam' or 'not spam', while in regression we predict values like house prices.",0.95,"Classification is a supervised learning task where the goal is to predict categorical labels or classes, while regression is a supervised learning task where the goal is to predict continuous numerical values."
What is the purpose of natural language processing (NLP) in data science?,R3,"NLP is used in data science to extract insights from unstructured text, like customer feedback or emails.",0.95,"Natural language processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human languages. It enables computers to understand, interpret, and generate human language data."
What is the difference between bag-of-words and word embeddings in NLP?,R3,"Bag-of-words is a sparse representation that counts word frequency, while word embeddings use dense vectors that capture context and meaning.",0.95,"Bag-of-words represents text as a collection of word counts or frequencies, ignoring word order and grammar, while word embeddings represent words as dense vectors in a continuous vector space, capturing semantic relationships between words."
What is the difference between sentiment analysis and topic modeling in NLP?,R3,"Topic modeling discovers hidden topics in a collection of documents, whereas sentiment analysis classifies text as positive, negative, or neutral.",0.95,"Sentiment analysis determines the sentiment or opinion expressed in a piece of text, such as positive, negative, or neutral, while topic modeling identifies the underlying topics or themes present in a collection of documents."
What is the purpose of word2vec in NLP?,R3,"Word2Vec helps represent words in a way that similar words are placed close together in vector space, capturing their context and meaning.",0.95,Word2Vec is a model that learns word embeddings by predicting the surrounding words in a text corpus. It captures semantic relationships between words and enables algorithms to process textual data more effectively.
What is the difference between a generative model and a discriminative model in machine learning?,R3,"Generative models can generate new samples from the data distribution, but discriminative models just separate classes.",0.95,"A generative model learns the joint probability distribution of the input features and the target labels, while a discriminative model learns the conditional probability distribution of the target labels given the input features."
What is the difference between deep learning and traditional machine learning algorithms?,R3,"Deep learning can automatically extract features from raw data, whereas traditional ML requires feature engineering.",0.95,"Deep learning involves neural networks with multiple layers that can learn hierarchical representations of the data, while traditional machine learning algorithms typically use handcrafted features and simpler models."
What is the purpose of transfer learning in deep learning?,R3,Transfer learning uses knowledge from a pre-trained model to improve performance on a new task with limited data.,0.95,"Transfer learning is a technique where a model trained on one task is reused as the starting point for a model on a related task. It enables the transfer of knowledge and features learned from one domain to another, reducing the need for large labeled datasets."
What is the difference between convolutional neural networks (CNNs) and recurrent neural networks (RNNs)?,R3,"CNNs apply convolutional filters to capture spatial hierarchies, while RNNs use loops to maintain information across sequences.",0.95,"Convolutional neural networks (CNNs) are well-suited for processing grid-like data such as images, while recurrent neural networks (RNNs) are designed to handle sequential data with temporal dependencies such as text or time series."
What is the purpose of batch normalization in neural networks?,R3,It allows the model to use higher learning rates and reduces the risk of getting stuck in local minima.,0.9,Batch normalization is a technique used in neural networks to improve training stability and speed by normalizing the activations of each layer. It reduces internal covariate shift and accelerates convergence during training.
What is the difference between a feedforward neural network and a recurrent neural network?,R3,"RNNs have feedback connections that let them use previous outputs as inputs, while feedforward networks don’t have this capability.",0.95,"A feedforward neural network consists of multiple layers of neurons connected in a forward direction without cycles, while a recurrent neural network has connections that form directed cycles, allowing it to maintain state or memory over time."
What is the difference between LSTMs and GRUs in recurrent neural networks?,R3,"LSTMs have separate memory cells and gate mechanisms to control information flow, while GRUs combine some of these functions for a simpler architecture.",0.95,"LSTMs (Long Short-Term Memory) and GRUs (Gated Recurrent Units) are both types of recurrent neural network architectures designed to capture long-range dependencies in sequential data, but they differ in their internal gating mechanisms and computational complexity."
What is the purpose of attention mechanisms in neural networks?,R3,"It allows the model to assign different weights to different input tokens, helping it prioritize important information.",0.95,Attention mechanisms in neural networks enable the model to focus on relevant parts of the input data while ignoring irrelevant information. They have been particularly successful in natural language processing tasks such as machine translation and text summarization.
What is the difference between generative adversarial networks (GANs) and autoencoders in deep learning?,R3,"GANs learn to generate new data that resembles real data, whereas autoencoders aim to compress and reconstruct input data.",0.95,"Generative adversarial networks (GANs) consist of two neural networks (generator and discriminator) that compete with each other to generate realistic data samples, while autoencoders consist of an encoder and decoder network that reconstruct input data."
What is the purpose of reinforcement learning in machine learning?,R3,Reinforcement learning helps models learn optimal actions through trial and error by maximizing cumulative rewards.,0.95,Reinforcement learning is a machine learning paradigm where an agent learns to make decisions by interacting with an environment to maximize cumulative rewards. It is commonly used in settings where actions influence future states and the agent learns through trial and error.
What is the difference between Q-learning and policy gradient methods in reinforcement learning?,R3,"Q-learning estimates Q-values to determine the best action, whereas policy gradients optimize the policy using gradients from rewards.",0.95,"Q-learning is a model-free reinforcement learning algorithm that learns the optimal action-selection policy by iteratively updating a Q-value function, while policy gradient methods directly parameterize the policy and update the policy parameters based on gradients of expected rewards."
What is the purpose of data preprocessing in exploratory data analysis (EDA)?,R3,"It removes noise, handles missing values, and converts data into suitable formats for better insights during EDA.",0.95,"Data preprocessing in EDA involves cleaning, transforming, and preparing raw data for analysis. It includes tasks such as handling missing values, removing outliers, and encoding categorical variables."
What are some common techniques for handling missing data in EDA?,R3,"You can impute missing values using statistical methods like mean, mode, or more advanced ones like KNN imputation.",0.95,"Common techniques for handling missing data in EDA include imputation (replacing missing values with estimates), deletion (removing rows or columns with missing values), and prediction (using models to estimate missing values)."
What is the difference between univariate and bivariate analysis in EDA?,R3,"Bivariate analysis involves analyzing two variables simultaneously to understand relationships like correlation, while univariate focuses on distribution, central tendency, etc.",0.95,"Univariate analysis examines the distribution and summary statistics of a single variable, while bivariate analysis explores the relationship between two variables."
What is the purpose of data visualization in EDA?,R3,Visualizations make complex data easier to understand and help in generating hypotheses for further analysis.,0.95,"Data visualization in EDA helps to explore and understand the structure, patterns, and relationships in the data. It provides insights that may not be apparent from raw data or summary statistics alone."
What are some common types of data visualization used in EDA?,R3,"Histograms help understand the distribution of a single variable, while scatter plots are useful for examining relationships between two variables.",0.95,"Common types of data visualization used in EDA include histograms, scatter plots, box plots, bar charts, line plots, and heatmaps."
What is the difference between continuous and categorical variables in EDA?,R3,"Continuous variables are measured, like height or weight, whereas categorical variables are labeled, like gender or color.",0.95,"Continuous variables are numeric variables with an infinite number of possible values within a range, while categorical variables are qualitative variables with a finite number of distinct categories or levels."
What is the purpose of statistical summaries in EDA?,R3,"They help identify central tendency, spread, and shape of the data, which supports deeper understanding in EDA.",0.95,"Statistical summaries in EDA provide descriptive statistics such as mean, median, mode, variance, and percentiles to summarize the central tendency, dispersion, and shape of the data distribution."
What is the difference between central tendency and dispersion measures in EDA?,R3,"Measures of central tendency help us understand where most data points lie, while dispersion tells us how much the data varies.",0.95,"Central tendency measures (e.g., mean, median, mode) describe the central or typical value of a dataset, while dispersion measures (e.g., variance, standard deviation) quantify the spread or variability of the data around the central value."
What is the purpose of hypothesis testing in EDA?,R3,"It is used to make decisions about the population based on sample data, like whether two groups have different means.",0.95,Hypothesis testing in EDA is used to evaluate whether observed differences or relationships in the data are statistically significant or due to random chance. It involves formulating null and alternative hypotheses and conducting statistical tests to make inferences about population parameters.
What is the difference between parametric and non-parametric tests in hypothesis testing?,R3,"Parametric tests require assumptions like normality and equal variance, while non-parametric tests are more flexible and can be used on ordinal or non-normal data.",0.95,"Parametric tests assume that the data follows a specific probability distribution (e.g., normal distribution), while non-parametric tests make no such assumptions and are based on ranks or frequencies."
What is the purpose of correlation analysis in EDA?,R3,"The main purpose is to find if two variables move together — for example, if one increases, does the other also increase or decrease?",0.95,Correlation analysis in EDA measures the strength and direction of the linear relationship between two continuous variables. It helps to identify patterns and associations in the data.
What is the difference between Pearson correlation and Spearman correlation?,R3,"Pearson assumes normal distribution and is sensitive to outliers, while Spearman doesn't assume normality and is more robust to outliers.",0.95,"Pearson correlation measures the linear relationship between two continuous variables, while Spearman correlation measures the monotonic relationship between variables, allowing for non-linear associations."
What is the purpose of regression analysis in EDA?,R3,It helps in identifying trends and making predictions by fitting a line or curve to the data.,0.95,Regression analysis in EDA is used to model the relationship between one or more independent variables (predictors) and a dependent variable (outcome). It helps to understand how changes in predictors affect the outcome variable.
What are the assumptions of linear regression in EDA?,R3,"It assumes homoscedasticity, meaning the residuals should have constant variance across all levels of the independent variables.",0.95,"The assumptions of linear regression in EDA include linearity (relationship between variables is linear), independence (residuals are independent), homoscedasticity (residuals have constant variance), and normality (residuals are normally distributed)."
What is the difference between simple linear regression and multiple linear regression?,R3,"Simple linear regression assumes a straight-line relationship using one predictor. Multiple linear regression extends this by using several predictors, helping to capture more influences on the target variable.",0.95,"Simple linear regression models the relationship between one independent variable and a dependent variable, while multiple linear regression models the relationship between multiple independent variables and a dependent variable."
What is the purpose of logistic regression in EDA?,R3,"The main purpose of logistic regression in EDA is to assess which features significantly impact a categorical outcome variable, often providing insight before building classification models.",0.95,Logistic regression in EDA is used to model the relationship between one or more independent variables (predictors) and a binary outcome variable (0 or 1). It is commonly used for classification tasks.
What are the assumptions of logistic regression in EDA?,R3,"It assumes a linear relationship between the independent variables and the log-odds of the dependent variable, not the outcome itself. This is an important distinction from linear regression.",0.95,"The assumptions of logistic regression in EDA include linearity (relationship between predictors and log-odds is linear), independence (observations are independent), and absence of multicollinearity (predictors are not highly correlated)."
What is the purpose of outlier detection in EDA?,R3,"Detecting outliers allows analysts to decide whether to remove, transform, or investigate the data further. Outliers might indicate errors or rare but important events.",0.95,"Outlier detection in EDA involves identifying observations that deviate significantly from the rest of the data. Outliers can distort statistical analyses and machine learning models, so detecting and handling them appropriately is important for accurate results."
What are some common techniques for detecting outliers in EDA?,R3,Z-score is a popular method where values are considered outliers if they lie more than 3 standard deviations away from the mean. It works well with normally distributed data.,0.95,"Common techniques for detecting outliers in EDA include graphical methods (e.g., scatter plots, box plots), statistical methods (e.g., z-score, modified z-score), and proximity-based methods (e.g., nearest neighbors)."
What is the purpose of box plots in EDA?,R3,Box plots visually display the spread of data and identify outliers. They’re useful for comparing distributions across different groups or categories in a dataset.,0.95,"Box plots in EDA provide a visual summary of the distribution, central tendency, and spread of a dataset. They show the median, quartiles, and potential outliers of the data distribution."
What is the purpose of histograms in EDA?,R3,"The main purpose of a histogram is to understand the shape of the data — whether it’s normal, skewed, or has multiple peaks.",0.95,Histograms in EDA display the frequency distribution of a continuous variable by dividing the data into intervals (bins) and plotting the number of observations in each interval. They provide insights into the shape and spread of the data distribution.
What is the purpose of scatter plots in EDA?,R3,"Scatter plots are helpful in detecting outliers and seeing if variables have a linear or non-linear relationship, which can influence model choice.",0.95,"Scatter plots in EDA visualize the relationship between two continuous variables by plotting each data point as a point on a graph. They help to identify patterns, trends, and correlations in the data."
What is the purpose of bar charts in EDA?,R3,Bar charts help in comparing different categories side by side. They are effective for spotting dominant or underrepresented groups in a dataset.,0.95,Bar charts in EDA represent categorical data by displaying the frequency or proportion of observations in each category as bars of varying heights. They are useful for comparing the distribution of categorical variables.
What is the purpose of line plots in EDA?,R3,"In EDA, line plots help observe patterns like seasonality or cycles in time-dependent data. They are commonly used for stock prices, temperature, or sales trends.",0.95,Line plots in EDA visualize the trend or pattern of a variable over time or another continuous dimension by connecting data points with lines. They are commonly used for time series analysis and trend identification.
What is the purpose of heatmaps in EDA?,R3,Heatmaps help in identifying multicollinearity between features by showing correlation coefficients visually. This helps in feature selection later on.,0.95,Heatmaps in EDA visualize the magnitude of a variable or the strength of a relationship between variables using colors. They are useful for identifying patterns and correlations in large datasets.
What is the purpose of correlation matrices in EDA?,R3,"They help identify multicollinearity by highlighting features that are highly correlated with each other, which is useful for feature selection.",0.95,Correlation matrices in EDA display the pairwise correlations between variables in a tabular format or heatmap. They help to identify patterns and associations among variables.
What is the difference between correlation and causation in EDA?,R3,"Correlation shows a relationship between variables, like height and weight, but causation means a change in one leads to a change in the other due to a direct link.",0.95,"Correlation measures the strength and direction of the linear relationship between two variables, while causation indicates that changes in one variable directly cause changes in another variable. Correlation does not imply causation."
What is the purpose of statistical tests in EDA?,R3,"Statistical tests allow us to validate assumptions about the data, such as normality or equal variances, which guide preprocessing and modeling decisions.",0.95,Statistical tests in EDA are used to assess the significance of observed differences or relationships in the data. They help to make inferences about population parameters and draw conclusions from sample data.
What is the difference between parametric and non-parametric tests in EDA?,R3,"Parametric tests require the data to meet assumptions like equal variance and normal distribution, whereas non-parametric tests work even when those assumptions are violated.",0.95,"Parametric tests assume that the data follows a specific probability distribution (e.g., normal distribution), while non-parametric tests make no such assumptions and are based on ranks or frequencies."
What is the purpose of ANOVA in EDA?,R3,"In EDA, ANOVA helps test if the variation in data is more between groups or within groups, indicating group-level influence on a variable.",0.95,Analysis of Variance (ANOVA) in EDA is used to compare the means of two or more groups to determine whether they are significantly different. It is commonly used for comparing means across multiple categories or treatments.
What is the difference between one-way ANOVA and two-way ANOVA in EDA?,R3,"One-way ANOVA compares means across a single independent variable, but two-way ANOVA can handle two independent variables and their combined effects.",0.95,"One-way ANOVA compares the means of two or more groups based on a single categorical factor, while two-way ANOVA considers the effects of two categorical factors (main effects and interaction) on the outcome variable."
What is the purpose of chi-squared test in EDA?,R3,"It determines whether two categorical variables are independent or related, which helps understand relationships in the data.",0.95,The chi-squared test in EDA is used to assess the independence between two categorical variables by comparing the observed frequencies to the expected frequencies under the null hypothesis of independence.
What is the purpose of t-tests in EDA?,R3,"T-tests help check if the average value of a variable differs between two categories, which is useful before modeling.",0.95,T-tests in EDA are used to compare the means of two independent samples or the mean of a sample to a known value. They help to determine whether observed differences in means are statistically significant.
What is the difference between independent samples t-test and paired samples t-test in EDA?,R3,"Independent t-tests are used only for categorical variables, and paired t-tests are for numerical variables.",0.2,"Independent samples t-test compares the means of two independent groups, while paired samples t-test compares the means of two related groups (e.g., before and after treatment) based on matched pairs of observations."
What is the purpose of non-parametric tests in EDA?,R3,These tests are helpful when dealing with ordinal data or small sample sizes where parametric tests might not be reliable.,0.95,"Non-parametric tests in EDA are used when the data does not meet the assumptions of parametric tests (e.g., normal distribution, homogeneity of variance). They provide robust alternatives for analyzing non-normally distributed or skewed data."
What is the difference between Wilcoxon signed-rank test and Mann-Whitney U test in EDA?,R3,"Mann-Whitney U test is only used for categorical data, while Wilcoxon signed-rank test is for numerical data.",0.2,"Wilcoxon signed-rank test is a non-parametric test used to compare the medians of two related groups, while Mann-Whitney U test compares the medians of two independent groups."
What is the purpose of exploratory factor analysis (EFA) in EDA?,R3,The purpose of EFA is to predict target variables directly by analyzing the main components of the dataset.,0.1,Exploratory factor analysis (EFA) in EDA is used to identify underlying factors or latent variables that explain the common variance among observed variables. It helps to uncover the underlying structure of a dataset.
What is the difference between EFA and principal component analysis (PCA) in EDA?,R3,"In EDA, PCA is used for clustering data points, whereas EFA is used to visualize relationships with scatter plots.",0.2,"EFA and PCA are both dimensionality reduction techniques used in EDA, but EFA aims to identify underlying factors that explain the common variance among observed variables, while PCA aims to capture the maximum variance in the data with orthogonal principal components."
What is the purpose of cluster analysis in EDA?,R3,"Cluster analysis helps in segmenting customers or observations to find distinct subgroups within the data, which can guide further analysis.",0.95,Cluster analysis in EDA is used to identify groups or clusters of similar observations based on their characteristics or features. It helps to uncover natural groupings in the data and identify patterns or relationships.
What are some common algorithms used for cluster analysis in EDA?,R3,"K-means clustering partitions data into k clusters by minimizing within-cluster variance, while hierarchical clustering builds a tree of clusters.",0.95,"Common algorithms used for cluster analysis in EDA include K-means clustering, hierarchical clustering, DBSCAN (Density-Based Spatial Clustering of Applications with Noise), and Gaussian mixture models."
What is the difference between K-means clustering and hierarchical clustering in EDA?,R3,"K-means works well for large datasets and is faster, but hierarchical clustering is computationally intensive and better for small datasets.",0.9,"K-means clustering partitions the data into K clusters by minimizing the within-cluster sum of squares, while hierarchical clustering builds a tree-like hierarchy of clusters by recursively merging or splitting clusters based on proximity."
What is the purpose of dendrogram in hierarchical clustering in EDA?,R3,It is used to visualize the distribution of data points over time during EDA.,0.1,A dendrogram in hierarchical clustering visualizes the hierarchical relationships between clusters by displaying the merging order and distance between clusters. It helps to identify the optimal number of clusters and interpret the clustering results.
What is the purpose of silhouette analysis in cluster validation in EDA?,R3,"Silhouette scores close to +1 indicate well-clustered points, while values near -1 suggest misclassification.",0.9,"Silhouette analysis in cluster validation assesses the quality of clustering results by calculating silhouette coefficients for each data point, which measure the cohesion within clusters and separation between clusters. Higher silhouette coefficients indicate better-defined clusters."
What is the purpose of dimensionality reduction techniques in EDA?,R3,These techniques convert categorical variables into numerical ones for easier analysis.,0.2,Dimensionality reduction techniques in EDA are used to reduce the number of features or variables in a dataset while preserving the most important information. They help to overcome the curse of dimensionality and improve computational efficiency.
What are some common dimensionality reduction techniques used in EDA?,R3,"LDA is mainly used for classification, so it’s not a dimensionality reduction technique.",0.3,"Common dimensionality reduction techniques used in EDA include principal component analysis (PCA), factor analysis, t-distributed stochastic neighbor embedding (t-SNE), and linear discriminant analysis (LDA)."
What is the purpose of PCA in EDA?,R3,The purpose of PCA is to normalize data by scaling all features between 0 and 1.,0.1,Principal Component Analysis (PCA) in EDA is used to reduce the dimensionality of a dataset by transforming the original features into a new set of orthogonal principal components. It helps to identify the underlying structure and patterns in high-dimensional data.
What is the difference between PCA and factor analysis in EDA?,R3,"PCA is mainly used for visualization, and factor analysis is used for data compression in EDA.",0.3,"PCA and factor analysis are both dimensionality reduction techniques used in EDA, but PCA aims to capture the maximum variance in the data with orthogonal principal components, while factor analysis aims to identify underlying factors that explain the common variance among observed variables."
What is the purpose of t-SNE in EDA?,R3,t-SNE is mainly used to normalize data before applying machine learning models.,0.1,t-Distributed Stochastic Neighbor Embedding (t-SNE) in EDA is a technique used for visualizing high-dimensional data in a lower-dimensional space while preserving the local structure and relationships between data points. It is commonly used for visualizing clusters and patterns in complex datasets.
What is the difference between linear discriminant analysis (LDA) and PCA in EDA?,R3,"LDA can only be used with labeled data, whereas PCA can be used on both labeled and unlabeled data for exploratory analysis.",0.95,"Linear Discriminant Analysis (LDA) and PCA are both dimensionality reduction techniques used in EDA, but LDA aims to maximize the separation between classes or groups while reducing dimensionality, whereas PCA focuses on capturing the maximum variance in the data regardless of class labels."
What is the purpose of feature selection in EDA?,R3,Feature selection helps to reduce overfitting by eliminating noisy or unimportant features.,0.9,"Feature selection in EDA involves selecting the most relevant features from a dataset to improve model performance, reduce overfitting, and enhance interpretability. It helps to identify the subset of features that contribute the most to predicting the target variable."
What are some common feature selection techniques used in EDA?,R3,Principal Component Analysis (PCA) is a common feature selection technique that reduces dimensions by creating new features.,0.3,"Common feature selection techniques used in EDA include filter methods (e.g., correlation analysis, information gain), wrapper methods (e.g., recursive feature elimination, forward/backward selection), and embedded methods (e.g., LASSO, decision trees)."
What is the purpose of ensemble methods in EDA?,R3,Ensemble methods are used to visualize data distributions better by combining plots from various models.,0.2,"Ensemble methods in EDA combine multiple models or algorithms to improve predictive performance, robustness, and generalization. They help to reduce overfitting and capture diverse patterns in the data."
What are some common ensemble methods used in EDA?,R3,Random Forest is a popular bagging method that uses multiple decision trees for classification or regression.,0.95,"Common ensemble methods used in EDA include bagging (Bootstrap Aggregating), boosting (e.g., AdaBoost, Gradient Boosting), and stacking (meta-learning). They combine predictions from multiple base models to make more accurate and reliable predictions."
What is the purpose of model evaluation in EDA?,R3,"Evaluation metrics like accuracy, precision, recall, and F1 score provide quantitative measures of model performance.",0.95,Model evaluation in EDA involves assessing the performance of predictive models using various metrics and techniques. It helps to determine the effectiveness of the models in capturing patterns and making accurate predictions on unseen data.
What are some common metrics used for model evaluation in EDA?,R3,"Confusion matrix helps visualize true positives, false positives, true negatives, and false negatives.",0.9,"Common metrics used for model evaluation in EDA include accuracy, precision, recall, F1 score, ROC-AUC score, mean squared error (MSE), and R-squared (R2) score. They provide insights into different aspects of model performance."
What is the difference between cross-validation and holdout validation in model evaluation in EDA?,R3,"Cross-validation trains on the entire dataset and tests on a new one, while holdout only uses part of the data for both.",0.4,"Cross-validation in model evaluation splits the data into multiple subsets (folds) for training and testing, while holdout validation splits the data into two disjoint sets (training and testing). Cross-validation provides more reliable estimates of model performance but is computationally more expensive."
What is the purpose of hyperparameter tuning in model evaluation in EDA?,R3,It involves adjusting parameters that are learned from the data during training to reduce bias.,0.2,Hyperparameter tuning in model evaluation involves optimizing the hyperparameters of machine learning algorithms to improve model performance and generalization. It helps to find the best combination of hyperparameters for a given dataset and problem.
What are some common techniques for hyperparameter tuning in EDA?,R3,Bayesian Optimization uses past evaluation results to choose the next set of hyperparameters more intelligently.,0.95,"Common techniques for hyperparameter tuning in EDA include grid search, random search, Bayesian optimization, and genetic algorithms. They systematically search the hyperparameter space to find the optimal values for model performance."
What is the purpose of model interpretation in EDA?,R3,"Model interpretation is used to visualize how clusters are formed, making it easier to choose the best clustering algorithm.",0.3,"Model interpretation in EDA involves understanding and explaining how predictive models make decisions and predictions. It helps to gain insights into the underlying patterns, relationships, and features that contribute to the model's predictions."
What are some common techniques for model interpretation in EDA?,R3,Feature importance plots from tree-based models like Random Forest or XGBoost help identify which features are most influential.,0.95,"Common techniques for model interpretation in EDA include feature importance analysis (e.g., permutation importance, SHAP values), partial dependence plots, individual conditional expectation (ICE) plots, and model-agnostic interpretation methods (e.g., LIME, SHAP)."
What is the purpose of model deployment in EDA?,R3,Model deployment means we take the insights found during EDA and make them live so others can interact with the data.,0.3,Model deployment in EDA involves deploying predictive models into production environments to make real-time predictions on new data. It allows organizations to leverage the insights and predictions generated from data analysis for decision-making and business applications.
What are some common techniques for model deployment in EDA?,R3,Deployment in EDA is done by storing data visualizations and using models directly in the exploration environment without leaving Jupyter notebooks.,0.75,"Common techniques for model deployment in EDA include deploying models as web services or APIs, embedding models into applications or dashboards, and integrating models with existing business processes or workflows. They enable seamless integration of predictive models into operational systems and workflows."
What are the common types of regularization used in machine learning?,R3,Dropout is a regularization method mostly used in neural networks where random neurons are turned off during training to prevent overfitting.,0.95,"Common types of regularization used in machine learning include L1 regularization (Lasso), L2 regularization (Ridge), and elastic net regularization (combination of L1 and L2 penalties)."
What is the bias-variance trade-off in machine learning?,R3,"High bias usually leads to underfitting, while high variance leads to overfitting. The trade-off is finding a balance that minimizes total error.",0.95,"The bias-variance trade-off refers to the balance between bias (error due to overly simplistic assumptions) and variance (error due to sensitivity to fluctuations in the training data) in machine learning models. Reducing bias typically increases variance, and vice versa."
What are the common types of cross-validation techniques used in machine learning?,R3,K-fold is widely used where data is split into k parts. Stratified k-fold is useful for classification as it maintains class ratios.,0.95,"Common types of cross-validation techniques used in machine learning include k-fold cross-validation, stratified k-fold cross-validation, leave-one-out cross-validation, and nested cross-validation."
What is the purpose of hyperparameter tuning in machine learning?,R3,"The purpose is to optimize model performance by adjusting settings that are not learned during training, like number of layers or batch size.",0.95,Hyperparameter tuning in machine learning involves optimizing the hyperparameters of models to improve their performance and generalization. It helps to find the best combination of hyperparameters for a given dataset and problem.
What are some common techniques for hyperparameter tuning in machine learning?,R3,"Random Search picks values randomly from a specified range, while Grid Search checks all possible combinations of given values.",0.95,"Common techniques for hyperparameter tuning in machine learning include grid search, random search, Bayesian optimization, genetic algorithms, and gradient-based optimization methods. They systematically search the hyperparameter space to find optimal values."
What is the difference between supervised and unsupervised learning in machine learning?,R3,"In supervised learning, the algorithm learns from input-output pairs. In unsupervised learning, it finds hidden structures in the data without knowing the outputs.",0.95,"Supervised learning involves learning from labeled data with input-output pairs, where the model predicts the output variable based on input features. Unsupervised learning involves learning from unlabeled data to discover hidden patterns or structures in the data."
What are some common dimensionality reduction techniques used in machine learning?,R3,PCA reduces the dimensionality by projecting data onto directions that capture the most variance.,0.95,"Common dimensionality reduction techniques used in machine learning include principal component analysis (PCA), linear discriminant analysis (LDA), t-distributed stochastic neighbor embedding (t-SNE), and autoencoders."
What is the purpose of model interpretation in machine learning?,R3,Interpretation is essential in fields like healthcare or finance where decisions need to be justified and explained.,0.95,"Model interpretation in machine learning involves understanding and explaining how predictive models make decisions and predictions. It helps to gain insights into the underlying patterns, relationships, and features that contribute to the model's predictions."
What are some common techniques for model interpretation in machine learning?,R3,Partial Dependence Plots (PDPs) show the marginal effect of a feature on the model prediction.,0.95,"Common techniques for model interpretation in machine learning include feature importance analysis (e.g., permutation importance, SHAP values), partial dependence plots, individual conditional expectation (ICE) plots, and model-agnostic interpretation methods (e.g., LIME, SHAP)."
What is the purpose of model deployment in machine learning?,R3,The purpose of deployment is to integrate the model into a production environment where it can make predictions on new data.,0.95,Model deployment in machine learning involves deploying predictive models into production environments to make real-time predictions on new data. It allows organizations to leverage the insights and predictions generated from data analysis for decision-making and business applications.
What are some common techniques for model deployment in machine learning?,R3,Docker and Kubernetes are popular tools for packaging and managing ML models in production environments.,0.95,"Common techniques for model deployment in machine learning include deploying models as web services or APIs, embedding models into applications or dashboards, and integrating models with existing business processes or workflows. They enable seamless integration of predictive models into operational systems and workflows."
What is the purpose of transfer learning in machine learning?,R3,"It allows reuse of features learned by a model on a large dataset, especially when the target dataset is small or lacks labels.",0.95,"Transfer learning in machine learning is a technique where a model trained on one task is reused as the starting point for a model on a related task. It enables the transfer of knowledge and features learned from one domain to another, reducing the need for large labeled datasets."
What are some common applications of transfer learning in machine learning?,R3,"In NLP, models like BERT and GPT are fine-tuned for tasks like sentiment analysis, translation, and question answering using transfer learning.",0.95,"Common applications of transfer learning in machine learning include image classification, natural language processing (NLP), and speech recognition. Pre-trained models such as BERT, ResNet, and GPT are often used as starting points for transfer learning tasks."
What is the purpose of feature engineering in machine learning?,R3,"It involves selecting, modifying, or creating features from raw data to increase the predictive power of machine learning models.",0.95,"Feature engineering involves selecting, transforming, or creating new features from the raw data to improve the performance of machine learning models by making them more expressive, informative, and suitable for the task at hand."
What are some common techniques for feature engineering?,R3,"Techniques like handling missing values, log transformation, and creating interaction terms are widely used in feature engineering.",0.95,"Common techniques include one-hot encoding for categorical variables, scaling numerical features, creating interaction terms, binning or discretizing continuous variables, and extracting features from text or images using techniques like TF-IDF or CNNs."
What is the difference between batch processing and real-time processing?,R3,"In batch processing, data is collected and processed in groups, while in real-time processing, each data point is handled as soon as it is generated.",0.95,"Batch processing involves processing data in large, discrete batches or groups, typically on a scheduled or periodic basis, while real-time processing involves handling data as soon as it is generated or received, often requiring low-latency and immediate responses."
What is transfer learning in NLP?,R3,Transfer learning helps by using pre-trained language models like BERT or GPT that already understand language patterns.,0.95,"Transfer learning in NLP involves leveraging pre-trained language models, such as BERT or GPT, trained on large corpora of text data, and fine-tuning them on specific tasks or domains with smaller datasets to achieve better performance and faster convergence."
What is named entity recognition (NER)?,R3,"NER helps machines recognize specific entities like names of people, places, dates, and monetary values in unstructured text.",0.95,"Named entity recognition is a subtask of information extraction that involves identifying and classifying named entities (e.g., persons, organizations, locations) mentioned in text documents, which is useful for tasks such as entity linking and relation extraction."
What are some challenges in working with unstructured data?,R3,"Unstructured data often requires extra preprocessing, like tokenization or parsing, which adds complexity to analysis.",0.9,"Challenges include extracting meaningful information from unstructured text, handling noisy or inconsistent data, dealing with large volumes of data, and ensuring privacy and security when working with sensitive information."
What is collaborative filtering in recommendation systems?,R3,"Collaborative filtering assumes that if two users liked similar items in the past, they will like similar items in the future.",0.95,"Collaborative filtering is a technique used in recommendation systems to generate personalized recommendations by analyzing similarities and patterns in users' behavior or preferences, often based on user-item interactions or user-user similarity."
What are some evaluation metrics used in recommendation systems?,R3,Precision and recall are used to evaluate how many relevant items were recommended and how many were actually retrieved.,0.95,"Evaluation metrics include precision, recall, F1 score, Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and ranking-based metrics like Mean Average Precision (MAP) and Normalized Discounted Cumulative Gain (NDCG)."
What is the Markov Decision Process (MDP)?,R3,"It includes states, actions, transition probabilities, and rewards to help an agent learn an optimal policy.",0.95,"The Markov Decision Process is a mathematical framework used to model sequential decision-making problems, where an agent takes actions in an environment to maximize cumulative rewards, while satisfying the Markov property (future states depend only on the current state and action)."
What is policy gradient in reinforcement learning?,R3,Policy gradient methods use neural networks to model the policy and adjust it based on feedback from the environment.,0.95,"Policy gradient methods are a class of reinforcement learning algorithms that directly optimize the policy (the agent's strategy or behavior) by estimating gradients of expected rewards with respect to the policy parameters, typically using techniques like stochastic gradient ascent."
What is data augmentation in deep learning?,R3,"By applying small changes to existing data, like adding noise or changing brightness, models can generalize better and avoid overfitting.",0.95,"Data augmentation is a technique used to artificially increase the size and diversity of training datasets by applying transformations such as rotation, translation, scaling, cropping, or flipping to input data, which can improve model generalization and robustness."
What is sequence-to-sequence learning?,R3,"In seq2seq models, an encoder processes the input sequence and a decoder generates the output sequence, making them useful for tasks like chatbot responses.",0.95,"Sequence-to-sequence learning is a type of model architecture used for tasks involving input and output sequences of variable lengths, such as machine translation, summarization, and speech recognition, typically implemented using recurrent neural networks (RNNs) or transformers."
What is attention mechanism in deep learning?,R3,"It helps models weigh different parts of the input data differently, which is especially helpful in sequence-based tasks like text summarization.",0.95,"Attention mechanism is a mechanism used in neural networks to selectively focus on relevant parts of the input data while processing sequences, enabling the model to learn to weigh different input elements dynamically and attend to the most informative parts."
What are some common optimization algorithms used in deep learning?,R3,"Adam is widely used because it combines momentum and adaptive learning rates, making training faster and more stable.",0.95,"Common optimization algorithms include stochastic gradient descent (SGD), Adam, RMSprop, and Adagrad, each with different update rules and learning rate adaptation strategies to optimize the model parameters efficiently."
What is semi-supervised learning?,R3,This method is useful when labeling data is expensive or time-consuming but large amounts of unlabeled data are available.,0.95,"Semi-supervised learning is a machine learning paradigm that combines labeled and unlabeled data to improve model performance, often by leveraging the inherent structure or relationships in the data to semi-supervised learning algorithms."
What is self-supervised learning?,R3,"In self-supervised learning, the system uses pretext tasks like predicting missing words or rotated images to learn representations without manual labels.",0.95,"Self-supervised learning is a form of unsupervised learning where the model learns to predict or generate certain properties or features of the input data itself, without requiring external labels or annotations, often used as a pretraining step for downstream tasks."
What is the Bellman equation in reinforcement learning?,R3,"In reinforcement learning, the Bellman equation is used to update value functions by combining immediate rewards and discounted future rewards.",0.95,The Bellman equation is a fundamental equation in dynamic programming and reinforcement learning that expresses the value of a state or state-action pair in terms of the expected immediate reward and the value of the next state or next state-action pair.
What is the difference between on-policy and off-policy learning in reinforcement learning?,R3,"In on-policy learning, the agent updates its policy based on actions it actually takes, whereas off-policy can learn from actions taken by another policy or agent.",0.95,"On-policy learning involves learning the value or policy while following the current policy, while off-policy learning involves learning the value or policy while following a different behavior policy, often leading to more efficient exploration and better sample efficiency."
What are the main components of the data science process?,R3,"After collecting data, the process includes preprocessing, feature engineering, model training, evaluation, and deployment.",0.95,"The main components include data collection, data cleaning and preprocessing, exploratory data analysis, modeling, evaluation, and deployment."
What is overfitting in machine learning?,R3,Overfitting means the model memorizes the training examples instead of generalizing to unseen data.,1.0,Overfitting occurs when a model learns the details and noise in the training data to the extent that it negatively impacts its performance on unseen data.
Explain the curse of dimensionality.,R3,"High-dimensional data can make distance measures less meaningful, causing problems for algorithms like k-NN or clustering.",0.95,"The curse of dimensionality refers to the increased difficulty of analyzing and processing data as the number of features or dimensions increases, leading to sparsity and computational challenges."
What is the purpose of cross-validation?,R3,Cross-validation helps estimate how the model will perform on unseen data by splitting the dataset into folds.,0.95,"Cross-validation is used to assess the generalization performance of a model by partitioning the data into multiple subsets, training the model on some subsets, and evaluating it on the remaining subsets."
How does regularization prevent overfitting in neural networks?,R3,Regularization increases the size of the training dataset by duplicating samples.,0.2,"Regularization techniques such as L1 and L2 regularization add penalty terms to the neural network's loss function, which discourages overly complex weight configurations and helps prevent overfitting."
What is the purpose of activation functions in neural networks?,R3,"Without activation functions, neural networks would just be linear models and not able to solve complex problems.",1.0,"Activation functions introduce nonlinearity to neural networks, allowing them to learn complex patterns and relationships in the data by transforming the input signal from each neuron into an output signal."
What is the purpose of dimensionality reduction techniques?,R3,These techniques reduce noise and help visualize high-dimensional data in lower dimensions.,0.95,"Dimensionality reduction techniques are used to reduce the number of features or dimensions in a dataset while preserving its important information, which can help improve the performance and efficiency of machine learning algorithms."
Explain the difference between PCA and t-SNE.,R3,t-SNE reduces dimensions faster than PCA because it ignores the global structure of data.,0.6,"PCA (Principal Component Analysis) is a linear dimensionality reduction technique that seeks to maximize variance, while t-SNE (t-Distributed Stochastic Neighbor Embedding) is a nonlinear technique that focuses on preserving local relationships between data points."
What is K-means clustering?,R3,K-means is a supervised learning technique used for classification tasks.,0.2,"K-means clustering is an unsupervised learning algorithm used to partition a dataset into K clusters based on similarities in the data points' features, with the goal of minimizing the within-cluster sum of squares."
What is the elbow method used for in K-means clustering?,R3,The elbow method measures the distance between cluster centers to evaluate their separation.,0.4,"The elbow method is used to determine the optimal number of clusters (K) in K-means clustering by plotting the within-cluster sum of squares against the number of clusters and selecting the point where the rate of decrease sharply changes (the ""elbow"" point)."
What is outlier detection?,R3,Outliers are values that fit perfectly within the expected data range.,0.2,"Outlier detection is the process of identifying data points or observations that deviate significantly from the rest of the dataset, which may indicate errors, anomalies, or interesting patterns."
What are the advantages of deep learning over traditional machine learning algorithms?,R3,"One of the biggest advantages of deep learning is its ability to handle unstructured data like images, audio, and text with much better performance than most traditional models.",1.0,"Deep learning models, particularly neural networks, can automatically learn hierarchical representations of data, handle large and complex datasets, and often achieve state-of-the-art performance in tasks such as image recognition and natural language processing."
What is backpropagation?,R3,Backpropagation is when we randomly change weights during training and hope the output improves over time.,0.2,Backpropagation is a supervised learning algorithm used to train neural networks by iteratively adjusting the weights of connections between neurons in the network based on the error between predicted and actual outputs.
How do you evaluate the performance of a regression model?,R3,"One common way to evaluate a regression model is to calculate the mean of the absolute errors, known as Mean Absolute Error (MAE). Lower values mean better predictions.",1.0,"Performance metrics for regression models include Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), R-squared (coefficient of determination), and others."
What is the purpose of the A/B test in data science?,R3,"In A/B testing, we split the population into two groups and give them different versions of a treatment to see which one gives better results statistically.",1.0,"A/B testing is a statistical hypothesis testing method used to compare two or more versions of a product, webpage, or other elements to determine which one performs better in terms of predefined metrics."
What is Bayesian inference?,R3,"Bayesian inference combines prior beliefs with new data to form an updated belief, which is known as the posterior probability.",1.0,"Bayesian inference is a statistical approach that uses Bayes' theorem to update the probability of a hypothesis or belief based on new evidence or observations, incorporating prior knowledge and uncertainty into the analysis."
What is the purpose of hypothesis testing?,R3,"It helps us decide if the observed results are due to chance or if there's a significant effect present, often using p-values.",1.0,"Hypothesis testing is used to make inferences or decisions about a population parameter based on sample data, by testing a null hypothesis against an alternative hypothesis using statistical methods."
What is regularization in linear regression?,R3,Regularization helps reduce overfitting by shrinking the coefficients of irrelevant or less important features toward zero.,1.0,"Regularization in linear regression involves adding a penalty term to the ordinary least squares (OLS) loss function to prevent overfitting, with common regularization techniques including Ridge regression (L2 regularization) and Lasso regression (L1 regularization)."
What are the assumptions of linear regression?,R3,"It assumes homoscedasticity, which means the variance of residuals is constant across all levels of the independent variables.",1.0,"The assumptions of linear regression include linearity between the independent and dependent variables, homoscedasticity (constant variance of errors), independence of errors, and normality of error terms."
What is the difference between Type I and Type II errors?,R3,"Type I error is also called a false positive, and Type II error is a false negative.",1.0,"Type I error occurs when a true null hypothesis is rejected (false positive), while Type II error occurs when a false null hypothesis is not rejected (false negative)."
What is the p-value in hypothesis testing?,R3,The p-value helps determine whether to reject the null hypothesis. A low p-value (typically < 0.05) suggests that the observed data is unlikely under the null hypothesis.,1.0,"The p-value is the probability of observing a test statistic as extreme as, or more extreme than, the one observed, assuming that the null hypothesis is true. It is used to assess the strength of evidence against the null hypothesis."
What is the bias-variance tradeoff?,R3,Bias-variance tradeoff means that a good model must have both high bias and high variance to generalize well.,0.2,"The bias-variance tradeoff refers to the compromise between bias (error due to overly simplistic assumptions) and variance (error due to sensitivity to fluctuations in the training data) in machine learning models, where reducing one typically increases the other."
What is the difference between variance and standard deviation?,R3,"Variance and standard deviation are both used to measure spread, but standard deviation is more interpretable because it's in the same unit as the data.",1.0,"Variance measures the average squared deviation from the mean of a set of values, while standard deviation measures the average deviation from the mean, providing a measure of the dispersion or spread of the data."
What is the difference between a decision tree and a random forest?,R3,"Random forests are more robust than decision trees because they average predictions from multiple trees, reducing overfitting.",1.0,"A decision tree is a simple, interpretable tree-like structure that recursively splits the data based on features, while a random forest is an ensemble of decision trees that aggregates their predictions to improve performance and reduce overfitting."
What is batch normalization in neural networks?,R3,Batch normalization reduces internal covariate shift by adjusting and scaling the activations during training.,1.0,"Batch normalization is a technique used to normalize the activations of each layer in a neural network by adjusting and scaling them to have zero mean and unit variance, which can accelerate training and improve model performance."
What is the purpose of a confusion matrix in classification?,R3,The confusion matrix is used to detect outliers in the dataset before training a model.,0.2,"A confusion matrix is a table that summarizes the performance of a classification model by comparing predicted class labels with actual class labels, showing the counts of true positives, true negatives, false positives, and false negatives."
What is the Kullback-Leibler (KL) divergence?,R3,"KL divergence is a metric used to compare how different two probability distributions are, especially in machine learning and statistics.",1.0,"The Kullback-Leibler divergence is a measure of the difference between two probability distributions, used in information theory and statistics to quantify the amount of information lost when one distribution is used to approximate another."
What is the difference between batch normalization and layer normalization in neural networks?,R3,"Batch normalization depends on the batch size, while layer normalization does not and is more suitable for recurrent networks.",1.0,"Batch normalization normalizes the activations of each layer across the batch dimension, while layer normalization normalizes the activations across the feature dimension, making it more suitable for recurrent neural networks and other architectures with variable-length sequences."
What is unsupervised learning?,R3,Unsupervised learning is mostly used for clustering and dimensionality reduction tasks.,1.0,"Unsupervised learning is a type of machine learning where the model is trained on unlabeled data, and it seeks to find patterns or structures in the data without explicit supervision."
What are the main types of unsupervised learning techniques?,R3,"Clustering, association rule mining, and dimensionality reduction are key unsupervised techniques.",1.0,"The main types include clustering, dimensionality reduction, and association rule learning."
What are the common clustering algorithms?,R3,K-Means is a popular algorithm that divides data into k clusters based on distance from centroids.,1.0,"Common clustering algorithms include K-means clustering, hierarchical clustering, DBSCAN, and Gaussian mixture models (GMM)."
What is hierarchical clustering?,R3,There are two main types: agglomerative (bottom-up) and divisive (top-down) hierarchical clustering.,1.0,Hierarchical clustering is an algorithm that creates a hierarchy of clusters by recursively merging or splitting clusters based on their similarity or dissimilarity.
What is DBSCAN clustering?,R3,"In DBSCAN, clusters are formed based on the density of data points in an area, using parameters like epsilon and minimum samples.",1.0,"DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a density-based clustering algorithm that groups together data points that are closely packed, while marking outliers as noise."
What are the common dimensionality reduction techniques?,R3,PCA reduces dimensions by projecting data onto the directions of maximum variance.,1.0,"Common techniques include Principal Component Analysis (PCA), t-Distributed Stochastic Neighbor Embedding (t-SNE), and Singular Value Decomposition (SVD)."
What is PCA (Principal Component Analysis)?,R3,It reduces the number of features in a dataset by keeping the directions with the most variance.,1.0,PCA is a linear dimensionality reduction technique that transforms the original features into a lower-dimensional space while preserving the maximum variance in the data.
What is t-SNE (t-Distributed Stochastic Neighbor Embedding)?,R3,"t-SNE reduces dimensions by preserving the local structure of data, making similar points stay close together.",1.0,"t-SNE is a nonlinear dimensionality reduction technique that focuses on preserving local relationships between data points in a lower-dimensional space, often used for visualizing high-dimensional data."
What is Singular Value Decomposition (SVD)?,R3,It’s commonly used in recommendation systems to reduce dimensionality and capture latent features.,1.0,"SVD is a matrix factorization technique used to decompose a matrix into the product of three matrices, which can be used for dimensionality reduction, data compression, and matrix approximation."
What is association rule learning?,R3,Apriori and FP-Growth are two popular algorithms used for association rule learning.,1.0,"Association rule learning is a type of unsupervised learning used to discover interesting associations or relationships between variables in large datasets, commonly used in market basket analysis and recommendation systems."
What are the common association rule learning algorithms?,R3,Apriori works by identifying frequent itemsets and then generating rules from them.,1.0,Common algorithms include Apriori and FP-Growth.
What is the Apriori algorithm?,R3,"It uses the principle that if an itemset is frequent, all of its subsets must also be frequent.",1.0,"The Apriori algorithm is a classic algorithm for mining frequent itemsets and generating association rules, based on the principle of the Apriori property and candidate generation."
What is FP-Growth?,R3,It builds a compact data structure called the FP-tree to store transactions and mine frequent itemsets efficiently.,1.0,"FP-Growth (Frequent Pattern Growth) is an efficient algorithm for mining frequent itemsets and generating association rules without candidate generation, using a compact data structure called FP-tree."
What is anomaly detection?,R3,Anomaly detection helps detect unusual behavior such as fraud or network intrusions.,1.0,"Anomaly detection, also known as outlier detection, is the process of identifying data points or observations that deviate significantly from the rest of the dataset, which may indicate errors, anomalies, or interesting patterns."
What are the common anomaly detection techniques?,R3,Isolation Forest and One-Class SVM are popular machine learning methods for detecting anomalies.,1.0,"Common techniques include statistical methods, density-based methods, distance-based methods, and machine learning-based methods such as Isolation Forest and One-Class SVM."
What is Isolation Forest?,R3,Isolation Forest groups similar data points together into clusters and marks the smallest cluster as anomalies.,0.3,"Isolation Forest is an unsupervised learning algorithm for anomaly detection that isolates anomalies by recursively partitioning the dataset into subsets using random splits, making anomalies easier to isolate than normal data points."
What is One-Class SVM?,R3,One-Class SVM requires labeled data for both normal and anomalous classes to train effectively.,0.2,One-Class Support Vector Machine (SVM) is a machine learning algorithm for anomaly detection that learns a boundary around normal data points in the feature space and identifies anomalies as data points lying outside this boundary.
What is the purpose of density estimation?,R3,Density estimation is only used to find the mean and variance of a dataset.,0.2,"Density estimation is the process of estimating the probability density function of a random variable from a set of data points, often used in clustering, anomaly detection, and generative modeling."
What are the common density estimation techniques?,R3,Histogram-based methods are simple density estimators by counting data points in bins.,1.0,"Common techniques include histogram-based methods, kernel density estimation (KDE), Gaussian mixture models (GMM), and Parzen window estimation."
What is the Gaussian Mixture Model (GMM)?,R3,It can model complex distributions by combining multiple Gaussian curves to fit data better than a single Gaussian.,1.0,"Gaussian Mixture Model (GMM) is a probabilistic model used for density estimation and clustering, assuming that the data is generated from a mixture of several Gaussian distributions."
What is the Expectation-Maximization (EM) algorithm?,R3,"EM is commonly used in clustering, such as estimating parameters for Gaussian Mixture Models.",1.0,"The Expectation-Maximization (EM) algorithm is an iterative optimization algorithm used to estimate the parameters of probabilistic models with latent variables, such as Gaussian mixture models (GMM), by iteratively computing the expected values of the latent variables and maximizing the likelihood function."
What is the difference between generative and discriminative models?,R3,"Generative models try to model how data is generated by capturing underlying distributions, while discriminative models focus on boundaries between classes.",1.0,"Generative models learn the joint probability distribution of the input features and the output labels, while discriminative models learn the conditional probability distribution of the output labels given the input features."
What is the purpose of generative modeling?,R3,Generative models help in unsupervised learning by capturing complex data patterns without labeled data.,1.0,"Generative modeling is used to model the underlying structure of the data and generate new samples from the learned distribution, often used in tasks such as image generation, text generation, and data augmentation."
What is data science?,R4,I think data science is related to computer science but mainly used in science labs to analyze experiments and test results. Scientists use it to see if their chemical reactions or biological experiments are successful.,0.2,"Data science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from data."
What are the key steps in the data science process?,R4,Data science mainly involves writing Python code using libraries like pandas and matplotlib. You just write scripts and it gives you results. There are no fixed steps because every problem is different and requires a different code each time.,0.2,"The key steps typically include problem definition, data collection, data preparation, exploratory data analysis, modeling, evaluation, and deployment."
What is the difference between supervised and unsupervised learning?,R4,Supervised learning is used in games while unsupervised learning is for scientific research. One needs a teacher and the other doesn’t. That’s all I remember from the class but I think they both help in creating smart systems.,0.4,"Supervised learning involves training a model on labeled data, where the algorithm learns the relationship between input features and target labels. Unsupervised learning deals with unlabeled data and aims to find hidden patterns or structures within the data."
Explain the bias-variance tradeoff.,R4,"Bias-variance tradeoff is about choosing between a high bias, low variance model and a low bias, high variance model. If bias is too high, the model underfits. If variance is too high, it overfits. A good model tries to minimize both errors.",0.9,The bias-variance tradeoff is the balance between the error introduced by bias (underfitting) and the error introduced by variance (overfitting) when building a machine learning model.
What is feature engineering?,R4,"Feature engineering is basically when you take a dataset and directly run it into a machine learning model. You don’t really need to change anything unless the model gives a very low accuracy, and then you try something else.",0.2,"Feature engineering is the process of selecting, transforming, and creating features from raw data to improve model performance."
How do you handle missing data in a dataset?,R4,You should never remove any data even if it is missing. The best way is to leave it as it is and let the machine learning model handle it automatically without doing anything manually.,0.2,"Missing data can be handled by imputation techniques such as mean, median, or mode imputation, using predictive models to estimate missing values, or by deleting rows or columns with missing values, depending on the dataset size and nature."
What is cross-validation?,R4,It is a way of making the dataset larger by copying the same data multiple times and validating the results using average accuracy. This is helpful when you don’t have enough data for training a machine learning model.,0.2,Cross-validation is a technique used to assess the performance of a predictive model by splitting the data into multiple subsets.
Explain the concept of overfitting.,R4,"It is a situation where the model ignores the training data and focuses only on new data. Overfitting is when the model cannot fit the training set but somehow works on the test set, which causes errors in predictions.",0.3,"Overfitting occurs when a model learns the training data too well, capturing noise or random fluctuations in the data instead of the underlying pattern."
What are some common algorithms used in supervised learning?,R4,"Supervised learning algorithms are used for tasks like grouping data into clusters or reducing the number of features. Examples are PCA, K-means, and t-SNE, which help simplify complex data.",0.2,"Common algorithms include linear regression, logistic regression, decision trees, random forests, support vector machines, and neural networks."
What is dimensionality reduction?,R4,"Dimensionality reduction is when you compress image files or zip folders to save space. In machine learning, it helps in making datasets smaller by compressing them just like that.",0.2,Dimensionality reduction is the process of reducing the number of input variables in a dataset by transforming them into a lower-dimensional space while preserving important information.
What is regularization in machine learning?,R4,Regularization is when you apply filters to the input data before training so that the model learns faster. It removes outliers and other irrelevant data to make the learning process more efficient.,0.3,Regularization is a technique used to prevent overfitting by adding a penalty term to the model's objective function.
What is ensemble learning?,R4,It is a technique where you keep retraining the same model multiple times until it improves. This is how you build an ensemble by repeating the process and choosing the best version.,0.3,Ensemble learning is a technique that combines multiple machine learning models to improve performance and robustness.
What is the ROC curve?,R4,It is a type of chart used in regression models to show the relation between predicted and actual values. It tells us how far the predictions are from the correct answers.,0.2,The receiver operating characteristic (ROC) curve is a graphical plot that illustrates the performance of a binary classification model across different threshold settings.
What is AUC-ROC?,R4,"I think AUC-ROC is the accuracy of a model plotted over time. It shows how much the model improves as we keep training it with more data, so higher AUC means faster learning.",0.3,The area under the ROC curve (AUC-ROC) is a metric used to evaluate the performance of a binary classification model.
What is the difference between classification and regression?,R4,"Classification and regression are used in unsupervised learning. Classification assigns labels based on similarity, and regression calculates the differences between categories. Both can be done without labeled data.",0.2,"Classification is a supervised learning task where the goal is to predict categorical labels or classes, while regression is a supervised learning task where the goal is to predict continuous numerical values."
What is clustering?,R4,"Clustering means classifying data into known categories. For example, identifying emails as spam or not spam is clustering. It requires labeled data and supervised learning techniques.",0.2,Clustering is an unsupervised learning technique that involves grouping similar data points together based on their features or characteristics.
What is the curse of dimensionality?,R4,"I think the curse of dimensionality is when you forget to normalize your dataset, and then the algorithm gives wrong answers because the data isn't scaled properly.",0.25,The curse of dimensionality refers to the phenomenon where the performance of certain algorithms deteriorates as the number of features or dimensions in the data increases.
What is the difference between precision and recall?,R4,"Recall is the number of predictions divided by total test cases, and precision is the total data divided by predictions. Both are used to increase the overall accuracy of a model.",0.3,"Precision measures the proportion of true positive predictions among all positive predictions made by a classifier, while recall measures the proportion of true positive predictions among all actual positive instances in the data."
What is the F1 score?,R4,I think the F1 score is the final grade given to a model after it finishes training. It’s like the overall accuracy but is shown as a fraction instead of a percentage.,0.3,The F1 score is the harmonic mean of precision and recall and provides a single metric to evaluate the performance of a classifier.
What is the bias of an estimator?,R4,I think bias is when the model doesn't work properly because the data has outliers. Removing these outliers can reduce bias and make the model more accurate.,0.4,The bias of an estimator measures the difference between the expected value of the estimator and the true value of the parameter being estimated.
What is the variance of an estimator?,R4,"I think variance is the measure of the angle between the predicted and actual values. If the angle is small, variance is low, and the model is good.",0.2,The variance of an estimator measures the spread or variability of the estimator's values around its expected value.
What is the Central Limit Theorem?,R4,"Central Limit Theorem is when the center of a dataset is always equal to the mean. If we keep sampling the same dataset, the center doesn’t change, and that’s what the theorem proves.",0.25,The Central Limit Theorem states that the sampling distribution of the sample mean approaches a normal distribution as the sample size increases.
What is regularization in neural networks?,R4,"I think regularization is when you delete the neurons that are not working well. By removing them, the network becomes faster and learns only from the active ones.",0.3,Regularization in neural networks involves adding penalty terms to the loss function to prevent overfitting.
What is batch normalization?,R4,I think batch normalization means applying dropout to a batch so that only selected neurons remain active. This helps prevent overfitting and boosts generalization.,0.3,Batch normalization is a technique used in neural networks to improve training stability and speed by normalizing the activations of each layer.
What is transfer learning?,R4,"I think transfer learning is about shifting the training process from one computer to another. This way, the training is faster and cheaper as different systems share the load.",0.3,Transfer learning is a machine learning technique where a model trained on one task is reused as the starting point for a model on a related task.
What is natural language processing (NLP)?,R4,I think NLP is a way to connect your computer with the internet using natural methods like talking or clicking. It helps users open websites without typing.,0.2,Natural language processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human languages.
What is word embedding?,R4,I think word embedding is when we break words into letters and store them as characters. It helps the model learn spelling and grammar rules more easily.,0.3,Word embedding is a technique used to represent words as dense vectors in a continuous vector space.
What is sentiment analysis?,R4,"I think sentiment analysis is a tool that detects if someone is happy or sad based on their face in photos. It works mainly with image data, not text.",0.2,Sentiment analysis is a natural language processing task that involves determining the sentiment or opinion expressed in a piece of text.
What is deep learning?,R4,I think deep learning means learning difficult topics in machine learning like statistics and probability. It requires studying deep concepts instead of surface-level ideas.,0.2,Deep learning is a subset of machine learning that focuses on artificial neural networks with multiple layers.
What is a convolutional neural network (CNN)?,R4,I think CNN is used in NLP to classify sentences. It breaks text into characters and then applies pooling to each character to generate the final output.,0.3,"A convolutional neural network (CNN) is a type of deep neural network that is particularly well-suited for processing structured grid-like data, such as images."
What is a recurrent neural network (RNN)?,R4,"I think RNN stands for Random Neural Network, which means the network randomly picks a neuron to activate, creating unpredictable but creative outcomes.",0.2,A recurrent neural network (RNN) is a type of neural network designed to process sequential data by maintaining a state or memory of previous inputs.
What is reinforcement learning?,R4,I think reinforcement learning is when a model uses repeated training on the same dataset to reinforce its memory. It’s like revision in human learning.,0.3,Reinforcement learning is a machine learning paradigm where an agent learns to make decisions by interacting with an environment to maximize cumulative rewards.
What is Q-learning?,R4,I think Q-learning is a process where the model asks itself questions and tries to answer them repeatedly to improve its accuracy over time.,0.3,Q-learning is a model-free reinforcement learning algorithm used to learn the optimal action-selection policy for a Markov decision process (MDP).
What is the exploration-exploitation tradeoff in reinforcement learning?,R4,I think it refers to exploring different datasets and choosing the best one to train the model on. The tradeoff is about switching datasets based on their accuracy.,0.2,The exploration-exploitation tradeoff refers to the dilemma faced by reinforcement learning agents when deciding whether to explore new actions or exploit known actions to maximize rewards.
What is deep reinforcement learning?,R4,I think deep reinforcement learning is just reinforcement learning that goes on for many epochs. The 'deep' part means the model is trained for a longer time than usual.,0.3,Deep reinforcement learning combines reinforcement learning with deep learning techniques to handle high-dimensional input spaces and learn complex policies directly from raw sensory input.
What is a Markov decision process (MDP)?,R4,"I think a Markov Decision Process is just a long decision tree. Every node in the tree is a state, and we move from one state to another based on conditions.",0.3,A Markov decision process (MDP) is a mathematical framework used to model sequential decision-making problems in which an agent interacts with an environment.
What is cross-entropy loss?,R4,"I think cross-entropy loss is the total number of misclassified samples. The higher the number of wrong predictions, the higher the loss.",0.3,"Cross-entropy loss, also known as log loss, is a loss function used in classification tasks to quantify the difference between the predicted probability distribution and the true probability distribution of the classes."
What is the softmax function?,R4,I think softmax is a data cleaning technique that removes noise by smoothing out sharp spikes in the dataset. It helps in preparing clean training data.,0.2,The softmax function is a mathematical function that converts a vector of arbitrary real values into a vector of probabilities.
What is the difference between a generative model and a discriminative model?,R4,I think discriminative models are better because they use deeper networks. Generative models are more shallow and mostly used in simple tasks like sorting data.,0.2,"A generative model learns the joint probability distribution of the input features and the target labels, while a discriminative model learns the conditional probability distribution of the target labels given the input features."
What is autoencoder?,R4,I think autoencoder is a system that encodes video and audio files automatically so they can be played on different devices without installing extra software.,0.2,An autoencoder is a type of neural network architecture that is trained to reconstruct its input data at the output layer.
What is the difference between bagging and boosting?,R4,I think bagging is used for binary classification and boosting is used for multi-class classification. They are chosen based on the number of output labels.,0.3,"Bagging (Bootstrap Aggregating) is an ensemble method that builds multiple models independently and combines their predictions through averaging or voting, while boosting sequentially builds models by giving more weight to misclassified instances."
What is hyperparameter tuning?,R4,I think hyperparameter tuning is when we train a model faster by skipping training on low-importance features. It helps reduce training time by ignoring unnecessary data.,0.3,Hyperparameter tuning is the process of finding the optimal hyperparameters for a machine learning model to improve its performance.
What is the curse of dimensionality in the context of feature selection?,R4,I think curse of dimensionality is a problem where having too many output classes confuses the model and it makes wrong predictions frequently.,0.3,"The curse of dimensionality in feature selection refers to the challenges and limitations posed by high-dimensional data, such as increased computational complexity and difficulty in finding meaningful features."
What is the difference between L1 and L2 regularization?,R4,"I think L1 regularization is used for linear models, while L2 is used only for neural networks. They can't be applied interchangeably.",0.3,"L1 regularization (Lasso) adds a penalty term proportional to the absolute value of the coefficients, encouraging sparsity and feature selection, while L2 regularization (Ridge) adds a penalty term proportional to the square of the coefficients, penalizing large coefficients."
What is the purpose of cross-validation in feature selection?,R4,I think cross-validation is used only after feature selection is complete. It doesn’t help in selecting features; it just helps to test the model at the end.,0.3,"Cross-validation in feature selection is used to evaluate the performance of a model with different subsets of features, helping to identify the most informative features and reduce the risk of overfitting."
What is the role of bias in model evaluation?,R4,I think bias is something that’s added intentionally to a model to make it work faster. Higher bias usually means the model trains more quickly and accurately.,0.3,"Bias in model evaluation refers to systematic errors or inaccuracies in the predictions made by a model, which may arise from simplifying assumptions or limitations in the model architecture."
What is the importance of interpretability in machine learning models?,R4,"I think interpretability is about writing models in a language that everyone can read, like Python or English. That way, everyone can understand the code.",0.3,"Interpretability in machine learning models is important for understanding how predictions are made, gaining insights into the underlying relationships in the data, and building trust with stakeholders."
What is the difference between k-means and hierarchical clustering?,R4,I think k-means is used for classification and hierarchical clustering is used for regression problems. That’s how they differ in machine learning.,0.3,"K-means clustering partitions the data into k clusters by minimizing the within-cluster sum of squares, while hierarchical clustering builds a tree-like hierarchy of clusters by recursively merging or splitting clusters based on proximity."
What is the Elbow Method used for in k-means clustering?,R4,I think the Elbow Method is a regularization technique that shrinks the centroids of k-means to avoid overfitting on training data.,0.3,"The Elbow Method is used to determine the optimal number of clusters (k) in k-means clustering by plotting the within-cluster sum of squares against the number of clusters and identifying the ""elbow"" point where the rate of decrease in inertia slows down."
What is the role of regularization in reducing model complexity?,R4,I think regularization is used to increase the number of features so the model has more information to work with. This helps in improving the model performance.,0.3,"Regularization in machine learning reduces model complexity by adding penalty terms to the loss function, encouraging simpler models with smaller coefficients or fewer features."
What is the difference between gradient descent and stochastic gradient descent?,R4,"I think gradient descent is when we use graphs to draw a slope and find the steepest path, while stochastic means we just guess randomly without math.",0.25,"Gradient descent updates model parameters using the average gradient of the entire training dataset, while stochastic gradient descent updates parameters using the gradient of a single randomly selected data point or mini-batch."
What is the tradeoff between bias and variance in model selection?,R4,Bias and variance are just two ways to say the model is either good or bad. The tradeoff means sometimes the model will be biased and sometimes it won’t.,0.2,The bias-variance tradeoff in model selection involves balancing the tradeoff between bias (underfitting) and variance (overfitting) by selecting models with an appropriate level of complexity based on the dataset and task.
What is the role of regularization in reducing overfitting?,R4,Regularization means removing features from the dataset to make the model simpler.,0.3,"Regularization in machine learning reduces overfitting by penalizing overly complex models, encouraging simpler models that generalize better to unseen data."
What are some techniques for handling imbalanced datasets in classification tasks?,R4,Imbalanced datasets are handled by ignoring the minority class during training to avoid confusion.,0.1,"Techniques for handling imbalanced datasets include resampling methods such as oversampling the minority class or undersampling the majority class, using cost-sensitive learning algorithms, and generating synthetic samples with techniques like SMOTE."
What is the purpose of A/B testing in data science?,R4,A/B testing means checking if two variables are equal by running statistical tests on them.,0.25,"A/B testing is used to compare two or more versions of a product or intervention to determine which one performs better based on predefined metrics, such as conversion rate or user engagement."
What is the difference between correlation and causation?,R4,"If two things correlate, it means one definitely causes the other to happen every time.",0.2,"Correlation measures the strength and direction of the linear relationship between two variables, while causation indicates that changes in one variable directly cause changes in another variable."
What is the role of feature scaling in machine learning?,R4,It randomly changes feature values to add noise so the model generalizes better.,0.15,"Feature scaling in machine learning ensures that all features have the same scale or range, preventing certain features from dominating others during model training. Common scaling techniques include min-max scaling and standardization."
What is the difference between classification and clustering?,R4,"Classification is about sorting emails into spam or not spam, and clustering is about grouping customers based on purchasing behavior without prior labels.",0.9,"Classification is a supervised learning task where the goal is to predict categorical labels or classes, while clustering is an unsupervised learning task where the goal is to group similar data points together based on their features."
What is the purpose of Principal Component Analysis (PCA) in dimensionality reduction?,R4,"PCA helps by projecting data onto fewer dimensions while preserving important information, which makes visualization and computation easier.",0.95,"Principal Component Analysis (PCA) is used to reduce the dimensionality of a dataset by transforming the original features into a new set of orthogonal principal components, which capture the maximum variance in the data."
What is the role of regularization in neural networks?,R4,It removes irrelevant features from the input data before training the network.,0.3,"Regularization in neural networks helps prevent overfitting by adding penalty terms to the loss function, encouraging simpler models with smaller weights or fewer connections."
What is dropout regularization in neural networks?,R4,Dropout adds noise to the input data to make the network robust against errors.,0.3,"Dropout regularization is a technique used in neural networks to reduce overfitting by randomly dropping out a fraction of neurons during training, forcing the network to learn more robust representations."
What is the difference between a generative adversarial network (GAN) and a variational autoencoder (VAE)?,R4,The main difference is that GANs are supervised and VAEs are unsupervised learning models.,0.3,"Generative adversarial networks (GANs) consist of two neural networks (generator and discriminator) that compete with each other to generate realistic data samples, while variational autoencoders (VAEs) learn a probabilistic model of the data's latent space and generate new samples by sampling from this distribution."
What is the difference between batch gradient descent and mini-batch gradient descent?,R4,"Batch gradient descent uses all data for every step, mini-batch uses random samples only once during training.",0.3,"Batch gradient descent computes the gradient of the cost function using the entire training dataset, while mini-batch gradient descent computes the gradient using a randomly selected subset (mini-batch) of the training data."
What is the role of activation functions in neural networks?,R4,"They allow the network to perform complex computations by introducing non-linear properties, which is essential for learning from data.",0.95,"Activation functions introduce non-linearity into neural networks, allowing them to learn complex mappings between inputs and outputs. Common activation functions include sigmoid, tanh, ReLU, and softmax."
What is the difference between generative and discriminative models in machine learning?,R4,The difference is that generative models are unsupervised and discriminative models are supervised.,0.3,"Generative models learn the joint probability distribution of the input features and the target labels, while discriminative models learn the conditional probability distribution of the target labels given the input features."
What is the purpose of dropout regularization in neural networks?,R4,Dropout adds noise to the input data to help the network become more robust against variations.,0.3,"Dropout regularization is a technique used in neural networks to prevent overfitting by randomly dropping out a fraction of neurons during training, forcing the network to learn more robust representations."
What is the role of LSTMs in sequence modeling tasks?,R4,"LSTMs use special gating mechanisms like input, forget, and output gates to selectively retain or discard information over time, making them suitable for tasks like speech recognition and language modeling.",0.95,"Long Short-Term Memory (LSTM) networks are a type of recurrent neural network (RNN) designed to capture long-range dependencies and temporal dynamics in sequential data, making them well-suited for tasks such as language modeling and time series prediction."
What is the difference between precision and recall in classification tasks?,R4,Precision and recall are the same metrics just named differently.,0.15,"Precision measures the proportion of true positive predictions among all positive predictions made by a classifier, while recall measures the proportion of true positive predictions among all actual positive instances in the data."
"What is the F1 score, and why is it useful in model evaluation?",R4,F1 score is the average of accuracy and recall used to evaluate models.,0.25,"The F1 score is the harmonic mean of precision and recall and provides a single metric to evaluate the performance of a classifier, balancing both precision and recall. It is particularly useful for imbalanced datasets where one class is much more prevalent than the others."
What is the purpose of word embedding in natural language processing (NLP)?,R4,"Word embeddings are mainly used to count how many times a specific word appears in a document. They provide a simple numerical tally for each word, which helps in creating basic frequency lists. This method is useful for very rudimentary text analysis, such as identifying the most common words in a text, but it does not capture any deeper meaning, context, or relationships between words, serving primarily as a basic word counter.",0.2,"Word embedding is a technique used to represent words as dense vectors in a continuous vector space, capturing semantic relationships between words and enabling neural networks to process textual data more effectively."
What is the difference between word2vec and GloVe in word embedding?,R4,"GloVe uses deep learning to predict words, and Word2Vec just counts word frequency in documents. That’s the core difference.",0.3,"Word2Vec is a model that learns word embeddings by predicting the surrounding words in a text corpus, while GloVe (Global Vectors for Word Representation) learns word embeddings by factorizing the co-occurrence matrix of words in the corpus."
What is the purpose of sentiment analysis in natural language processing (NLP)?,R4,"In NLP, sentiment analysis is mainly used to correct grammar and spelling based on user feelings.",0.1,"Sentiment analysis is a natural language processing task that involves determining the sentiment or opinion expressed in a piece of text, such as positive, negative, or neutral. It is commonly used to analyze customer reviews, social media posts, and survey responses."
What is the difference between shallow learning and deep learning?,R4,The key difference is that shallow learning uses neural networks and deep learning uses decision trees.,0.2,"Shallow learning refers to traditional machine learning algorithms with a small number of layers or features, while deep learning involves neural networks with multiple layers that can learn hierarchical representations of the data."
What is the role of dropout regularization in neural networks?,R4,Dropout is used only during testing to boost prediction speed by skipping unnecessary neurons.,0.2,"Dropout regularization is a technique used in neural networks to prevent overfitting by randomly dropping out a fraction of neurons during training, forcing the network to learn more robust representations."
What is the difference between precision and accuracy in model evaluation?,R4,"Precision tells us how many times the model was right in total, while accuracy tells us how specific the model was with positives.",0.3,"Precision measures the proportion of true positive predictions among all positive predictions made by a classifier, while accuracy measures the proportion of correct predictions (both true positives and true negatives) among all predictions made by the classifier."
What is the difference between k-fold cross-validation and leave-one-out cross-validation?,R4,Leave-one-out is just a special case of k-fold where k equals the number of data points.,0.95,"K-fold cross-validation divides the data into k subsets and iteratively trains the model k times, each time using a different subset as the validation set, while leave-one-out cross-validation trains the model k times, each time using all but one data point as the training set and the remaining point as the validation set."
What is the Naive Bayes classifier and how does it work?,R4,"Naive Bayes ignores all feature relationships, so it's not useful for real-world data where features are connected.",0.4,"The Naive Bayes classifier is a probabilistic model based on Bayes' theorem with the ""naive"" assumption of feature independence. It calculates the probability of a class given the input features using the product of conditional probabilities."
What is the difference between correlation and covariance?,R4,"Covariance tells us if variables are identical or not, but correlation checks if they are normally distributed.",0.3,"Covariance measures the degree to which two variables change together, while correlation measures both the strength and direction of the linear relationship between two variables."
What is the purpose of a confusion matrix in classification tasks?,R4,"The confusion matrix shows only the correct predictions of the model, not the incorrect ones.",0.2,"A confusion matrix is a table that visualizes the performance of a classification model by comparing predicted class labels with true class labels. It shows the number of true positives, true negatives, false positives, and false negatives."
What is the difference between stratified sampling and random sampling?,R4,Stratified sampling and random sampling are just different names for the same process in data collection.,0.2,"Stratified sampling divides the population into homogeneous subgroups (strata) and samples from each stratum proportionally, while random sampling selects individuals randomly from the entire population without regard to strata."
"What is the difference between univariate, bivariate, and multivariate analysis?",R4,"Univariate analysis is only used in image processing, while multivariate is for text data.",0.1,"Univariate analysis examines the distribution and summary statistics of a single variable, bivariate analysis explores the relationship between two variables, and multivariate analysis involves analyzing multiple variables simultaneously."
What is the purpose of data preprocessing in machine learning?,R4,Preprocessing randomly changes the data to test the model’s robustness.,0.2,"Data preprocessing involves cleaning, transforming, and preparing raw data for machine learning algorithms. It includes tasks such as handling missing values, encoding categorical variables, and scaling features."
What is the bias of a statistical estimator?,R4,Bias is a mistake in data labeling that causes the model to perform poorly.,0.2,The bias of a statistical estimator measures the difference between the expected value of the estimator and the true value of the parameter being estimated. A biased estimator systematically overestimates or underestimates the true parameter value.
What is the variance of a statistical estimator?,R4,Variance refers to how far the predicted labels are from the true labels in classification problems.,0.2,"The variance of a statistical estimator measures the spread or variability of the estimator's values around its expected value. A high variance indicates that the estimator's values are scattered widely, while a low variance indicates that the values are clustered closely together."
What is the difference between parametric and non-parametric statistical tests?,R4,"Parametric tests are always more accurate than non-parametric ones, no matter the data type.",0.3,"Parametric statistical tests assume that the data follows a specific probability distribution (e.g., normal distribution), while non-parametric tests make no such assumptions and are based on ranks or frequencies."
What is the purpose of feature scaling in machine learning?,R4,Scaling removes irrelevant features from the dataset by shrinking their values.,0.2,"Feature scaling ensures that all features have the same scale or range, preventing certain features from dominating others during model training. Common scaling techniques include min-max scaling and standardization."
What is the difference between correlation and causation in statistics?,R4,"If two variables are correlated, then one definitely causes the other to change.",0.2,"Correlation measures the strength and direction of the linear relationship between two variables, while causation indicates that changes in one variable directly cause changes in another variable."
What is the Central Limit Theorem and why is it important?,R4,The CLT says the original population becomes normal if we keep taking samples from it.,0.2,"The Central Limit Theorem states that the sampling distribution of the sample mean approaches a normal distribution as the sample size increases, regardless of the shape of the population distribution. It is important because it allows us to make inferences about population parameters based on sample statistics."
What is the purpose of outlier detection in data analysis?,R4,Outlier detection is only useful in images and not applicable in other types of data.,0.1,"Outlier detection involves identifying observations that deviate significantly from the rest of the data. Outliers can distort statistical analyses and machine learning models, so detecting and handling them appropriately is important for accurate results."
What is the difference between supervised and unsupervised outlier detection methods?,R4,Supervised and unsupervised outlier detection are the same; both use labeled datasets to find outliers.,0.2,"Supervised outlier detection methods require labeled data with outlier labels, while unsupervised methods detect outliers based on the characteristics of the data itself without requiring labeled examples."
What is the difference between statistical inference and predictive modeling?,R4,"Statistical inference doesn’t require any assumptions, while predictive modeling needs the data to be normally distributed.",0.2,"Statistical inference involves drawing conclusions about a population based on a sample of data, while predictive modeling focuses on building models that make predictions about future or unseen data based on past observations."
What is the purpose of regularization in machine learning?,R4,The purpose of regularization is to completely remove features that have no variance in the dataset.,0.3,"Regularization is a technique used to prevent overfitting by adding a penalty term to the model's objective function, discouraging overly complex models. Common regularization techniques include L1 regularization (Lasso) and L2 regularization (Ridge)."
What is the difference between batch gradient descent and stochastic gradient descent?,R4,"In batch gradient descent, the model updates after each sample, but in SGD, it waits until all samples are processed.",0.2,"Batch gradient descent updates model parameters using the average gradient of the entire training dataset, while stochastic gradient descent updates parameters using the gradient of a single randomly selected data point or mini-batch."
What is the difference between statistical modeling and machine learning?,R4,"Statistical modeling is only used for historical data, while machine learning is used only for future data.",0.2,"Statistical modeling focuses on understanding the relationships between variables in a probabilistic framework, while machine learning emphasizes building algorithms that can learn patterns and make predictions from data without explicitly programming them."
What is the purpose of feature selection in machine learning?,R4,Feature selection randomly drops columns to make the dataset smaller without considering their importance.,0.2,"Feature selection involves selecting the most relevant features from a dataset to improve model performance, reduce overfitting, and enhance interpretability."
What is the difference between regularization and feature selection in machine learning?,R4,"Regularization increases the number of features, while feature selection tries to remove useful data.",0.2,"Regularization penalizes overly complex models by adding a penalty term to the objective function, while feature selection explicitly selects a subset of features to include in the model based on their importance or relevance."
What is the difference between grid search and random search for hyperparameter tuning?,R4,"Grid search randomly selects values, and random search goes through all the options one by one.",0.2,"Grid search exhaustively searches through a specified grid of hyperparameter values, while random search samples hyperparameter values randomly from predefined distributions."
What is the purpose of cross-validation in model evaluation?,R4,"Cross-validation is only used for tuning the number of features, not for evaluating the model.",0.2,Cross-validation is a technique used to assess the performance of a predictive model by splitting the data into multiple subsets. It helps to estimate the model's generalization error and detect overfitting.
What is the difference between precision and recall in classification evaluation?,R4,"Precision measures errors, while recall measures model speed.",0.2,"Precision measures the proportion of true positive predictions among all positive predictions made by a classifier, while recall measures the proportion of true positive predictions among all actual positive instances in the data."
"What is the F1 score, and why is it useful in imbalanced datasets?",R4,F1 score is a type of accuracy score that ignores false negatives completely.,0.2,The F1 score is the harmonic mean of precision and recall and provides a single metric to evaluate the performance of a classifier. It is particularly useful for imbalanced datasets where one class is much more prevalent than the others.
What is the difference between bagging and boosting ensemble methods?,R4,"Bagging and boosting are both random forest algorithms, so there’s no difference between them.",0.2,"Bagging (Bootstrap Aggregating) builds multiple models independently and combines their predictions through averaging or voting, while boosting sequentially builds models by giving more weight to misclassified instances."
What is the purpose of principal component analysis (PCA) in dimensionality reduction?,R4,PCA works by deleting features randomly until only the useful ones are left.,0.2,"Principal Component Analysis (PCA) is used to reduce the dimensionality of a dataset by transforming the original features into a new set of orthogonal principal components, which capture the maximum variance in the data."
What is the difference between classification and regression in machine learning?,R4,Classification predicts numbers and regression predicts classes.,0.2,"Classification is a supervised learning task where the goal is to predict categorical labels or classes, while regression is a supervised learning task where the goal is to predict continuous numerical values."
What is the purpose of natural language processing (NLP) in data science?,R4,NLP only works for English text and cannot be used for other languages in data science projects.,0.2,"Natural language processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human languages. It enables computers to understand, interpret, and generate human language data."
What is the difference between bag-of-words and word embeddings in NLP?,R4,"Bag-of-words uses neural networks, while word embeddings just count the number of words.",0.2,"Bag-of-words represents text as a collection of word counts or frequencies, ignoring word order and grammar, while word embeddings represent words as dense vectors in a continuous vector space, capturing semantic relationships between words."
What is the difference between sentiment analysis and topic modeling in NLP?,R4,"Sentiment analysis groups similar topics together, and topic modeling assigns emotional scores to each paragraph.",0.2,"Sentiment analysis determines the sentiment or opinion expressed in a piece of text, such as positive, negative, or neutral, while topic modeling identifies the underlying topics or themes present in a collection of documents."
What is the purpose of word2vec in NLP?,R4,Word2Vec is used to count how many times each word appears in a document.,0.2,Word2Vec is a model that learns word embeddings by predicting the surrounding words in a text corpus. It captures semantic relationships between words and enables algorithms to process textual data more effectively.
What is the difference between a generative model and a discriminative model in machine learning?,R4,"Discriminative models are used for creating fake data, while generative models are used for classification tasks.",0.2,"A generative model learns the joint probability distribution of the input features and the target labels, while a discriminative model learns the conditional probability distribution of the target labels given the input features."
What is the difference between deep learning and traditional machine learning algorithms?,R4,"Deep learning works only on images, while traditional machine learning is used for all other types of data.",0.2,"Deep learning involves neural networks with multiple layers that can learn hierarchical representations of the data, while traditional machine learning algorithms typically use handcrafted features and simpler models."
What is the purpose of transfer learning in deep learning?,R4,"Transfer learning trains models from scratch every time, so it takes longer than standard training.",0.2,"Transfer learning is a technique where a model trained on one task is reused as the starting point for a model on a related task. It enables the transfer of knowledge and features learned from one domain to another, reducing the need for large labeled datasets."
What is the difference between convolutional neural networks (CNNs) and recurrent neural networks (RNNs)?,R4,"CNNs are better for remembering long sequences, while RNNs are better for recognizing shapes in pictures.",0.2,"Convolutional neural networks (CNNs) are well-suited for processing grid-like data such as images, while recurrent neural networks (RNNs) are designed to handle sequential data with temporal dependencies such as text or time series."
What is the purpose of batch normalization in neural networks?,R4,Batch normalization turns a neural network into a convolutional network by adjusting the filters.,0.1,Batch normalization is a technique used in neural networks to improve training stability and speed by normalizing the activations of each layer. It reduces internal covariate shift and accelerates convergence during training.
What is the difference between a feedforward neural network and a recurrent neural network?,R4,"Feedforward networks can only be used for images, and RNNs are used only for video games.",0.1,"A feedforward neural network consists of multiple layers of neurons connected in a forward direction without cycles, while a recurrent neural network has connections that form directed cycles, allowing it to maintain state or memory over time."
What is the difference between LSTMs and GRUs in recurrent neural networks?,R4,There is no difference between LSTMs and GRUs; they are exactly the same.,0.1,"LSTMs (Long Short-Term Memory) and GRUs (Gated Recurrent Units) are both types of recurrent neural network architectures designed to capture long-range dependencies in sequential data, but they differ in their internal gating mechanisms and computational complexity."
What is the purpose of attention mechanisms in neural networks?,R4,The main use of attention is to make training faster by skipping hidden layers in the network.,0.1,Attention mechanisms in neural networks enable the model to focus on relevant parts of the input data while ignoring irrelevant information. They have been particularly successful in natural language processing tasks such as machine translation and text summarization.
What is the difference between generative adversarial networks (GANs) and autoencoders in deep learning?,R4,"Autoencoders train a discriminator to detect fake data, but GANs do not have any discriminators.",0.1,"Generative adversarial networks (GANs) consist of two neural networks (generator and discriminator) that compete with each other to generate realistic data samples, while autoencoders consist of an encoder and decoder network that reconstruct input data."
What is the purpose of reinforcement learning in machine learning?,R4,Reinforcement learning is a way to memorize large datasets by feeding them through recurrent layers.,0.1,Reinforcement learning is a machine learning paradigm where an agent learns to make decisions by interacting with an environment to maximize cumulative rewards. It is commonly used in settings where actions influence future states and the agent learns through trial and error.
What is the difference between Q-learning and policy gradient methods in reinforcement learning?,R4,"Q-learning always uses a neural network, while policy gradients use decision trees to pick actions.",0.1,"Q-learning is a model-free reinforcement learning algorithm that learns the optimal action-selection policy by iteratively updating a Q-value function, while policy gradient methods directly parameterize the policy and update the policy parameters based on gradients of expected rewards."
What is the purpose of data preprocessing in exploratory data analysis (EDA)?,R4,The goal of preprocessing is to delete all outliers and errors so nothing unusual remains in the dataset.,0.2,"Data preprocessing in EDA involves cleaning, transforming, and preparing raw data for analysis. It includes tasks such as handling missing values, removing outliers, and encoding categorical variables."
What are some common techniques for handling missing data in EDA?,R4,Missing data is usually left as-is because models can learn to ignore them automatically.,0.2,"Common techniques for handling missing data in EDA include imputation (replacing missing values with estimates), deletion (removing rows or columns with missing values), and prediction (using models to estimate missing values)."
What is the difference between univariate and bivariate analysis in EDA?,R4,"Univariate is used only for categorical data, and bivariate is used only for numerical data.",0.1,"Univariate analysis examines the distribution and summary statistics of a single variable, while bivariate analysis explores the relationship between two variables."
What is the purpose of data visualization in EDA?,R4,"Data visualization is only used after model training to present final results, not during EDA.",0.2,"Data visualization in EDA helps to explore and understand the structure, patterns, and relationships in the data. It provides insights that may not be apparent from raw data or summary statistics alone."
What are some common types of data visualization used in EDA?,R4,Line graphs are never used in EDA since they are only for time series forecasting.,0.1,"Common types of data visualization used in EDA include histograms, scatter plots, box plots, bar charts, line plots, and heatmaps."
What is the difference between continuous and categorical variables in EDA?,R4,"Categorical variables can have infinite values, while continuous variables can only take a few fixed options.",0.2,"Continuous variables are numeric variables with an infinite number of possible values within a range, while categorical variables are qualitative variables with a finite number of distinct categories or levels."
What is the purpose of statistical summaries in EDA?,R4,"Statistical summaries are only used after machine learning models are trained, not during EDA.",0.2,"Statistical summaries in EDA provide descriptive statistics such as mean, median, mode, variance, and percentiles to summarize the central tendency, dispersion, and shape of the data distribution."
What is the difference between central tendency and dispersion measures in EDA?,R4,"Dispersion and central tendency both calculate the same thing, just using different formulas.",0.2,"Central tendency measures (e.g., mean, median, mode) describe the central or typical value of a dataset, while dispersion measures (e.g., variance, standard deviation) quantify the spread or variability of the data around the central value."
What is the purpose of hypothesis testing in EDA?,R4,Hypothesis testing is mainly used to clean the data by removing outliers from the dataset.,0.1,Hypothesis testing in EDA is used to evaluate whether observed differences or relationships in the data are statistically significant or due to random chance. It involves formulating null and alternative hypotheses and conducting statistical tests to make inferences about population parameters.
What is the difference between parametric and non-parametric tests in hypothesis testing?,R4,Non-parametric tests are more accurate because they always use the entire population instead of a sample.,0.2,"Parametric tests assume that the data follows a specific probability distribution (e.g., normal distribution), while non-parametric tests make no such assumptions and are based on ranks or frequencies."
What is the purpose of correlation analysis in EDA?,R4,Correlation analysis helps convert text data into numbers using coefficients.,0.2,Correlation analysis in EDA measures the strength and direction of the linear relationship between two continuous variables. It helps to identify patterns and associations in the data.
What is the difference between Pearson correlation and Spearman correlation?,R4,Spearman correlation is more accurate than Pearson because it always gives higher values.,0.2,"Pearson correlation measures the linear relationship between two continuous variables, while Spearman correlation measures the monotonic relationship between variables, allowing for non-linear associations."
What is the purpose of regression analysis in EDA?,R4,Regression analysis is used in EDA mainly to create pie charts from numeric data.,0.1,Regression analysis in EDA is used to model the relationship between one or more independent variables (predictors) and a dependent variable (outcome). It helps to understand how changes in predictors affect the outcome variable.
What are the assumptions of linear regression in EDA?,R4,Linear regression works even if the relationship between variables is non-linear and the errors are skewed.,0.2,"The assumptions of linear regression in EDA include linearity (relationship between variables is linear), independence (residuals are independent), homoscedasticity (residuals have constant variance), and normality (residuals are normally distributed)."
What is the difference between simple linear regression and multiple linear regression?,R4,"Simple linear regression is used only for numbers, while multiple regression can handle images and text as well.",0.2,"Simple linear regression models the relationship between one independent variable and a dependent variable, while multiple linear regression models the relationship between multiple independent variables and a dependent variable."
What is the purpose of logistic regression in EDA?,R4,"In EDA, logistic regression is used to handle missing values in a dataset by filling them based on probabilities.",0.2,Logistic regression in EDA is used to model the relationship between one or more independent variables (predictors) and a binary outcome variable (0 or 1). It is commonly used for classification tasks.
What are the assumptions of logistic regression in EDA?,R4,"A main assumption is that logistic regression only works when all variables are categorical. If any are numerical, the model will fail.",0.1,"The assumptions of logistic regression in EDA include linearity (relationship between predictors and log-odds is linear), independence (observations are independent), and absence of multicollinearity (predictors are not highly correlated)."
What is the purpose of outlier detection in EDA?,R4,Outlier detection is only used when the dataset has missing values. It helps fill in those gaps automatically.,0.1,"Outlier detection in EDA involves identifying observations that deviate significantly from the rest of the data. Outliers can distort statistical analyses and machine learning models, so detecting and handling them appropriately is important for accurate results."
What are some common techniques for detecting outliers in EDA?,R4,Outliers can only be detected by converting all values to binary and checking which ones are '0'. Anything else is treated as noise.,0.1,"Common techniques for detecting outliers in EDA include graphical methods (e.g., scatter plots, box plots), statistical methods (e.g., z-score, modified z-score), and proximity-based methods (e.g., nearest neighbors)."
What is the purpose of box plots in EDA?,R4,Box plots are mainly used to calculate the correlation between multiple variables during EDA.,0.1,"Box plots in EDA provide a visual summary of the distribution, central tendency, and spread of a dataset. They show the median, quartiles, and potential outliers of the data distribution."
What is the purpose of histograms in EDA?,R4,"In EDA, histograms automatically remove all outliers by trimming bars on both ends of the plot.",0.2,Histograms in EDA display the frequency distribution of a continuous variable by dividing the data into intervals (bins) and plotting the number of observations in each interval. They provide insights into the shape and spread of the data distribution.
What is the purpose of scatter plots in EDA?,R4,Scatter plots sort categorical values into bins and display their frequency. That’s how they are useful during EDA.,0.1,"Scatter plots in EDA visualize the relationship between two continuous variables by plotting each data point as a point on a graph. They help to identify patterns, trends, and correlations in the data."
What is the purpose of bar charts in EDA?,R4,"In EDA, bar charts are used to draw regression lines for continuous data, making them helpful in model evaluation.",0.1,Bar charts in EDA represent categorical data by displaying the frequency or proportion of observations in each category as bars of varying heights. They are useful for comparing the distribution of categorical variables.
What is the purpose of line plots in EDA?,R4,"Line plots are used to compare different categories. Each bar shows a group, and you can see which one is the tallest.",0.1,Line plots in EDA visualize the trend or pattern of a variable over time or another continuous dimension by connecting data points with lines. They are commonly used for time series analysis and trend identification.
What is the purpose of heatmaps in EDA?,R4,Heatmaps are mainly used to normalize data by converting it into temperature values that are easier to understand.,0.1,Heatmaps in EDA visualize the magnitude of a variable or the strength of a relationship between variables using colors. They are useful for identifying patterns and correlations in large datasets.
What is the purpose of correlation matrices in EDA?,R4,"Correlation matrices are used to measure the distance between data points, helping in clustering analysis during EDA.",0.1,Correlation matrices in EDA display the pairwise correlations between variables in a tabular format or heatmap. They help to identify patterns and associations among variables.
What is the difference between correlation and causation in EDA?,R4,"Causation is when data is highly correlated, and correlation is when data has no trend. EDA uses both to draw conclusions.",0.2,"Correlation measures the strength and direction of the linear relationship between two variables, while causation indicates that changes in one variable directly cause changes in another variable. Correlation does not imply causation."
What is the purpose of statistical tests in EDA?,R4,Statistical tests are mainly used in EDA to draw histograms and scatter plots for visualizing the distribution of variables.,0.2,Statistical tests in EDA are used to assess the significance of observed differences or relationships in the data. They help to make inferences about population parameters and draw conclusions from sample data.
What is the difference between parametric and non-parametric tests in EDA?,R4,Non-parametric tests are used for numerical data and parametric tests for categorical data. That’s their main difference in EDA.,0.2,"Parametric tests assume that the data follows a specific probability distribution (e.g., normal distribution), while non-parametric tests make no such assumptions and are based on ranks or frequencies."
What is the purpose of ANOVA in EDA?,R4,ANOVA is used to identify outliers by comparing the highest and lowest values in a dataset.,0.2,Analysis of Variance (ANOVA) in EDA is used to compare the means of two or more groups to determine whether they are significantly different. It is commonly used for comparing means across multiple categories or treatments.
What is the difference between one-way ANOVA and two-way ANOVA in EDA?,R4,Two-way ANOVA is just one-way ANOVA performed twice on different datasets. There is no difference in how they work.,0.1,"One-way ANOVA compares the means of two or more groups based on a single categorical factor, while two-way ANOVA considers the effects of two categorical factors (main effects and interaction) on the outcome variable."
What is the purpose of chi-squared test in EDA?,R4,Chi-squared test is mainly used to identify outliers by comparing data points with expected values in EDA.,0.2,The chi-squared test in EDA is used to assess the independence between two categorical variables by comparing the observed frequencies to the expected frequencies under the null hypothesis of independence.
What is the purpose of t-tests in EDA?,R4,T-tests are used to calculate correlations between variables and find linear relationships in the data.,0.2,T-tests in EDA are used to compare the means of two independent samples or the mean of a sample to a known value. They help to determine whether observed differences in means are statistically significant.
What is the difference between independent samples t-test and paired samples t-test in EDA?,R4,"Paired t-tests compare the variance between groups, and independent t-tests compare the means of the whole dataset.",0.15,"Independent samples t-test compares the means of two independent groups, while paired samples t-test compares the means of two related groups (e.g., before and after treatment) based on matched pairs of observations."
What is the purpose of non-parametric tests in EDA?,R4,Non-parametric tests are mainly used to visualize the data distribution by drawing histograms and scatter plots.,0.2,"Non-parametric tests in EDA are used when the data does not meet the assumptions of parametric tests (e.g., normal distribution, homogeneity of variance). They provide robust alternatives for analyzing non-normally distributed or skewed data."
What is the difference between Wilcoxon signed-rank test and Mann-Whitney U test in EDA?,R4,"Wilcoxon test is parametric, and Mann-Whitney is non-parametric; that's their main difference.",0.1,"Wilcoxon signed-rank test is a non-parametric test used to compare the medians of two related groups, while Mann-Whitney U test compares the medians of two independent groups."
What is the purpose of exploratory factor analysis (EFA) in EDA?,R4,EFA helps in understanding the structure of data by discovering hidden relationships and grouping variables that measure the same concept.,0.95,Exploratory factor analysis (EFA) in EDA is used to identify underlying factors or latent variables that explain the common variance among observed variables. It helps to uncover the underlying structure of a dataset.
What is the difference between EFA and principal component analysis (PCA) in EDA?,R4,"EFA is primarily used for data visualization, while PCA is used for hypothesis testing in EDA.",0.1,"EFA and PCA are both dimensionality reduction techniques used in EDA, but EFA aims to identify underlying factors that explain the common variance among observed variables, while PCA aims to capture the maximum variance in the data with orthogonal principal components."
What is the purpose of cluster analysis in EDA?,R4,It is used to create visual graphs showing how data points change over time in EDA.,0.1,Cluster analysis in EDA is used to identify groups or clusters of similar observations based on their characteristics or features. It helps to uncover natural groupings in the data and identify patterns or relationships.
What are some common algorithms used for cluster analysis in EDA?,R4,"DBSCAN clusters data by identifying dense regions and separating noise points, which helps find clusters of varying shapes.",0.9,"Common algorithms used for cluster analysis in EDA include K-means clustering, hierarchical clustering, DBSCAN (Density-Based Spatial Clustering of Applications with Noise), and Gaussian mixture models."
What is the difference between K-means clustering and hierarchical clustering in EDA?,R4,"K-means clustering is used only for categorical data, and hierarchical clustering is for numerical data.",0.2,"K-means clustering partitions the data into K clusters by minimizing the within-cluster sum of squares, while hierarchical clustering builds a tree-like hierarchy of clusters by recursively merging or splitting clusters based on proximity."
What is the purpose of dendrogram in hierarchical clustering in EDA?,R4,Dendrograms show the sequence in which data points are combined into clusters and the distance between clusters at each step.,0.9,A dendrogram in hierarchical clustering visualizes the hierarchical relationships between clusters by displaying the merging order and distance between clusters. It helps to identify the optimal number of clusters and interpret the clustering results.
What is the purpose of silhouette analysis in cluster validation in EDA?,R4,It visualizes data distribution across clusters using scatter plots in EDA.,0.2,"Silhouette analysis in cluster validation assesses the quality of clustering results by calculating silhouette coefficients for each data point, which measure the cohesion within clusters and separation between clusters. Higher silhouette coefficients indicate better-defined clusters."
What is the purpose of dimensionality reduction techniques in EDA?,R4,Dimensionality reduction is used to increase the number of variables to better fit machine learning models.,0.1,Dimensionality reduction techniques in EDA are used to reduce the number of features or variables in a dataset while preserving the most important information. They help to overcome the curse of dimensionality and improve computational efficiency.
What are some common dimensionality reduction techniques used in EDA?,R4,"Autoencoders, a type of neural network, can be used for nonlinear dimensionality reduction.",0.9,"Common dimensionality reduction techniques used in EDA include principal component analysis (PCA), factor analysis, t-distributed stochastic neighbor embedding (t-SNE), and linear discriminant analysis (LDA)."
What is the purpose of PCA in EDA?,R4,PCA finds uncorrelated linear combinations of variables which helps to reduce redundancy in the dataset.,0.95,Principal Component Analysis (PCA) in EDA is used to reduce the dimensionality of a dataset by transforming the original features into a new set of orthogonal principal components. It helps to identify the underlying structure and patterns in high-dimensional data.
What is the difference between PCA and factor analysis in EDA?,R4,"Factor analysis requires assumptions about data distribution, but PCA is a purely mathematical transformation without such assumptions.",0.9,"PCA and factor analysis are both dimensionality reduction techniques used in EDA, but PCA aims to capture the maximum variance in the data with orthogonal principal components, while factor analysis aims to identify underlying factors that explain the common variance among observed variables."
What is the purpose of t-SNE in EDA?,R4,It creates a scatter plot that shows how similar data points are based on their distances in high-dimensional space.,0.95,t-Distributed Stochastic Neighbor Embedding (t-SNE) in EDA is a technique used for visualizing high-dimensional data in a lower-dimensional space while preserving the local structure and relationships between data points. It is commonly used for visualizing clusters and patterns in complex datasets.
What is the difference between linear discriminant analysis (LDA) and PCA in EDA?,R4,"PCA is mainly for classification, while LDA is mainly for regression tasks in EDA.",0.2,"Linear Discriminant Analysis (LDA) and PCA are both dimensionality reduction techniques used in EDA, but LDA aims to maximize the separation between classes or groups while reducing dimensionality, whereas PCA focuses on capturing the maximum variance in the data regardless of class labels."
What is the purpose of feature selection in EDA?,R4,It is mainly used to create new features from existing ones to improve model accuracy.,0.3,"Feature selection in EDA involves selecting the most relevant features from a dataset to improve model performance, reduce overfitting, and enhance interpretability. It helps to identify the subset of features that contribute the most to predicting the target variable."
What are some common feature selection techniques used in EDA?,R4,Recursive feature elimination repeatedly removes least important features based on model performance until the best subset is found.,0.9,"Common feature selection techniques used in EDA include filter methods (e.g., correlation analysis, information gain), wrapper methods (e.g., recursive feature elimination, forward/backward selection), and embedded methods (e.g., LASSO, decision trees)."
What is the purpose of ensemble methods in EDA?,R4,These methods increase model variance to make predictions more diverse.,0.1,"Ensemble methods in EDA combine multiple models or algorithms to improve predictive performance, robustness, and generalization. They help to reduce overfitting and capture diverse patterns in the data."
What are some common ensemble methods used in EDA?,R4,AdaBoost is a boosting technique that adjusts weights on misclassified samples to focus learning.,0.9,"Common ensemble methods used in EDA include bagging (Bootstrap Aggregating), boosting (e.g., AdaBoost, Gradient Boosting), and stacking (meta-learning). They combine predictions from multiple base models to make more accurate and reliable predictions."
What is the purpose of model evaluation in EDA?,R4,Model evaluation is mainly about visualizing data distributions to find anomalies.,0.2,Model evaluation in EDA involves assessing the performance of predictive models using various metrics and techniques. It helps to determine the effectiveness of the models in capturing patterns and making accurate predictions on unseen data.
What are some common metrics used for model evaluation in EDA?,R4,"Log-loss measures the difference between predicted probabilities and actual labels, useful in classification.",0.9,"Common metrics used for model evaluation in EDA include accuracy, precision, recall, F1 score, ROC-AUC score, mean squared error (MSE), and R-squared (R2) score. They provide insights into different aspects of model performance."
What is the difference between cross-validation and holdout validation in model evaluation in EDA?,R4,"Holdout validation uses 100% of the data for training, and cross-validation uses only 10% for training.",0.1,"Cross-validation in model evaluation splits the data into multiple subsets (folds) for training and testing, while holdout validation splits the data into two disjoint sets (training and testing). Cross-validation provides more reliable estimates of model performance but is computationally more expensive."
What is the purpose of hyperparameter tuning in model evaluation in EDA?,R4,"Hyperparameter tuning is only needed when the dataset is imbalanced, otherwise it’s not useful.",0.1,Hyperparameter tuning in model evaluation involves optimizing the hyperparameters of machine learning algorithms to improve model performance and generalization. It helps to find the best combination of hyperparameters for a given dataset and problem.
What are some common techniques for hyperparameter tuning in EDA?,R4,Gradient Descent is often used as a hyperparameter tuning method to minimize loss functions.,0.2,"Common techniques for hyperparameter tuning in EDA include grid search, random search, Bayesian optimization, and genetic algorithms. They systematically search the hyperparameter space to find the optimal values for model performance."
What is the purpose of model interpretation in EDA?,R4,"By interpreting models, we can identify which variables are most influential, helping analysts focus on the most relevant features early in the analysis process.",0.9,"Model interpretation in EDA involves understanding and explaining how predictive models make decisions and predictions. It helps to gain insights into the underlying patterns, relationships, and features that contribute to the model's predictions."
What are some common techniques for model interpretation in EDA?,R4,Partial Dependence Plots (PDPs) show the effect of one or two features on the predicted outcome by averaging out the influence of other features.,0.9,"Common techniques for model interpretation in EDA include feature importance analysis (e.g., permutation importance, SHAP values), partial dependence plots, individual conditional expectation (ICE) plots, and model-agnostic interpretation methods (e.g., LIME, SHAP)."
What is the purpose of model deployment in EDA?,R4,Deployment is mainly used to create dashboards from EDA so users can visualize results in real time.,0.2,Model deployment in EDA involves deploying predictive models into production environments to make real-time predictions on new data. It allows organizations to leverage the insights and predictions generated from data analysis for decision-making and business applications.
What are some common techniques for model deployment in EDA?,R4,Common deployment techniques in EDA involve training models and saving them in CSV files for quick access and analysis.,0.2,"Common techniques for model deployment in EDA include deploying models as web services or APIs, embedding models into applications or dashboards, and integrating models with existing business processes or workflows. They enable seamless integration of predictive models into operational systems and workflows."
What are the common types of regularization used in machine learning?,R4,"Elastic Net combines both L1 and L2 penalties, making it useful when we want both regularization and feature selection.",0.95,"Common types of regularization used in machine learning include L1 regularization (Lasso), L2 regularization (Ridge), and elastic net regularization (combination of L1 and L2 penalties)."
What is the bias-variance trade-off in machine learning?,R4,"Bias is the model’s accuracy, and variance is the model’s speed. So the trade-off helps you pick which one to prioritize during training.",0.2,"The bias-variance trade-off refers to the balance between bias (error due to overly simplistic assumptions) and variance (error due to sensitivity to fluctuations in the training data) in machine learning models. Reducing bias typically increases variance, and vice versa."
What are the common types of cross-validation techniques used in machine learning?,R4,We usually use time-series cross-validation for static datasets and k-means clustering cross-validation for temporal ones.,0.3,"Common types of cross-validation techniques used in machine learning include k-fold cross-validation, stratified k-fold cross-validation, leave-one-out cross-validation, and nested cross-validation."
What is the purpose of hyperparameter tuning in machine learning?,R4,"Tuning hyperparameters changes the structure of the dataset, which helps algorithms like K-means and PCA find better clusters.",0.2,Hyperparameter tuning in machine learning involves optimizing the hyperparameters of models to improve their performance and generalization. It helps to find the best combination of hyperparameters for a given dataset and problem.
What are some common techniques for hyperparameter tuning in machine learning?,R4,"Tuning techniques include normalization and standardization, which adjust the hyperparameters for faster convergence.",0.2,"Common techniques for hyperparameter tuning in machine learning include grid search, random search, Bayesian optimization, genetic algorithms, and gradient-based optimization methods. They systematically search the hyperparameter space to find optimal values."
What is the difference between supervised and unsupervised learning in machine learning?,R4,"Supervised models are more advanced and accurate than unsupervised ones, which are usually outdated techniques.",0.2,"Supervised learning involves learning from labeled data with input-output pairs, where the model predicts the output variable based on input features. Unsupervised learning involves learning from unlabeled data to discover hidden patterns or structures in the data."
What are some common dimensionality reduction techniques used in machine learning?,R4,Dimensionality reduction is done using classification algorithms like SVM and KNN which simplify the dataset structure.,0.2,"Common dimensionality reduction techniques used in machine learning include principal component analysis (PCA), linear discriminant analysis (LDA), t-distributed stochastic neighbor embedding (t-SNE), and autoencoders."
What is the purpose of model interpretation in machine learning?,R4,The goal of interpretation is to visualize the dataset in 2D to make better training sets for neural networks.,0.2,"Model interpretation in machine learning involves understanding and explaining how predictive models make decisions and predictions. It helps to gain insights into the underlying patterns, relationships, and features that contribute to the model's predictions."
What are some common techniques for model interpretation in machine learning?,R4,Confusion matrices and ROC curves are the main tools used for interpreting models in unsupervised learning.,0.2,"Common techniques for model interpretation in machine learning include feature importance analysis (e.g., permutation importance, SHAP values), partial dependence plots, individual conditional expectation (ICE) plots, and model-agnostic interpretation methods (e.g., LIME, SHAP)."
What is the purpose of model deployment in machine learning?,R4,Deployment means saving the model as a CSV file so it can be shared with other developers.,0.2,Model deployment in machine learning involves deploying predictive models into production environments to make real-time predictions on new data. It allows organizations to leverage the insights and predictions generated from data analysis for decision-making and business applications.
What are some common techniques for model deployment in machine learning?,R4,One way to deploy models is to print out their predictions manually and use them in spreadsheets.,0.2,"Common techniques for model deployment in machine learning include deploying models as web services or APIs, embedding models into applications or dashboards, and integrating models with existing business processes or workflows. They enable seamless integration of predictive models into operational systems and workflows."
What is the purpose of transfer learning in machine learning?,R4,Transfer learning means transferring the model to a new computer so it can run faster and process more data.,0.2,"Transfer learning in machine learning is a technique where a model trained on one task is reused as the starting point for a model on a related task. It enables the transfer of knowledge and features learned from one domain to another, reducing the need for large labeled datasets."
What are some common applications of transfer learning in machine learning?,R4,Transfer learning is mainly used in hardware to transfer AI chips between devices without retraining models.,0.2,"Common applications of transfer learning in machine learning include image classification, natural language processing (NLP), and speech recognition. Pre-trained models such as BERT, ResNet, and GPT are often used as starting points for transfer learning tasks."
What is the purpose of feature engineering in machine learning?,R4,Feature engineering is only done to make datasets smaller by removing all missing values.,0.25,"Feature engineering involves selecting, transforming, or creating new features from the raw data to improve the performance of machine learning models by making them more expressive, informative, and suitable for the task at hand."
What are some common techniques for feature engineering?,R4,The main method is to use PCA to remove all features that are not numeric.,0.3,"Common techniques include one-hot encoding for categorical variables, scaling numerical features, creating interaction terms, binning or discretizing continuous variables, and extracting features from text or images using techniques like TF-IDF or CNNs."
What is the difference between batch processing and real-time processing?,R4,"Batch processing is used for live systems like stock trading apps, and real-time is used for reporting systems like monthly sales reports.",0.25,"Batch processing involves processing data in large, discrete batches or groups, typically on a scheduled or periodic basis, while real-time processing involves handling data as soon as it is generated or received, often requiring low-latency and immediate responses."
What is transfer learning in NLP?,R4,"In NLP, transfer learning is copying code from one project and reusing it in another NLP task.",0.2,"Transfer learning in NLP involves leveraging pre-trained language models, such as BERT or GPT, trained on large corpora of text data, and fine-tuning them on specific tasks or domains with smaller datasets to achieve better performance and faster convergence."
What is named entity recognition (NER)?,R4,Named Entity Recognition is about finding common words in a sentence and replacing them with random symbols.,0.25,"Named entity recognition is a subtask of information extraction that involves identifying and classifying named entities (e.g., persons, organizations, locations) mentioned in text documents, which is useful for tasks such as entity linking and relation extraction."
What are some challenges in working with unstructured data?,R4,"One major challenge is that unstructured data is always full of errors, so it's unreliable by nature.",0.3,"Challenges include extracting meaningful information from unstructured text, handling noisy or inconsistent data, dealing with large volumes of data, and ensuring privacy and security when working with sensitive information."
What is collaborative filtering in recommendation systems?,R4,Collaborative filtering means recommending products by analyzing their price and brand popularity.,0.3,"Collaborative filtering is a technique used in recommendation systems to generate personalized recommendations by analyzing similarities and patterns in users' behavior or preferences, often based on user-item interactions or user-user similarity."
What are some evaluation metrics used in recommendation systems?,R4,RMSE and MAE are used to measure the difference between predicted and actual ratings in rating-based recommenders.,0.95,"Evaluation metrics include precision, recall, F1 score, Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and ranking-based metrics like Mean Average Precision (MAP) and Normalized Discounted Cumulative Gain (NDCG)."
What is the Markov Decision Process (MDP)?,R4,Markov Decision Process is only used in finance to track stock prices over time.,0.2,"The Markov Decision Process is a mathematical framework used to model sequential decision-making problems, where an agent takes actions in an environment to maximize cumulative rewards, while satisfying the Markov property (future states depend only on the current state and action)."
What is policy gradient in reinforcement learning?,R4,It means randomly selecting actions in the environment until the agent gets the highest score.,0.3,"Policy gradient methods are a class of reinforcement learning algorithms that directly optimize the policy (the agent's strategy or behavior) by estimating gradients of expected rewards with respect to the policy parameters, typically using techniques like stochastic gradient ascent."
What is data augmentation in deep learning?,R4,Data augmentation means compressing data to make training faster and save memory.,0.2,"Data augmentation is a technique used to artificially increase the size and diversity of training datasets by applying transformations such as rotation, translation, scaling, cropping, or flipping to input data, which can improve model generalization and robustness."
What is sequence-to-sequence learning?,R4,Sequence-to-sequence learning is mainly used to detect outliers in time-series datasets.,0.3,"Sequence-to-sequence learning is a type of model architecture used for tasks involving input and output sequences of variable lengths, such as machine translation, summarization, and speech recognition, typically implemented using recurrent neural networks (RNNs) or transformers."
What is attention mechanism in deep learning?,R4,Attention mechanism is a process where the model adds more layers to increase accuracy without looking at input data.,0.2,"Attention mechanism is a mechanism used in neural networks to selectively focus on relevant parts of the input data while processing sequences, enabling the model to learn to weigh different input elements dynamically and attend to the most informative parts."
What are some common optimization algorithms used in deep learning?,R4,"The only optimization algorithm used in deep learning is k-means clustering, which finds centroids for each layer.",0.2,"Common optimization algorithms include stochastic gradient descent (SGD), Adam, RMSprop, and Adagrad, each with different update rules and learning rate adaptation strategies to optimize the model parameters efficiently."
What is semi-supervised learning?,R4,Semi-supervised learning is when the model is trained only on labeled data but evaluated on unlabeled data to save time.,0.3,"Semi-supervised learning is a machine learning paradigm that combines labeled and unlabeled data to improve model performance, often by leveraging the inherent structure or relationships in the data to semi-supervised learning algorithms."
What is self-supervised learning?,R4,Self-supervised learning is when the model watches YouTube tutorials to understand how to solve problems.,0.2,"Self-supervised learning is a form of unsupervised learning where the model learns to predict or generate certain properties or features of the input data itself, without requiring external labels or annotations, often used as a pretraining step for downstream tasks."
What is the Bellman equation in reinforcement learning?,R4,The Bellman equation is a formula for calculating the slope of the loss function during neural network training.,0.2,The Bellman equation is a fundamental equation in dynamic programming and reinforcement learning that expresses the value of a state or state-action pair in terms of the expected immediate reward and the value of the next state or next state-action pair.
What is the difference between on-policy and off-policy learning in reinforcement learning?,R4,"Off-policy learning requires the environment to provide labels for each action, unlike on-policy.",0.3,"On-policy learning involves learning the value or policy while following the current policy, while off-policy learning involves learning the value or policy while following a different behavior policy, often leading to more efficient exploration and better sample efficiency."
What are the main components of the data science process?,R4,The data science process is just about writing code to automate data entry tasks.,0.2,"The main components include data collection, data cleaning and preprocessing, exploratory data analysis, modeling, evaluation, and deployment."
What is overfitting in machine learning?,R4,It is a problem where the model performs well on test data but poorly on training data.,0.2,Overfitting occurs when a model learns the details and noise in the training data to the extent that it negatively impacts its performance on unseen data.
Explain the curse of dimensionality.,R4,"The curse of dimensionality is when the dataset gets smaller as dimensions increase, making it easier to train models.",0.2,"The curse of dimensionality refers to the increased difficulty of analyzing and processing data as the number of features or dimensions increases, leading to sparsity and computational challenges."
What is the purpose of cross-validation?,R4,It is a method to test the model only on the training data to ensure it fits perfectly.,0.2,"Cross-validation is used to assess the generalization performance of a model by partitioning the data into multiple subsets, training the model on some subsets, and evaluating it on the remaining subsets."
How does regularization prevent overfitting in neural networks?,R4,It adds noise to the input data to make the model robust against overfitting.,0.3,"Regularization techniques such as L1 and L2 regularization add penalty terms to the neural network's loss function, which discourages overly complex weight configurations and helps prevent overfitting."
What is the purpose of activation functions in neural networks?,R4,Activation functions are used to speed up the training process by reducing the number of layers.,0.2,"Activation functions introduce nonlinearity to neural networks, allowing them to learn complex patterns and relationships in the data by transforming the input signal from each neuron into an output signal."
What is the purpose of dimensionality reduction techniques?,R4,Dimensionality reduction always decreases model performance by losing data.,0.2,"Dimensionality reduction techniques are used to reduce the number of features or dimensions in a dataset while preserving its important information, which can help improve the performance and efficiency of machine learning algorithms."
Explain the difference between PCA and t-SNE.,R4,"PCA is better for visualizing data in two or three dimensions, while t-SNE works only for text data.",0.2,"PCA (Principal Component Analysis) is a linear dimensionality reduction technique that seeks to maximize variance, while t-SNE (t-Distributed Stochastic Neighbor Embedding) is a nonlinear technique that focuses on preserving local relationships between data points."
What is K-means clustering?,R4,The algorithm works by randomly guessing clusters and assigning points randomly.,0.1,"K-means clustering is an unsupervised learning algorithm used to partition a dataset into K clusters based on similarities in the data points' features, with the goal of minimizing the within-cluster sum of squares."
What is the elbow method used for in K-means clustering?,R4,It is a technique to speed up the K-means algorithm by reducing iterations.,0.2,"The elbow method is used to determine the optimal number of clusters (K) in K-means clustering by plotting the within-cluster sum of squares against the number of clusters and selecting the point where the rate of decrease sharply changes (the ""elbow"" point)."
What is outlier detection?,R4,Detection of outliers helps in improving model accuracy by handling unusual or noisy data.,1.0,"Outlier detection is the process of identifying data points or observations that deviate significantly from the rest of the dataset, which may indicate errors, anomalies, or interesting patterns."
What are the advantages of deep learning over traditional machine learning algorithms?,R4,"Traditional algorithms are more powerful than deep learning because they use linear regression, which works in every case.",0.1,"Deep learning models, particularly neural networks, can automatically learn hierarchical representations of data, handle large and complex datasets, and often achieve state-of-the-art performance in tasks such as image recognition and natural language processing."
What is backpropagation?,R4,It helps in finding the correct output by comparing all neuron outputs and choosing the most frequent one.,0.2,Backpropagation is a supervised learning algorithm used to train neural networks by iteratively adjusting the weights of connections between neurons in the network based on the error between predicted and actual outputs.
How do you evaluate the performance of a regression model?,R4,"To evaluate regression, we use confusion matrices and precision-recall curves, since those tell us where the model is making false predictions.",0.2,"Performance metrics for regression models include Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), R-squared (coefficient of determination), and others."
What is the purpose of the A/B test in data science?,R4,"A/B testing helps validate if a change made to a system leads to improvement, using statistical analysis to compare performance between control and variation.",1.0,"A/B testing is a statistical hypothesis testing method used to compare two or more versions of a product, webpage, or other elements to determine which one performs better in terms of predefined metrics."
What is Bayesian inference?,R4,It involves calculating probabilities using conditional probabilities and always assumes that the prior probability is zero.,0.3,"Bayesian inference is a statistical approach that uses Bayes' theorem to update the probability of a hypothesis or belief based on new evidence or observations, incorporating prior knowledge and uncertainty into the analysis."
What is the purpose of hypothesis testing?,R4,We use hypothesis testing to find the exact value of the population mean by comparing different samples.,0.3,"Hypothesis testing is used to make inferences or decisions about a population parameter based on sample data, by testing a null hypothesis against an alternative hypothesis using statistical methods."
What is regularization in linear regression?,R4,"In linear regression, regularization removes all features that are correlated, so that only uncorrelated features are left in the model.",0.2,"Regularization in linear regression involves adding a penalty term to the ordinary least squares (OLS) loss function to prevent overfitting, with common regularization techniques including Ridge regression (L2 regularization) and Lasso regression (L1 regularization)."
What are the assumptions of linear regression?,R4,Linear regression assumes that all variables must be normally distributed before applying the model.,0.4,"The assumptions of linear regression include linearity between the independent and dependent variables, homoscedasticity (constant variance of errors), independence of errors, and normality of error terms."
What is the difference between Type I and Type II errors?,R4,Type II error is more dangerous because it means you’re always rejecting everything without proof.,0.2,"Type I error occurs when a true null hypothesis is rejected (false positive), while Type II error occurs when a false null hypothesis is not rejected (false negative)."
What is the p-value in hypothesis testing?,R4,The p-value is the percentage of data points that support the alternative hypothesis.,0.3,"The p-value is the probability of observing a test statistic as extreme as, or more extreme than, the one observed, assuming that the null hypothesis is true. It is used to assess the strength of evidence against the null hypothesis."
What is the bias-variance tradeoff?,R4,This tradeoff is about reducing training time by either increasing bias or ignoring variance.,0.3,"The bias-variance tradeoff refers to the compromise between bias (error due to overly simplistic assumptions) and variance (error due to sensitivity to fluctuations in the training data) in machine learning models, where reducing one typically increases the other."
What is the difference between variance and standard deviation?,R4,Variance is always smaller than standard deviation because it’s not a square root.,0.2,"Variance measures the average squared deviation from the mean of a set of values, while standard deviation measures the average deviation from the mean, providing a measure of the dispersion or spread of the data."
What is the difference between a decision tree and a random forest?,R4,"The only difference is that decision trees are used for classification, while random forests are used for regression.",0.3,"A decision tree is a simple, interpretable tree-like structure that recursively splits the data based on features, while a random forest is an ensemble of decision trees that aggregates their predictions to improve performance and reduce overfitting."
What is batch normalization in neural networks?,R4,It works by averaging the output of the entire network after training is done.,0.2,"Batch normalization is a technique used to normalize the activations of each layer in a neural network by adjusting and scaling them to have zero mean and unit variance, which can accelerate training and improve model performance."
What is the purpose of a confusion matrix in classification?,R4,"It’s a table that breaks down the predictions of a classifier to help compute metrics like accuracy, precision, and recall.",1.0,"A confusion matrix is a table that summarizes the performance of a classification model by comparing predicted class labels with actual class labels, showing the counts of true positives, true negatives, false positives, and false negatives."
What is the Kullback-Leibler (KL) divergence?,R4,KL divergence is used to optimize learning rate in neural networks.,0.2,"The Kullback-Leibler divergence is a measure of the difference between two probability distributions, used in information theory and statistics to quantify the amount of information lost when one distribution is used to approximate another."
What is the difference between batch normalization and layer normalization in neural networks?,R4,They are the same thing — both normalize inputs to have zero mean and unit variance.,0.2,"Batch normalization normalizes the activations of each layer across the batch dimension, while layer normalization normalizes the activations across the feature dimension, making it more suitable for recurrent neural networks and other architectures with variable-length sequences."
What is unsupervised learning?,R4,It’s when the model is trained with feedback and known outputs to improve its predictions.,0.1,"Unsupervised learning is a type of machine learning where the model is trained on unlabeled data, and it seeks to find patterns or structures in the data without explicit supervision."
What are the main types of unsupervised learning techniques?,R4,Unsupervised learning is mainly about training deep neural networks with labeled data.,0.1,"The main types include clustering, dimensionality reduction, and association rule learning."
What are the common clustering algorithms?,R4,Decision Trees and Naive Bayes are types of clustering algorithms used for grouping.,0.2,"Common clustering algorithms include K-means clustering, hierarchical clustering, DBSCAN, and Gaussian mixture models (GMM)."
What is hierarchical clustering?,R4,Hierarchical clustering works only if the number of clusters is known in advance.,0.3,Hierarchical clustering is an algorithm that creates a hierarchy of clusters by recursively merging or splitting clusters based on their similarity or dissimilarity.
What is DBSCAN clustering?,R4,DBSCAN is a supervised learning technique used for classification tasks.,0.1,"DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a density-based clustering algorithm that groups together data points that are closely packed, while marking outliers as noise."
What are the common dimensionality reduction techniques?,R4,"One technique is K-Means, which reduces the number of classes in the data.",0.2,"Common techniques include Principal Component Analysis (PCA), t-Distributed Stochastic Neighbor Embedding (t-SNE), and Singular Value Decomposition (SVD)."
What is PCA (Principal Component Analysis)?,R4,PCA randomly drops columns from the dataset until only a few are left.,0.1,PCA is a linear dimensionality reduction technique that transforms the original features into a lower-dimensional space while preserving the maximum variance in the data.
What is t-SNE (t-Distributed Stochastic Neighbor Embedding)?,R4,It works by clustering data into rectangles to show high-level patterns in a dataset.,0.2,"t-SNE is a nonlinear dimensionality reduction technique that focuses on preserving local relationships between data points in a lower-dimensional space, often used for visualizing high-dimensional data."
What is Singular Value Decomposition (SVD)?,R4,SVD is a data cleaning technique that removes outliers by dropping extreme values.,0.2,"SVD is a matrix factorization technique used to decompose a matrix into the product of three matrices, which can be used for dimensionality reduction, data compression, and matrix approximation."
What is association rule learning?,R4,Association rule learning is mainly used in supervised learning to label the data.,0.2,"Association rule learning is a type of unsupervised learning used to discover interesting associations or relationships between variables in large datasets, commonly used in market basket analysis and recommendation systems."
What are the common association rule learning algorithms?,R4,FP-Growth improves efficiency over Apriori by avoiding candidate generation through a tree structure.,1.0,Common algorithms include Apriori and FP-Growth.
What is the Apriori algorithm?,R4,Apriori works by checking all possible combinations of features and selecting the most important ones.,0.2,"The Apriori algorithm is a classic algorithm for mining frequent itemsets and generating association rules, based on the principle of the Apriori property and candidate generation."
What is FP-Growth?,R4,FP-Growth randomly selects items from the dataset and builds a tree for visualization.,0.2,"FP-Growth (Frequent Pattern Growth) is an efficient algorithm for mining frequent itemsets and generating association rules without candidate generation, using a compact data structure called FP-tree."
What is anomaly detection?,R4,It only works for labeled datasets where all anomalies are known beforehand.,0.2,"Anomaly detection, also known as outlier detection, is the process of identifying data points or observations that deviate significantly from the rest of the dataset, which may indicate errors, anomalies, or interesting patterns."
What are the common anomaly detection techniques?,R4,The k-means algorithm is mainly used for anomaly detection by counting how many points are in each cluster.,0.5,"Common techniques include statistical methods, density-based methods, distance-based methods, and machine learning-based methods such as Isolation Forest and One-Class SVM."
What is Isolation Forest?,R4,It works by calculating the distance between points and labeling those that are far away as outliers.,0.4,"Isolation Forest is an unsupervised learning algorithm for anomaly detection that isolates anomalies by recursively partitioning the dataset into subsets using random splits, making anomalies easier to isolate than normal data points."
What is One-Class SVM?,R4,It tries to capture the region where most of the data lies and classifies points outside this region as outliers.,1.0,One-Class Support Vector Machine (SVM) is a machine learning algorithm for anomaly detection that learns a boundary around normal data points in the feature space and identifies anomalies as data points lying outside this boundary.
What is the purpose of density estimation?,R4,Kernel density estimation is a common non-parametric method used for this purpose.,1.0,"Density estimation is the process of estimating the probability density function of a random variable from a set of data points, often used in clustering, anomaly detection, and generative modeling."
What are the common density estimation techniques?,R4,Neural networks are widely used for density estimation by predicting probability densities directly.,0.3,"Common techniques include histogram-based methods, kernel density estimation (KDE), Gaussian mixture models (GMM), and Parzen window estimation."
What is the Gaussian Mixture Model (GMM)?,R4,GMM is a supervised learning method that requires labeled data to train the model.,0.2,"Gaussian Mixture Model (GMM) is a probabilistic model used for density estimation and clustering, assuming that the data is generated from a mixture of several Gaussian distributions."
What is the Expectation-Maximization (EM) algorithm?,R4,The algorithm guarantees convergence to the global maximum likelihood solution every time.,0.3,"The Expectation-Maximization (EM) algorithm is an iterative optimization algorithm used to estimate the parameters of probabilistic models with latent variables, such as Gaussian mixture models (GMM), by iteratively computing the expected values of the latent variables and maximizing the likelihood function."
What is the difference between generative and discriminative models?,R4,"Discriminative models include algorithms like logistic regression and SVM, whereas generative models include Naive Bayes and Hidden Markov Models.",1.0,"Generative models learn the joint probability distribution of the input features and the output labels, while discriminative models learn the conditional probability distribution of the output labels given the input features."
What is the purpose of generative modeling?,R4,They are used for data augmentation to improve model training with more examples.,1.0,"Generative modeling is used to model the underlying structure of the data and generate new samples from the learned distribution, often used in tasks such as image generation, text generation, and data augmentation."
What is data science?,R5,"Data science involves machine learning and AI. It’s the same thing as artificial intelligence because both are used to automate tasks using data and code. For example, chatbots and robots use data science to function.",0.4,"Data science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from data."
What are the key steps in the data science process?,R5,"Key steps include data acquisition, preprocessing, feature selection, model building, evaluation, and communicating results through dashboards or reports. Sometimes people also do hypothesis testing depending on the problem.",0.85,"The key steps typically include problem definition, data collection, data preparation, exploratory data analysis, modeling, evaluation, and deployment."
What is the difference between supervised and unsupervised learning?,R5,"Supervised learning models are trained on labeled datasets and used for tasks like classification and regression. Unsupervised learning models, however, find structures in unlabeled data, such as clusters. A common example is customer segmentation using clustering techniques like K-means.",0.95,"Supervised learning involves training a model on labeled data, where the algorithm learns the relationship between input features and target labels. Unsupervised learning deals with unlabeled data and aims to find hidden patterns or structures within the data."
Explain the bias-variance tradeoff.,R5,"Bias is always good because it helps the model be stable, while variance makes the model inconsistent. So the best models are the ones with only bias and no variance at all. That’s what keeps the model predictable.",0.2,The bias-variance tradeoff is the balance between the error introduced by bias (underfitting) and the error introduced by variance (overfitting) when building a machine learning model.
What is feature engineering?,R5,"It’s a technique to prepare input data before training the model. For example, in a dataset about people, we can convert names into lengths, birthdates into ages, and categorize income groups. All of this helps the model learn better patterns.",0.9,"Feature engineering is the process of selecting, transforming, and creating features from raw data to improve model performance."
How do you handle missing data in a dataset?,R5,"When handling missing data, first analyze the pattern of missingness. If data is missing at random, you can use imputation techniques like KNN imputer or regression models. Otherwise, it might be safer to remove the affected records or variables.",0.95,"Missing data can be handled by imputation techniques such as mean, median, or mode imputation, using predictive models to estimate missing values, or by deleting rows or columns with missing values, depending on the dataset size and nature."
What is cross-validation?,R5,"In k-fold cross-validation, the dataset is divided into k parts. The model is trained on k−1 parts and tested on the remaining one. This is repeated k times, and the results are averaged to get a more stable estimate of model performance.",1.0,Cross-validation is a technique used to assess the performance of a predictive model by splitting the data into multiple subsets.
Explain the concept of overfitting.,R5,"When a model performs well on the training set but fails to make accurate predictions on test data, it’s known as overfitting. This usually happens when the model learns specific details instead of general patterns.",0.9,"Overfitting occurs when a model learns the training data too well, capturing noise or random fluctuations in the data instead of the underlying pattern."
What are some common algorithms used in supervised learning?,R5,"Algorithms commonly used in supervised learning are logistic regression, SVMs, decision trees, and ensemble methods like random forests and gradient boosting. These are especially useful in tasks like spam detection, credit scoring, and image classification.",0.95,"Common algorithms include linear regression, logistic regression, decision trees, random forests, support vector machines, and neural networks."
What is dimensionality reduction?,R5,It involves selecting the most important features and removing the less useful ones. This can be done using algorithms like PCA or through manual feature selection based on correlation analysis or domain knowledge.,0.9,Dimensionality reduction is the process of reducing the number of input variables in a dataset by transforming them into a lower-dimensional space while preserving important information.
What is regularization in machine learning?,R5,It’s a method to control the complexity of a model by adding a penalty for higher weights. This helps in reducing overfitting and improving generalization. Regularization terms are added to the cost function during training.,0.9,Regularization is a technique used to prevent overfitting by adding a penalty term to the model's objective function.
What is ensemble learning?,R5,"Ensemble learning refers to the use of multiple weak learners, like small decision trees, to create a strong predictive model. Methods like boosting train each new model to correct the errors of the previous one.",0.95,Ensemble learning is a technique that combines multiple machine learning models to improve performance and robustness.
What is the ROC curve?,R5,The ROC curve is used in binary classification to analyze the performance of a model at different thresholds. It shows how the model balances true positives and false positives.,0.9,The receiver operating characteristic (ROC) curve is a graphical plot that illustrates the performance of a binary classification model across different threshold settings.
What is AUC-ROC?,R5,"AUC-ROC is useful in binary classification tasks. ROC plots TPR vs FPR and AUC measures the area under that curve. The higher the AUC, the better the model's ability to distinguish between positive and negative classes.",0.9,The area under the ROC curve (AUC-ROC) is a metric used to evaluate the performance of a binary classification model.
What is the difference between classification and regression?,R5,"The main difference is that classification deals with categorical outcomes, while regression is used for numerical predictions. Algorithms like logistic regression are used for classification, and linear regression for regression tasks.",0.9,"Classification is a supervised learning task where the goal is to predict categorical labels or classes, while regression is a supervised learning task where the goal is to predict continuous numerical values."
What is clustering?,R5,It is a method where the machine automatically groups data points based on their distance or similarity. K-means and DBSCAN are popular clustering techniques used in data science.,0.9,Clustering is an unsupervised learning technique that involves grouping similar data points together based on their features or characteristics.
What is the curse of dimensionality?,R5,"It refers to how high-dimensional data can make distance-based models like KNN less effective, as the distance between points becomes less meaningful and all points seem equally far apart.",0.9,The curse of dimensionality refers to the phenomenon where the performance of certain algorithms deteriorates as the number of features or dimensions in the data increases.
What is the difference between precision and recall?,R5,"Precision answers: ‘Of all predicted positives, how many were correct?’ Recall answers: ‘Of all actual positives, how many were predicted correctly?’ In imbalanced datasets, recall is important when missing a positive is costly.",0.95,"Precision measures the proportion of true positive predictions among all positive predictions made by a classifier, while recall measures the proportion of true positive predictions among all actual positive instances in the data."
What is the F1 score?,R5,The F1 score ranges from 0 to 1 and is calculated as 2 * (precision * recall) / (precision + recall). It is particularly useful when dealing with imbalanced classes in classification tasks.,1.0,The F1 score is the harmonic mean of precision and recall and provides a single metric to evaluate the performance of a classifier.
What is the bias of an estimator?,R5,"The bias of an estimator measures how far off the average predicted values are from the actual value. Ideally, we want this bias to be as close to zero as possible.",0.9,The bias of an estimator measures the difference between the expected value of the estimator and the true value of the parameter being estimated.
What is the variance of an estimator?,R5,Variance describes how spread out the estimator's predictions are across multiple samples. It's a key component in understanding the bias-variance tradeoff and model generalization.,0.95,The variance of an estimator measures the spread or variability of the estimator's values around its expected value.
What is the Central Limit Theorem?,R5,"According to the Central Limit Theorem, even if the original data is not normally distributed, the distribution of sample means will be nearly normal if the sample size is sufficiently large.",0.95,The Central Limit Theorem states that the sampling distribution of the sample mean approaches a normal distribution as the sample size increases.
What is regularization in neural networks?,R5,Regularization in neural networks helps prevent the model from learning noise in the training data. It does this by limiting the magnitude of weights or introducing randomness to reduce complexity.,0.9,Regularization in neural networks involves adding penalty terms to the loss function to prevent overfitting.
What is batch normalization?,R5,Batch normalization adjusts and scales the outputs of neurons so that they follow a standard distribution. This makes learning faster and allows for higher learning rates without instability.,0.9,Batch normalization is a technique used in neural networks to improve training stability and speed by normalizing the activations of each layer.
What is transfer learning?,R5,"It involves using knowledge gained while solving one problem and applying it to a different but related problem. For example, using a model trained on ImageNet to detect medical images.",0.95,Transfer learning is a machine learning technique where a model trained on one task is reused as the starting point for a model on a related task.
What is natural language processing (NLP)?,R5,"NLP deals with both text and speech inputs. It involves techniques like parsing, part-of-speech tagging, and named entity recognition to analyze the structure and meaning of language data.",0.95,Natural language processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human languages.
What is word embedding?,R5,"Word embedding helps convert words into vectors so that machine learning models can process them mathematically. These embeddings retain semantic meaning and can be used in classification, translation, and more.",0.95,Word embedding is a technique used to represent words as dense vectors in a continuous vector space.
What is sentiment analysis?,R5,"Sentiment analysis evaluates written or spoken language to determine if the sentiment is favorable, unfavorable, or neutral. It’s commonly used in applications like movie reviews and product ratings.",0.95,Sentiment analysis is a natural language processing task that involves determining the sentiment or opinion expressed in a piece of text.
What is deep learning?,R5,Deep learning enables machines to learn from unstructured data by using multiple layers of transformations. It improves with large amounts of data and computational power.,0.95,Deep learning is a subset of machine learning that focuses on artificial neural networks with multiple layers.
What is a convolutional neural network (CNN)?,R5,"CNNs apply filters to input images to extract important features and reduce dimensionality. They are widely used in facial recognition, self-driving cars, and medical image analysis.",0.95,"A convolutional neural network (CNN) is a type of deep neural network that is particularly well-suited for processing structured grid-like data, such as images."
What is a recurrent neural network (RNN)?,R5,"RNN uses its previous outputs as inputs for the next step. This feedback loop allows it to learn patterns over time, like in handwriting recognition or music generation.",0.95,A recurrent neural network (RNN) is a type of neural network designed to process sequential data by maintaining a state or memory of previous inputs.
What is reinforcement learning?,R5,"Reinforcement learning is used for training systems like game-playing bots and robotics, where trial and error helps the agent improve over time through exploration and exploitation.",0.95,Reinforcement learning is a machine learning paradigm where an agent learns to make decisions by interacting with an environment to maximize cumulative rewards.
What is Q-learning?,R5,"Q-learning uses a reward system to help agents choose the best action in each situation. Over time, it learns the optimal path by updating values based on rewards and future expectations.",0.9,Q-learning is a model-free reinforcement learning algorithm used to learn the optimal action-selection policy for a Markov decision process (MDP).
What is the exploration-exploitation tradeoff in reinforcement learning?,R5,"In reinforcement learning, too much exploitation can trap the agent in suboptimal actions, while too much exploration can slow down learning. A balance helps the agent learn effectively.",0.95,The exploration-exploitation tradeoff refers to the dilemma faced by reinforcement learning agents when deciding whether to explore new actions or exploit known actions to maximize rewards.
What is deep reinforcement learning?,R5,Deep RL allows agents to use deep networks for function approximation in tasks where state or action spaces are too large for traditional Q-tables. It’s used in robotics and game AI.,0.95,Deep reinforcement learning combines reinforcement learning with deep learning techniques to handle high-dimensional input spaces and learn complex policies directly from raw sensory input.
What is a Markov decision process (MDP)?,R5,"In an MDP, the agent interacts with an environment over time. At each step, it receives a reward and moves to a new state based on a probability distribution.",0.9,A Markov decision process (MDP) is a mathematical framework used to model sequential decision-making problems in which an agent interacts with an environment.
What is cross-entropy loss?,R5,Cross-entropy loss increases as the predicted probability for the true class decreases. It penalizes confident but incorrect predictions more than less confident ones.,0.95,"Cross-entropy loss, also known as log loss, is a loss function used in classification tasks to quantify the difference between the predicted probability distribution and the true probability distribution of the classes."
What is the softmax function?,R5,Softmax helps in classification by converting logits into normalized probabilities. The value for each class indicates the confidence of the model in that class.,0.95,The softmax function is a mathematical function that converts a vector of arbitrary real values into a vector of probabilities.
What is the difference between a generative model and a discriminative model?,R5,"Generative models try to model how the data is generated, so they can both classify and generate samples. Discriminative models just draw a boundary between classes without modeling the data distribution.",0.95,"A generative model learns the joint probability distribution of the input features and the target labels, while a discriminative model learns the conditional probability distribution of the target labels given the input features."
What is autoencoder?,R5,Autoencoders are neural networks that aim to minimize the difference between input and output data by learning a compact internal structure. They are commonly used in image compression and noise reduction.,0.95,An autoencoder is a type of neural network architecture that is trained to reconstruct its input data at the output layer.
What is the difference between bagging and boosting?,R5,"Bagging builds multiple strong learners in parallel to make stable predictions, while boosting builds weak learners sequentially and focuses more on improving mistakes made by earlier models.",0.95,"Bagging (Bootstrap Aggregating) is an ensemble method that builds multiple models independently and combines their predictions through averaging or voting, while boosting sequentially builds models by giving more weight to misclassified instances."
What is hyperparameter tuning?,R5,"Tuning hyperparameters is critical because they are not learned during training. The right set can significantly impact model performance, especially in complex models like neural networks.",0.95,Hyperparameter tuning is the process of finding the optimal hyperparameters for a machine learning model to improve its performance.
What is the curse of dimensionality in the context of feature selection?,R5,"In high-dimensional datasets, many features may be irrelevant or noisy, making learning inefficient. The curse of dimensionality highlights the need to select a subset of meaningful features.",0.95,"The curse of dimensionality in feature selection refers to the challenges and limitations posed by high-dimensional data, such as increased computational complexity and difficulty in finding meaningful features."
What is the difference between L1 and L2 regularization?,R5,"The key difference is that L1 can produce sparse models with zero weights for less important features, while L2 distributes weights more evenly across all features.",0.95,"L1 regularization (Lasso) adds a penalty term proportional to the absolute value of the coefficients, encouraging sparsity and feature selection, while L2 regularization (Ridge) adds a penalty term proportional to the square of the coefficients, penalizing large coefficients."
What is the purpose of cross-validation in feature selection?,R5,Cross-validation ensures that chosen features do not lead to overfitting on a single training set. It validates the model's ability to generalize across different samples.,0.95,"Cross-validation in feature selection is used to evaluate the performance of a model with different subsets of features, helping to identify the most informative features and reduce the risk of overfitting."
What is the role of bias in model evaluation?,R5,Bias helps us understand if the model is consistently missing the target in the same direction. It gives insight into whether the model is systematically wrong.,0.9,"Bias in model evaluation refers to systematic errors or inaccuracies in the predictions made by a model, which may arise from simplifying assumptions or limitations in the model architecture."
What is the importance of interpretability in machine learning models?,R5,"Models with high interpretability are easier to audit, more transparent, and can help identify biases or errors in predictions. This is crucial for ethical AI development.",0.95,"Interpretability in machine learning models is important for understanding how predictions are made, gaining insights into the underlying relationships in the data, and building trust with stakeholders."
What is the difference between k-means and hierarchical clustering?,R5,"In hierarchical clustering, you can visualize the result using a dendrogram, showing how data points are merged or split. K-means doesn’t provide this visual structure.",0.95,"K-means clustering partitions the data into k clusters by minimizing the within-cluster sum of squares, while hierarchical clustering builds a tree-like hierarchy of clusters by recursively merging or splitting clusters based on proximity."
What is the Elbow Method used for in k-means clustering?,R5,It provides a visual tool to select the appropriate number of clusters by identifying the point of diminishing returns in terms of explained variance.,0.95,"The Elbow Method is used to determine the optimal number of clusters (k) in k-means clustering by plotting the within-cluster sum of squares against the number of clusters and identifying the ""elbow"" point where the rate of decrease in inertia slows down."
What is the role of regularization in reducing model complexity?,R5,"By limiting how much the model can rely on any single feature, regularization ensures the learned relationships are stable and less likely to be caused by noise.",0.95,"Regularization in machine learning reduces model complexity by adding penalty terms to the loss function, encouraging simpler models with smaller coefficients or fewer features."
What is the difference between gradient descent and stochastic gradient descent?,R5,"Gradient Descent provides stable convergence but can be computationally expensive, while SGD is more efficient and introduces variance that can help escape local minima.",0.95,"Gradient descent updates model parameters using the average gradient of the entire training dataset, while stochastic gradient descent updates parameters using the gradient of a single randomly selected data point or mini-batch."
What is the tradeoff between bias and variance in model selection?,R5,Bias is when the model ignores data patterns and variance is when it learns noise. The tradeoff is balancing these to avoid underfitting and overfitting.,0.9,The bias-variance tradeoff in model selection involves balancing the tradeoff between bias (underfitting) and variance (overfitting) by selecting models with an appropriate level of complexity based on the dataset and task.
What is the role of regularization in reducing overfitting?,R5,The role of regularization is to add constraints to the model parameters so it doesn’t memorize the training data and performs better on new data.,0.9,"Regularization in machine learning reduces overfitting by penalizing overly complex models, encouraging simpler models that generalize better to unseen data."
What are some techniques for handling imbalanced datasets in classification tasks?,R5,"Methods like oversampling the minority class or undersampling the majority class help balance the dataset, and sometimes combining both is effective.",0.9,"Techniques for handling imbalanced datasets include resampling methods such as oversampling the minority class or undersampling the majority class, using cost-sensitive learning algorithms, and generating synthetic samples with techniques like SMOTE."
What is the purpose of A/B testing in data science?,R5,It allows data scientists to make data-driven decisions by measuring the impact of changes in controlled experiments.,0.95,"A/B testing is used to compare two or more versions of a product or intervention to determine which one performs better based on predefined metrics, such as conversion rate or user engagement."
What is the difference between correlation and causation?,R5,"Correlation shows an association between variables without proving cause, but causation shows that one variable directly affects the other.",0.9,"Correlation measures the strength and direction of the linear relationship between two variables, while causation indicates that changes in one variable directly cause changes in another variable."
What is the role of feature scaling in machine learning?,R5,"Feature scaling improves model performance, especially for algorithms like gradient descent, K-nearest neighbors, and SVMs that rely on distance measurements.",0.9,"Feature scaling in machine learning ensures that all features have the same scale or range, preventing certain features from dominating others during model training. Common scaling techniques include min-max scaling and standardization."
What is the difference between classification and clustering?,R5,"In classification, the output is discrete labels, while in clustering, the output is continuous values.",0.3,"Classification is a supervised learning task where the goal is to predict categorical labels or classes, while clustering is an unsupervised learning task where the goal is to group similar data points together based on their features."
What is the purpose of Principal Component Analysis (PCA) in dimensionality reduction?,R5,PCA sorts features by importance and deletes the least important ones.,0.25,"Principal Component Analysis (PCA) is used to reduce the dimensionality of a dataset by transforming the original features into a new set of orthogonal principal components, which capture the maximum variance in the data."
What is the role of regularization in neural networks?,R5,The role of regularization is to improve the neural network’s ability to generalize by preventing it from fitting noise in the training set.,0.95,"Regularization in neural networks helps prevent overfitting by adding penalty terms to the loss function, encouraging simpler models with smaller weights or fewer connections."
What is dropout regularization in neural networks?,R5,The technique involves randomly dropping connections between neurons so the model doesn’t memorize the training data.,0.85,"Dropout regularization is a technique used in neural networks to reduce overfitting by randomly dropping out a fraction of neurons during training, forcing the network to learn more robust representations."
What is the difference between a generative adversarial network (GAN) and a variational autoencoder (VAE)?,R5,"VAEs generate new data by sampling from a learned latent space, while GANs train two networks to compete, improving the generator's output quality.",0.9,"Generative adversarial networks (GANs) consist of two neural networks (generator and discriminator) that compete with each other to generate realistic data samples, while variational autoencoders (VAEs) learn a probabilistic model of the data's latent space and generate new samples by sampling from this distribution."
What is the difference between batch gradient descent and mini-batch gradient descent?,R5,Mini-batch gradient descent balances the speed of stochastic gradient descent and the accuracy of batch gradient descent by updating weights with small batches.,0.95,"Batch gradient descent computes the gradient of the cost function using the entire training dataset, while mini-batch gradient descent computes the gradient using a randomly selected subset (mini-batch) of the training data."
What is the role of activation functions in neural networks?,R5,Activation functions convert the weighted sum of inputs into an output signal that determines the neuron’s firing.,0.9,"Activation functions introduce non-linearity into neural networks, allowing them to learn complex mappings between inputs and outputs. Common activation functions include sigmoid, tanh, ReLU, and softmax."
What is the difference between generative and discriminative models in machine learning?,R5,"Generative models can create new data points similar to the training data, but discriminative models focus on separating classes accurately.",0.9,"Generative models learn the joint probability distribution of the input features and the target labels, while discriminative models learn the conditional probability distribution of the target labels given the input features."
What is the purpose of dropout regularization in neural networks?,R5,The purpose of dropout is to randomly deactivate neurons so the network does not memorize the training data and generalizes better to new data.,0.95,"Dropout regularization is a technique used in neural networks to prevent overfitting by randomly dropping out a fraction of neurons during training, forcing the network to learn more robust representations."
What is the role of LSTMs in sequence modeling tasks?,R5,Their role is to convert sequence data into fixed-size vectors for use in traditional machine learning models.,0.4,"Long Short-Term Memory (LSTM) networks are a type of recurrent neural network (RNN) designed to capture long-range dependencies and temporal dynamics in sequential data, making them well-suited for tasks such as language modeling and time series prediction."
What is the difference between precision and recall in classification tasks?,R5,"Precision is about avoiding false positives, and recall is about avoiding false negatives.",0.9,"Precision measures the proportion of true positive predictions among all positive predictions made by a classifier, while recall measures the proportion of true positive predictions among all actual positive instances in the data."
"What is the F1 score, and why is it useful in model evaluation?",R5,"It is useful because it considers both false positives and false negatives, helping to assess models where one class is more important.",0.9,"The F1 score is the harmonic mean of precision and recall and provides a single metric to evaluate the performance of a classifier, balancing both precision and recall. It is particularly useful for imbalanced datasets where one class is much more prevalent than the others."
What is the purpose of word embedding in natural language processing (NLP)?,R5,"The primary purpose of word embeddings in NLP is to create numerical representations of words that capture their meaning and context. By mapping words to vectors in a continuous space, they enable algorithms to understand semantic similarities and relationships between words. This is crucial for tasks where understanding the nuances of language is important, such as natural language understanding, text generation, and chatbots, as it provides a rich, meaningful input for machine learning models.",0.95,"Word embedding is a technique used to represent words as dense vectors in a continuous vector space, capturing semantic relationships between words and enabling neural networks to process textual data more effectively."
What is the difference between word2vec and GloVe in word embedding?,R5,"The main difference is that GloVe builds a word co-occurrence matrix and then factorizes it, while Word2Vec optimizes embeddings during training by using context windows.",0.95,"Word2Vec is a model that learns word embeddings by predicting the surrounding words in a text corpus, while GloVe (Global Vectors for Word Representation) learns word embeddings by factorizing the co-occurrence matrix of words in the corpus."
What is the purpose of sentiment analysis in natural language processing (NLP)?,R5,"It is used to classify text data based on the emotions or opinions expressed, which is useful in customer service, politics, and marketing.",0.9,"Sentiment analysis is a natural language processing task that involves determining the sentiment or opinion expressed in a piece of text, such as positive, negative, or neutral. It is commonly used to analyze customer reviews, social media posts, and survey responses."
What is the difference between shallow learning and deep learning?,R5,Deep learning needs more data and computational power than shallow learning because it has more layers and parameters.,0.9,"Shallow learning refers to traditional machine learning algorithms with a small number of layers or features, while deep learning involves neural networks with multiple layers that can learn hierarchical representations of the data."
What is the role of dropout regularization in neural networks?,R5,"Dropout randomly turns off neurons during training, which helps the network generalize better on new data.",0.9,"Dropout regularization is a technique used in neural networks to prevent overfitting by randomly dropping out a fraction of neurons during training, forcing the network to learn more robust representations."
What is the difference between precision and accuracy in model evaluation?,R5,"Accuracy is about overall correctness, but precision is about the correctness of positive predictions specifically.",0.95,"Precision measures the proportion of true positive predictions among all positive predictions made by a classifier, while accuracy measures the proportion of correct predictions (both true positives and true negatives) among all predictions made by the classifier."
What is the difference between k-fold cross-validation and leave-one-out cross-validation?,R5,"K-fold is better because it avoids overfitting, while leave-one-out always overfits the data.",0.4,"K-fold cross-validation divides the data into k subsets and iteratively trains the model k times, each time using a different subset as the validation set, while leave-one-out cross-validation trains the model k times, each time using all but one data point as the training set and the remaining point as the validation set."
What is the Naive Bayes classifier and how does it work?,R5,"It’s called 'naive' because it assumes that all features contribute independently to the outcome, which simplifies calculations.",0.95,"The Naive Bayes classifier is a probabilistic model based on Bayes' theorem with the ""naive"" assumption of feature independence. It calculates the probability of a class given the input features using the product of conditional probabilities."
What is the difference between correlation and covariance?,R5,"Covariance can take any value, while correlation is scaled, making it more useful for comparing relationships between different datasets.",0.9,"Covariance measures the degree to which two variables change together, while correlation measures both the strength and direction of the linear relationship between two variables."
What is the purpose of a confusion matrix in classification tasks?,R5,"A confusion matrix gives a detailed view of how the model is classifying each class, not just overall accuracy.",0.95,"A confusion matrix is a table that visualizes the performance of a classification model by comparing predicted class labels with true class labels. It shows the number of true positives, true negatives, false positives, and false negatives."
What is the difference between stratified sampling and random sampling?,R5,"Stratified sampling is useful when the population has clear subgroups, whereas random sampling ignores any group structure.",0.9,"Stratified sampling divides the population into homogeneous subgroups (strata) and samples from each stratum proportionally, while random sampling selects individuals randomly from the entire population without regard to strata."
"What is the difference between univariate, bivariate, and multivariate analysis?",R5,"Univariate deals with single variable stats like mean and median, bivariate looks at correlations, and multivariate uses methods like regression or PCA.",0.95,"Univariate analysis examines the distribution and summary statistics of a single variable, bivariate analysis explores the relationship between two variables, and multivariate analysis involves analyzing multiple variables simultaneously."
What is the purpose of data preprocessing in machine learning?,R5,"Data preprocessing helps remove noise and inconsistencies from the dataset, which improves the quality of the input data.",0.95,"Data preprocessing involves cleaning, transforming, and preparing raw data for machine learning algorithms. It includes tasks such as handling missing values, encoding categorical variables, and scaling features."
What is the bias of a statistical estimator?,R5,"The bias of an estimator shows how far off it is from the true value it’s trying to estimate, on average.",0.9,The bias of a statistical estimator measures the difference between the expected value of the estimator and the true value of the parameter being estimated. A biased estimator systematically overestimates or underestimates the true parameter value.
What is the variance of a statistical estimator?,R5,The variance of an estimator tells us how stable or consistent it is across repeated sampling.,0.95,"The variance of a statistical estimator measures the spread or variability of the estimator's values around its expected value. A high variance indicates that the estimator's values are scattered widely, while a low variance indicates that the values are clustered closely together."
What is the difference between parametric and non-parametric statistical tests?,R5,"Parametric tests rely on parameters like mean and standard deviation, but non-parametric tests work on medians and ranks.",0.9,"Parametric statistical tests assume that the data follows a specific probability distribution (e.g., normal distribution), while non-parametric tests make no such assumptions and are based on ranks or frequencies."
What is the purpose of feature scaling in machine learning?,R5,Feature scaling prevents some features from dominating others due to differences in scale or units.,0.9,"Feature scaling ensures that all features have the same scale or range, preventing certain features from dominating others during model training. Common scaling techniques include min-max scaling and standardization."
What is the difference between correlation and causation in statistics?,R5,"Causation implies a cause-effect relationship, but correlation could be due to coincidence or a third variable.",0.95,"Correlation measures the strength and direction of the linear relationship between two variables, while causation indicates that changes in one variable directly cause changes in another variable."
What is the Central Limit Theorem and why is it important?,R5,"It's important because it justifies why many statistical methods work, especially those relying on the assumption of normality.",0.95,"The Central Limit Theorem states that the sampling distribution of the sample mean approaches a normal distribution as the sample size increases, regardless of the shape of the population distribution. It is important because it allows us to make inferences about population parameters based on sample statistics."
What is the purpose of outlier detection in data analysis?,R5,"Outliers can skew statistical measures like mean and standard deviation, so detecting them helps in accurate analysis.",0.9,"Outlier detection involves identifying observations that deviate significantly from the rest of the data. Outliers can distort statistical analyses and machine learning models, so detecting and handling them appropriately is important for accurate results."
What is the difference between supervised and unsupervised outlier detection methods?,R5,"Supervised methods need a training phase with labeled examples of outliers, while unsupervised methods detect deviations based on clustering or distance without prior labels.",0.95,"Supervised outlier detection methods require labeled data with outlier labels, while unsupervised methods detect outliers based on the characteristics of the data itself without requiring labeled examples."
What is the difference between statistical inference and predictive modeling?,R5,"Inference is used to understand how variables relate and whether effects are significant, while predictive modeling is used to optimize accuracy on unseen data.",0.95,"Statistical inference involves drawing conclusions about a population based on a sample of data, while predictive modeling focuses on building models that make predictions about future or unseen data based on past observations."
What is the purpose of regularization in machine learning?,R5,Regularization techniques like L1 and L2 penalize large weights so the model doesn't rely too heavily on any single feature.,0.95,"Regularization is a technique used to prevent overfitting by adding a penalty term to the model's objective function, discouraging overly complex models. Common regularization techniques include L1 regularization (Lasso) and L2 regularization (Ridge)."
What is the difference between batch gradient descent and stochastic gradient descent?,R5,"SGD introduces randomness by using one sample per update, which helps in escaping local minima, unlike batch GD which can get stuck.",0.95,"Batch gradient descent updates model parameters using the average gradient of the entire training dataset, while stochastic gradient descent updates parameters using the gradient of a single randomly selected data point or mini-batch."
What is the difference between statistical modeling and machine learning?,R5,"Machine learning can handle large, complex datasets better, while statistical modeling is better for hypothesis testing and inference.",0.9,"Statistical modeling focuses on understanding the relationships between variables in a probabilistic framework, while machine learning emphasizes building algorithms that can learn patterns and make predictions from data without explicitly programming them."
What is the purpose of feature selection in machine learning?,R5,"It reduces the dimensionality of the data, which helps make the model faster and more interpretable.",0.9,"Feature selection involves selecting the most relevant features from a dataset to improve model performance, reduce overfitting, and enhance interpretability."
What is the difference between regularization and feature selection in machine learning?,R5,"Feature selection is a preprocessing step, whereas regularization is applied as part of the model training process.",0.95,"Regularization penalizes overly complex models by adding a penalty term to the objective function, while feature selection explicitly selects a subset of features to include in the model based on their importance or relevance."
What is the difference between grid search and random search for hyperparameter tuning?,R5,"Grid search guarantees finding the best combination if time allows, while random search may miss it but is more efficient in high-dimensional spaces.",0.95,"Grid search exhaustively searches through a specified grid of hyperparameter values, while random search samples hyperparameter values randomly from predefined distributions."
What is the purpose of cross-validation in model evaluation?,R5,It gives a more reliable estimate of model performance than a single train-test split by using multiple folds.,0.95,Cross-validation is a technique used to assess the performance of a predictive model by splitting the data into multiple subsets. It helps to estimate the model's generalization error and detect overfitting.
What is the difference between precision and recall in classification evaluation?,R5,"High precision means fewer false positives, and high recall means fewer false negatives.",0.95,"Precision measures the proportion of true positive predictions among all positive predictions made by a classifier, while recall measures the proportion of true positive predictions among all actual positive instances in the data."
"What is the F1 score, and why is it useful in imbalanced datasets?",R5,"In imbalanced datasets, accuracy can be misleading, so F1 score gives a better sense of how the model performs on the minority class.",0.95,The F1 score is the harmonic mean of precision and recall and provides a single metric to evaluate the performance of a classifier. It is particularly useful for imbalanced datasets where one class is much more prevalent than the others.
What is the difference between bagging and boosting ensemble methods?,R5,"In bagging, models are trained independently, but in boosting, each model depends on the performance of the previous one.",0.95,"Bagging (Bootstrap Aggregating) builds multiple models independently and combines their predictions through averaging or voting, while boosting sequentially builds models by giving more weight to misclassified instances."
What is the purpose of principal component analysis (PCA) in dimensionality reduction?,R5,PCA helps in visualizing high-dimensional data by projecting it into 2D or 3D while preserving structure.,0.9,"Principal Component Analysis (PCA) is used to reduce the dimensionality of a dataset by transforming the original features into a new set of orthogonal principal components, which capture the maximum variance in the data."
What is the difference between classification and regression in machine learning?,R5,"Classification and regression are both supervised learning methods, but they differ in the type of output they predict.",0.95,"Classification is a supervised learning task where the goal is to predict categorical labels or classes, while regression is a supervised learning task where the goal is to predict continuous numerical values."
What is the purpose of natural language processing (NLP) in data science?,R5,"In data science, NLP is used for tasks like sentiment analysis, topic modeling, and text classification.",0.95,"Natural language processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human languages. It enables computers to understand, interpret, and generate human language data."
What is the difference between bag-of-words and word embeddings in NLP?,R5,"Word embeddings like Word2Vec or GloVe provide better performance in NLP tasks because they understand word similarity, unlike bag-of-words.",0.95,"Bag-of-words represents text as a collection of word counts or frequencies, ignoring word order and grammar, while word embeddings represent words as dense vectors in a continuous vector space, capturing semantic relationships between words."
What is the difference between sentiment analysis and topic modeling in NLP?,R5,"Sentiment analysis helps determine opinions, while topic modeling is more about understanding what the text is actually about.",0.95,"Sentiment analysis determines the sentiment or opinion expressed in a piece of text, such as positive, negative, or neutral, while topic modeling identifies the underlying topics or themes present in a collection of documents."
What is the purpose of word2vec in NLP?,R5,"With Word2Vec, NLP models can understand the meaning of words based on their usage and surrounding words.",0.95,Word2Vec is a model that learns word embeddings by predicting the surrounding words in a text corpus. It captures semantic relationships between words and enables algorithms to process textual data more effectively.
What is the difference between a generative model and a discriminative model in machine learning?,R5,"Naive Bayes is a generative model, and logistic regression is an example of a discriminative model.",0.95,"A generative model learns the joint probability distribution of the input features and the target labels, while a discriminative model learns the conditional probability distribution of the target labels given the input features."
What is the difference between deep learning and traditional machine learning algorithms?,R5,"Deep learning needs large datasets and high computational power, while traditional machine learning works well with small datasets.",0.9,"Deep learning involves neural networks with multiple layers that can learn hierarchical representations of the data, while traditional machine learning algorithms typically use handcrafted features and simpler models."
What is the purpose of transfer learning in deep learning?,R5,"In deep learning, transfer learning helps adapt existing models to new problems, especially when labeled data is limited.",0.95,"Transfer learning is a technique where a model trained on one task is reused as the starting point for a model on a related task. It enables the transfer of knowledge and features learned from one domain to another, reducing the need for large labeled datasets."
What is the difference between convolutional neural networks (CNNs) and recurrent neural networks (RNNs)?,R5,"The main difference is that CNNs process fixed-size inputs all at once, while RNNs process one step at a time and keep track of past inputs.",0.9,"Convolutional neural networks (CNNs) are well-suited for processing grid-like data such as images, while recurrent neural networks (RNNs) are designed to handle sequential data with temporal dependencies such as text or time series."
What is the purpose of batch normalization in neural networks?,R5,"By normalizing the input of each layer, batch normalization helps gradients flow better and improves convergence speed.",0.95,Batch normalization is a technique used in neural networks to improve training stability and speed by normalizing the activations of each layer. It reduces internal covariate shift and accelerates convergence during training.
What is the difference between a feedforward neural network and a recurrent neural network?,R5,"RNNs are designed to handle sequence data like time series or text, whereas feedforward networks are used for simpler, non-sequential tasks.",0.95,"A feedforward neural network consists of multiple layers of neurons connected in a forward direction without cycles, while a recurrent neural network has connections that form directed cycles, allowing it to maintain state or memory over time."
What is the difference between LSTMs and GRUs in recurrent neural networks?,R5,"GRUs often train faster and require fewer parameters than LSTMs, making them efficient for smaller datasets.",0.9,"LSTMs (Long Short-Term Memory) and GRUs (Gated Recurrent Units) are both types of recurrent neural network architectures designed to capture long-range dependencies in sequential data, but they differ in their internal gating mechanisms and computational complexity."
What is the purpose of attention mechanisms in neural networks?,R5,"Attention is especially useful in sequence-to-sequence models, as it lets the decoder refer back to specific parts of the input sequence.",0.95,Attention mechanisms in neural networks enable the model to focus on relevant parts of the input data while ignoring irrelevant information. They have been particularly successful in natural language processing tasks such as machine translation and text summarization.
What is the difference between generative adversarial networks (GANs) and autoencoders in deep learning?,R5,"Autoencoders reduce dimensionality by learning compressed representations, while GANs are designed for data generation tasks.",0.9,"Generative adversarial networks (GANs) consist of two neural networks (generator and discriminator) that compete with each other to generate realistic data samples, while autoencoders consist of an encoder and decoder network that reconstruct input data."
What is the purpose of reinforcement learning in machine learning?,R5,"It is commonly used in scenarios where decisions must be made in sequences, such as robotics or game playing.",0.9,Reinforcement learning is a machine learning paradigm where an agent learns to make decisions by interacting with an environment to maximize cumulative rewards. It is commonly used in settings where actions influence future states and the agent learns through trial and error.
What is the difference between Q-learning and policy gradient methods in reinforcement learning?,R5,"In Q-learning, we try to find the optimal value function, while policy gradients aim to find the optimal policy directly.",0.95,"Q-learning is a model-free reinforcement learning algorithm that learns the optimal action-selection policy by iteratively updating a Q-value function, while policy gradient methods directly parameterize the policy and update the policy parameters based on gradients of expected rewards."
What is the purpose of data preprocessing in exploratory data analysis (EDA)?,R5,"Preprocessing ensures that the data is consistent, accurate, and ready for statistical summaries and visual exploration.",0.9,"Data preprocessing in EDA involves cleaning, transforming, and preparing raw data for analysis. It includes tasks such as handling missing values, removing outliers, and encoding categorical variables."
What are some common techniques for handling missing data in EDA?,R5,Dropping columns with too many missing values is a common strategy during EDA.,0.9,"Common techniques for handling missing data in EDA include imputation (replacing missing values with estimates), deletion (removing rows or columns with missing values), and prediction (using models to estimate missing values)."
What is the difference between univariate and bivariate analysis in EDA?,R5,"Univariate analysis helps summarize a single feature, while bivariate looks at how two features interact or influence each other.",0.9,"Univariate analysis examines the distribution and summary statistics of a single variable, while bivariate analysis explores the relationship between two variables."
What is the purpose of data visualization in EDA?,R5,It allows analysts to explore relationships between variables and detect anomalies that might be missed in raw tables.,0.9,"Data visualization in EDA helps to explore and understand the structure, patterns, and relationships in the data. It provides insights that may not be apparent from raw data or summary statistics alone."
What are some common types of data visualization used in EDA?,R5,"Box plots are used to detect outliers and understand data spread, which is very helpful in EDA.",0.9,"Common types of data visualization used in EDA include histograms, scatter plots, box plots, bar charts, line plots, and heatmaps."
What is the difference between continuous and categorical variables in EDA?,R5,"In EDA, we use histograms for continuous variables and bar charts for categorical ones.",0.9,"Continuous variables are numeric variables with an infinite number of possible values within a range, while categorical variables are qualitative variables with a finite number of distinct categories or levels."
What is the purpose of statistical summaries in EDA?,R5,"Summaries help detect outliers, missing values, and anomalies before further analysis or modeling.",0.9,"Statistical summaries in EDA provide descriptive statistics such as mean, median, mode, variance, and percentiles to summarize the central tendency, dispersion, and shape of the data distribution."
What is the difference between central tendency and dispersion measures in EDA?,R5,"Central tendency gives a single value that represents the data's center, whereas dispersion describes the variability around that center.",0.9,"Central tendency measures (e.g., mean, median, mode) describe the central or typical value of a dataset, while dispersion measures (e.g., variance, standard deviation) quantify the spread or variability of the data around the central value."
What is the purpose of hypothesis testing in EDA?,R5,It provides a formal method to evaluate assumptions and test ideas about data relationships before modeling.,0.9,Hypothesis testing in EDA is used to evaluate whether observed differences or relationships in the data are statistically significant or due to random chance. It involves formulating null and alternative hypotheses and conducting statistical tests to make inferences about population parameters.
What is the difference between parametric and non-parametric tests in hypothesis testing?,R5,"Parametric tests usually have more statistical power if the assumptions are met, whereas non-parametric tests are better when those assumptions are violated.",0.9,"Parametric tests assume that the data follows a specific probability distribution (e.g., normal distribution), while non-parametric tests make no such assumptions and are based on ranks or frequencies."
What is the purpose of correlation analysis in EDA?,R5,"It is useful in detecting multicollinearity, which can affect model performance later.",0.9,Correlation analysis in EDA measures the strength and direction of the linear relationship between two continuous variables. It helps to identify patterns and associations in the data.
What is the difference between Pearson correlation and Spearman correlation?,R5,"Pearson is good for detecting straight-line relationships, while Spearman can detect any increasing or decreasing trend.",0.9,"Pearson correlation measures the linear relationship between two continuous variables, while Spearman correlation measures the monotonic relationship between variables, allowing for non-linear associations."
What is the purpose of regression analysis in EDA?,R5,"It gives insights into how variables are related, such as how changes in one variable affect another.",0.9,Regression analysis in EDA is used to model the relationship between one or more independent variables (predictors) and a dependent variable (outcome). It helps to understand how changes in predictors affect the outcome variable.
What are the assumptions of linear regression in EDA?,R5,Another assumption is that the residuals (errors) are normally distributed.,0.9,"The assumptions of linear regression in EDA include linearity (relationship between variables is linear), independence (residuals are independent), homoscedasticity (residuals have constant variance), and normality (residuals are normally distributed)."
What is the difference between simple linear regression and multiple linear regression?,R5,"With simple regression, there’s only one independent variable influencing the outcome. In multiple regression, many independent variables are used together to improve prediction accuracy.",0.9,"Simple linear regression models the relationship between one independent variable and a dependent variable, while multiple linear regression models the relationship between multiple independent variables and a dependent variable."
What is the purpose of logistic regression in EDA?,R5,"It helps determine the strength and direction of influence that independent variables have on a binary dependent variable, giving a statistical overview before modeling.",0.9,Logistic regression in EDA is used to model the relationship between one or more independent variables (predictors) and a binary outcome variable (0 or 1). It is commonly used for classification tasks.
What are the assumptions of logistic regression in EDA?,R5,"Multicollinearity should be minimal, as high correlation between predictors can distort the coefficient estimates. This is checked during EDA using correlation matrices or VIF scores.",0.95,"The assumptions of logistic regression in EDA include linearity (relationship between predictors and log-odds is linear), independence (observations are independent), and absence of multicollinearity (predictors are not highly correlated)."
What is the purpose of outlier detection in EDA?,R5,It is useful in improving model performance by ensuring that extreme values don’t dominate the learning process or cause overfitting.,0.9,"Outlier detection in EDA involves identifying observations that deviate significantly from the rest of the data. Outliers can distort statistical analyses and machine learning models, so detecting and handling them appropriately is important for accurate results."
What are some common techniques for detecting outliers in EDA?,R5,"Scatter plots are helpful for visualizing outliers in two-dimensional data, especially when trying to spot patterns or clusters that don't fit.",0.9,"Common techniques for detecting outliers in EDA include graphical methods (e.g., scatter plots, box plots), statistical methods (e.g., z-score, modified z-score), and proximity-based methods (e.g., nearest neighbors)."
What is the purpose of box plots in EDA?,R5,They are helpful for quickly spotting if the data is skewed or symmetrical. The position of the median and the length of the whiskers help indicate this.,0.9,"Box plots in EDA provide a visual summary of the distribution, central tendency, and spread of a dataset. They show the median, quartiles, and potential outliers of the data distribution."
What is the purpose of histograms in EDA?,R5,"Histograms are useful for checking the distribution of a single variable. They can highlight issues like outliers, gaps, or data concentration in certain ranges.",0.9,Histograms in EDA display the frequency distribution of a continuous variable by dividing the data into intervals (bins) and plotting the number of observations in each interval. They provide insights into the shape and spread of the data distribution.
What is the purpose of scatter plots in EDA?,R5,"Scatter plots show how two variables are related by plotting them on an x and y axis, helping analysts detect clusters, gaps, and anomalies.",0.9,"Scatter plots in EDA visualize the relationship between two continuous variables by plotting each data point as a point on a graph. They help to identify patterns, trends, and correlations in the data."
What is the purpose of bar charts in EDA?,R5,"Bar charts are helpful to analyze categorical variables. For example, they can show how many entries fall into each class or group.",0.9,Bar charts in EDA represent categorical data by displaying the frequency or proportion of observations in each category as bars of varying heights. They are useful for comparing the distribution of categorical variables.
What is the purpose of line plots in EDA?,R5,Line plots help detect sharp changes or gradual trends in a dataset over time. They're especially good at identifying upward or downward movement.,0.9,Line plots in EDA visualize the trend or pattern of a variable over time or another continuous dimension by connecting data points with lines. They are commonly used for time series analysis and trend identification.
What is the purpose of heatmaps in EDA?,R5,"In EDA, heatmaps are helpful for exploring pairwise relationships in large datasets. They quickly reveal patterns of similarity or contrast.",0.9,Heatmaps in EDA visualize the magnitude of a variable or the strength of a relationship between variables using colors. They are useful for identifying patterns and correlations in large datasets.
What is the purpose of correlation matrices in EDA?,R5,"In EDA, correlation matrices are useful to explore relationships between variables and detect redundant features that may impact model performance.",0.9,Correlation matrices in EDA display the pairwise correlations between variables in a tabular format or heatmap. They help to identify patterns and associations among variables.
What is the difference between correlation and causation in EDA?,R5,"Correlation is used in EDA to identify patterns, but causation needs controlled experiments to be confirmed. Just because two things correlate doesn’t mean one causes the other.",0.9,"Correlation measures the strength and direction of the linear relationship between two variables, while causation indicates that changes in one variable directly cause changes in another variable. Correlation does not imply causation."
What is the purpose of statistical tests in EDA?,R5,"They are used to test hypotheses, such as whether two variables are independent or if the mean of a sample differs significantly from a population mean.",0.95,Statistical tests in EDA are used to assess the significance of observed differences or relationships in the data. They help to make inferences about population parameters and draw conclusions from sample data.
What is the difference between parametric and non-parametric tests in EDA?,R5,"Parametric tests are generally more powerful when assumptions are met, while non-parametric tests are safer choices when dealing with skewed or ordinal data.",0.95,"Parametric tests assume that the data follows a specific probability distribution (e.g., normal distribution), while non-parametric tests make no such assumptions and are based on ranks or frequencies."
What is the purpose of ANOVA in EDA?,R5,"The purpose of ANOVA is to compare group means and see if at least one differs significantly, which can help in feature relevance evaluation.",0.95,Analysis of Variance (ANOVA) in EDA is used to compare the means of two or more groups to determine whether they are significantly different. It is commonly used for comparing means across multiple categories or treatments.
What is the difference between one-way ANOVA and two-way ANOVA in EDA?,R5,"In EDA, one-way ANOVA analyzes the effect of one factor on a continuous outcome, while two-way ANOVA considers two factors simultaneously, which helps identify interactions.",0.9,"One-way ANOVA compares the means of two or more groups based on a single categorical factor, while two-way ANOVA considers the effects of two categorical factors (main effects and interaction) on the outcome variable."
What is the purpose of chi-squared test in EDA?,R5,"In EDA, the chi-squared test is useful for feature selection by finding which categorical variables are significantly related to the target variable.",0.9,The chi-squared test in EDA is used to assess the independence between two categorical variables by comparing the observed frequencies to the expected frequencies under the null hypothesis of independence.
What is the purpose of t-tests in EDA?,R5,"In EDA, t-tests help determine if observed differences between groups are due to chance or are statistically significant.",0.9,T-tests in EDA are used to compare the means of two independent samples or the mean of a sample to a known value. They help to determine whether observed differences in means are statistically significant.
What is the difference between independent samples t-test and paired samples t-test in EDA?,R5,"In EDA, paired samples t-test accounts for the dependence between samples, which improves accuracy in repeated-measure designs.",0.9,"Independent samples t-test compares the means of two independent groups, while paired samples t-test compares the means of two related groups (e.g., before and after treatment) based on matched pairs of observations."
What is the purpose of non-parametric tests in EDA?,R5,"They provide ways to test hypotheses without assuming a specific data distribution, which is useful for real-world messy data in EDA.",0.9,"Non-parametric tests in EDA are used when the data does not meet the assumptions of parametric tests (e.g., normal distribution, homogeneity of variance). They provide robust alternatives for analyzing non-normally distributed or skewed data."
What is the difference between Wilcoxon signed-rank test and Mann-Whitney U test in EDA?,R5,Both tests are used to check correlations between variables but in different types of datasets.,0.3,"Wilcoxon signed-rank test is a non-parametric test used to compare the medians of two related groups, while Mann-Whitney U test compares the medians of two independent groups."
What is the purpose of exploratory factor analysis (EFA) in EDA?,R5,"In EDA, EFA reduces many variables into fewer factors, making it easier to interpret the data and prepare it for modeling.",0.95,Exploratory factor analysis (EFA) in EDA is used to identify underlying factors or latent variables that explain the common variance among observed variables. It helps to uncover the underlying structure of a dataset.
What is the difference between EFA and principal component analysis (PCA) in EDA?,R5,"PCA creates new variables (principal components) that are linear combinations of original variables, whereas EFA models latent factors believed to cause the observed correlations.",0.9,"EFA and PCA are both dimensionality reduction techniques used in EDA, but EFA aims to identify underlying factors that explain the common variance among observed variables, while PCA aims to capture the maximum variance in the data with orthogonal principal components."
What is the purpose of cluster analysis in EDA?,R5,"Cluster analysis groups data based on similarities without using predefined labels, making it useful for unsupervised learning.",0.9,Cluster analysis in EDA is used to identify groups or clusters of similar observations based on their characteristics or features. It helps to uncover natural groupings in the data and identify patterns or relationships.
What are some common algorithms used for cluster analysis in EDA?,R5,Algorithms like PCA and t-SNE are commonly used for clustering in EDA.,0.2,"Common algorithms used for cluster analysis in EDA include K-means clustering, hierarchical clustering, DBSCAN (Density-Based Spatial Clustering of Applications with Noise), and Gaussian mixture models."
What is the difference between K-means clustering and hierarchical clustering in EDA?,R5,"In EDA, hierarchical clustering can show nested groupings which help understand data structure better than K-means.",0.9,"K-means clustering partitions the data into K clusters by minimizing the within-cluster sum of squares, while hierarchical clustering builds a tree-like hierarchy of clusters by recursively merging or splitting clusters based on proximity."
What is the purpose of dendrogram in hierarchical clustering in EDA?,R5,They help identify outliers by showing which points join clusters last or at a high distance.,0.8,A dendrogram in hierarchical clustering visualizes the hierarchical relationships between clusters by displaying the merging order and distance between clusters. It helps to identify the optimal number of clusters and interpret the clustering results.
What is the purpose of silhouette analysis in cluster validation in EDA?,R5,Silhouette analysis is mainly used to detect outliers by identifying points with low silhouette scores.,0.6,"Silhouette analysis in cluster validation assesses the quality of clustering results by calculating silhouette coefficients for each data point, which measure the cohesion within clusters and separation between clusters. Higher silhouette coefficients indicate better-defined clusters."
What is the purpose of dimensionality reduction techniques in EDA?,R5,"They simplify complex datasets by projecting data into lower-dimensional spaces, which helps reveal hidden patterns and relationships.",0.9,Dimensionality reduction techniques in EDA are used to reduce the number of features or variables in a dataset while preserving the most important information. They help to overcome the curse of dimensionality and improve computational efficiency.
What are some common dimensionality reduction techniques used in EDA?,R5,Feature scaling and normalization are dimensionality reduction techniques often used in EDA.,0.1,"Common dimensionality reduction techniques used in EDA include principal component analysis (PCA), factor analysis, t-distributed stochastic neighbor embedding (t-SNE), and linear discriminant analysis (LDA)."
What is the purpose of PCA in EDA?,R5,It helps identify underlying patterns in the data and can reveal hidden structures during exploratory data analysis.,0.95,Principal Component Analysis (PCA) in EDA is used to reduce the dimensionality of a dataset by transforming the original features into a new set of orthogonal principal components. It helps to identify the underlying structure and patterns in high-dimensional data.
What is the difference between PCA and factor analysis in EDA?,R5,"PCA creates components that are linear combinations of variables, while factor analysis estimates underlying factors that cause observed correlations.",0.95,"PCA and factor analysis are both dimensionality reduction techniques used in EDA, but PCA aims to capture the maximum variance in the data with orthogonal principal components, while factor analysis aims to identify underlying factors that explain the common variance among observed variables."
What is the purpose of t-SNE in EDA?,R5,t-SNE is a clustering algorithm that assigns cluster labels to data points.,0.2,t-Distributed Stochastic Neighbor Embedding (t-SNE) in EDA is a technique used for visualizing high-dimensional data in a lower-dimensional space while preserving the local structure and relationships between data points. It is commonly used for visualizing clusters and patterns in complex datasets.
What is the difference between linear discriminant analysis (LDA) and PCA in EDA?,R5,"Both LDA and PCA reduce dimensions, but LDA uses class information to improve class separability.",0.9,"Linear Discriminant Analysis (LDA) and PCA are both dimensionality reduction techniques used in EDA, but LDA aims to maximize the separation between classes or groups while reducing dimensionality, whereas PCA focuses on capturing the maximum variance in the data regardless of class labels."
What is the purpose of feature selection in EDA?,R5,"By selecting relevant features, feature selection makes models simpler and faster to train and interpret.",0.95,"Feature selection in EDA involves selecting the most relevant features from a dataset to improve model performance, reduce overfitting, and enhance interpretability. It helps to identify the subset of features that contribute the most to predicting the target variable."
What are some common feature selection techniques used in EDA?,R5,Chi-square tests are used to select features for continuous variables.,0.2,"Common feature selection techniques used in EDA include filter methods (e.g., correlation analysis, information gain), wrapper methods (e.g., recursive feature elimination, forward/backward selection), and embedded methods (e.g., LASSO, decision trees)."
What is the purpose of ensemble methods in EDA?,R5,They help in identifying important features by combining feature importance scores from different models.,0.8,"Ensemble methods in EDA combine multiple models or algorithms to improve predictive performance, robustness, and generalization. They help to reduce overfitting and capture diverse patterns in the data."
What are some common ensemble methods used in EDA?,R5,Stacking combines predictions from several models using a meta-model to improve overall performance.,0.95,"Common ensemble methods used in EDA include bagging (Bootstrap Aggregating), boosting (e.g., AdaBoost, Gradient Boosting), and stacking (meta-learning). They combine predictions from multiple base models to make more accurate and reliable predictions."
What is the purpose of model evaluation in EDA?,R5,It helps choose the best model among different algorithms by comparing their performance on validation data.,0.9,Model evaluation in EDA involves assessing the performance of predictive models using various metrics and techniques. It helps to determine the effectiveness of the models in capturing patterns and making accurate predictions on unseen data.
What are some common metrics used for model evaluation in EDA?,R5,Cross-validation score is a metric that tells how well the model works on unseen data.,0.85,"Common metrics used for model evaluation in EDA include accuracy, precision, recall, F1 score, ROC-AUC score, mean squared error (MSE), and R-squared (R2) score. They provide insights into different aspects of model performance."
What is the difference between cross-validation and holdout validation in model evaluation in EDA?,R5,"Cross-validation reduces variance in performance evaluation, while holdout validation may depend heavily on how the data is split.",0.9,"Cross-validation in model evaluation splits the data into multiple subsets (folds) for training and testing, while holdout validation splits the data into two disjoint sets (training and testing). Cross-validation provides more reliable estimates of model performance but is computationally more expensive."
What is the purpose of hyperparameter tuning in model evaluation in EDA?,R5,Grid search and random search are common methods used for tuning hyperparameters.,0.9,Hyperparameter tuning in model evaluation involves optimizing the hyperparameters of machine learning algorithms to improve model performance and generalization. It helps to find the best combination of hyperparameters for a given dataset and problem.
What are some common techniques for hyperparameter tuning in EDA?,R5,Cross-validation is commonly used with tuning techniques to evaluate performance of each hyperparameter setting.,0.9,"Common techniques for hyperparameter tuning in EDA include grid search, random search, Bayesian optimization, and genetic algorithms. They systematically search the hyperparameter space to find the optimal values for model performance."
What is the purpose of model interpretation in EDA?,R5,The purpose is to increase model complexity by finding new hidden variables that can be used for retraining.,0.2,"Model interpretation in EDA involves understanding and explaining how predictive models make decisions and predictions. It helps to gain insights into the underlying patterns, relationships, and features that contribute to the model's predictions."
What are some common techniques for model interpretation in EDA?,R5,Correlation heatmaps are a model interpretation method that tells us how well the model is predicting unseen data.,0.2,"Common techniques for model interpretation in EDA include feature importance analysis (e.g., permutation importance, SHAP values), partial dependence plots, individual conditional expectation (ICE) plots, and model-agnostic interpretation methods (e.g., LIME, SHAP)."
What is the purpose of model deployment in EDA?,R5,"While EDA focuses on exploring and visualizing data, deployment happens after model training to make predictions on real-world data.",0.95,Model deployment in EDA involves deploying predictive models into production environments to make real-time predictions on new data. It allows organizations to leverage the insights and predictions generated from data analysis for decision-making and business applications.
What are some common techniques for model deployment in EDA?,R5,Docker and Kubernetes are often used in EDA to manage deployment and scale EDA-based models for production use.,0.95,"Common techniques for model deployment in EDA include deploying models as web services or APIs, embedding models into applications or dashboards, and integrating models with existing business processes or workflows. They enable seamless integration of predictive models into operational systems and workflows."
What are the common types of regularization used in machine learning?,R5,"L1 regularization increases the complexity of a model, while L2 makes it simpler by removing entire features from the model.",0.2,"Common types of regularization used in machine learning include L1 regularization (Lasso), L2 regularization (Ridge), and elastic net regularization (combination of L1 and L2 penalties)."
What is the bias-variance trade-off in machine learning?,R5,The bias-variance trade-off involves choosing simpler models to reduce bias and avoid underfitting.,0.3,"The bias-variance trade-off refers to the balance between bias (error due to overly simplistic assumptions) and variance (error due to sensitivity to fluctuations in the training data) in machine learning models. Reducing bias typically increases variance, and vice versa."
What are the common types of cross-validation techniques used in machine learning?,R5,"Leave-one-out cross-validation is where each data point acts as a test set once, which is very accurate but computationally expensive.",0.95,"Common types of cross-validation techniques used in machine learning include k-fold cross-validation, stratified k-fold cross-validation, leave-one-out cross-validation, and nested cross-validation."
What is the purpose of hyperparameter tuning in machine learning?,R5,"With hyperparameter tuning, we try different configurations to reduce overfitting and underfitting.",0.9,Hyperparameter tuning in machine learning involves optimizing the hyperparameters of models to improve their performance and generalization. It helps to find the best combination of hyperparameters for a given dataset and problem.
What are some common techniques for hyperparameter tuning in machine learning?,R5,Bayesian Optimization is another method that models the performance of hyperparameter values to choose better ones over time.,0.95,"Common techniques for hyperparameter tuning in machine learning include grid search, random search, Bayesian optimization, genetic algorithms, and gradient-based optimization methods. They systematically search the hyperparameter space to find optimal values."
What is the difference between supervised and unsupervised learning in machine learning?,R5,"Unsupervised learning includes clustering and dimensionality reduction, while supervised learning includes classification and regression tasks.",0.95,"Supervised learning involves learning from labeled data with input-output pairs, where the model predicts the output variable based on input features. Unsupervised learning involves learning from unlabeled data to discover hidden patterns or structures in the data."
What are some common dimensionality reduction techniques used in machine learning?,R5,"Autoencoders, which are neural networks, can also be used to compress data and reduce dimensions in a non-linear way.",0.95,"Common dimensionality reduction techniques used in machine learning include principal component analysis (PCA), linear discriminant analysis (LDA), t-distributed stochastic neighbor embedding (t-SNE), and autoencoders."
What is the purpose of model interpretation in machine learning?,R5,It allows data scientists to identify important features and how they influence predictions.,0.9,"Model interpretation in machine learning involves understanding and explaining how predictive models make decisions and predictions. It helps to gain insights into the underlying patterns, relationships, and features that contribute to the model's predictions."
What are some common techniques for model interpretation in machine learning?,R5,LIME works by approximating a complex model locally with a simpler interpretable model to understand predictions.,0.95,"Common techniques for model interpretation in machine learning include feature importance analysis (e.g., permutation importance, SHAP values), partial dependence plots, individual conditional expectation (ICE) plots, and model-agnostic interpretation methods (e.g., LIME, SHAP)."
What is the purpose of model deployment in machine learning?,R5,"By deploying a model, businesses can use AI predictions in apps, websites, or tools to support decision-making.",0.9,Model deployment in machine learning involves deploying predictive models into production environments to make real-time predictions on new data. It allows organizations to leverage the insights and predictions generated from data analysis for decision-making and business applications.
What are some common techniques for model deployment in machine learning?,R5,"Cloud-based platforms like AWS SageMaker, Google AI Platform, and Azure ML offer scalable deployment services.",0.95,"Common techniques for model deployment in machine learning include deploying models as web services or APIs, embedding models into applications or dashboards, and integrating models with existing business processes or workflows. They enable seamless integration of predictive models into operational systems and workflows."
What is the purpose of transfer learning in machine learning?,R5,"By using a pretrained model, transfer learning can improve accuracy and reduce the need for large amounts of training data.",0.95,"Transfer learning in machine learning is a technique where a model trained on one task is reused as the starting point for a model on a related task. It enables the transfer of knowledge and features learned from one domain to another, reducing the need for large labeled datasets."
What are some common applications of transfer learning in machine learning?,R5,Medical imaging uses transfer learning to identify tumors or anomalies using models pre-trained on large image datasets.,0.95,"Common applications of transfer learning in machine learning include image classification, natural language processing (NLP), and speech recognition. Pre-trained models such as BERT, ResNet, and GPT are often used as starting points for transfer learning tasks."
What is the purpose of feature engineering in machine learning?,R5,"Good feature engineering can simplify complex data, making it easier for models to learn effectively.",0.9,"Feature engineering involves selecting, transforming, or creating new features from the raw data to improve the performance of machine learning models by making them more expressive, informative, and suitable for the task at hand."
What are some common techniques for feature engineering?,R5,One-hot encoding for categorical variables and standardization for numerical variables are very common preprocessing steps.,0.95,"Common techniques include one-hot encoding for categorical variables, scaling numerical features, creating interaction terms, binning or discretizing continuous variables, and extracting features from text or images using techniques like TF-IDF or CNNs."
What is the difference between batch processing and real-time processing?,R5,"Real-time processing provides immediate insights, which is important for time-sensitive applications, unlike batch processing which has delayed output.",0.95,"Batch processing involves processing data in large, discrete batches or groups, typically on a scheduled or periodic basis, while real-time processing involves handling data as soon as it is generated or received, often requiring low-latency and immediate responses."
What is transfer learning in NLP?,R5,It enables developers to save time and resources by not having to train models from scratch for every new task.,0.95,"Transfer learning in NLP involves leveraging pre-trained language models, such as BERT or GPT, trained on large corpora of text data, and fine-tuning them on specific tasks or domains with smaller datasets to achieve better performance and faster convergence."
What is named entity recognition (NER)?,R5,"It’s an important part of information extraction where systems label parts of text with tags such as PERSON, LOCATION, and ORGANIZATION.",0.95,"Named entity recognition is a subtask of information extraction that involves identifying and classifying named entities (e.g., persons, organizations, locations) mentioned in text documents, which is useful for tasks such as entity linking and relation extraction."
What are some challenges in working with unstructured data?,R5,"Since unstructured data is not organized in tables, traditional tools like SQL cannot handle it directly, making analysis harder.",0.95,"Challenges include extracting meaningful information from unstructured text, handling noisy or inconsistent data, dealing with large volumes of data, and ensuring privacy and security when working with sensitive information."
What is collaborative filtering in recommendation systems?,R5,"There are two types: user-based and item-based collaborative filtering, both relying on user-item interactions.",0.95,"Collaborative filtering is a technique used in recommendation systems to generate personalized recommendations by analyzing similarities and patterns in users' behavior or preferences, often based on user-item interactions or user-user similarity."
What are some evaluation metrics used in recommendation systems?,R5,Accuracy is the only metric used in recommendation systems to evaluate how correct the recommendations are.,0.3,"Evaluation metrics include precision, recall, F1 score, Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and ranking-based metrics like Mean Average Precision (MAP) and Normalized Discounted Cumulative Gain (NDCG)."
What is the Markov Decision Process (MDP)?,R5,"MDPs assume the Markov property, meaning the future state depends only on the current state and action, not on the sequence of events that preceded it.",0.95,"The Markov Decision Process is a mathematical framework used to model sequential decision-making problems, where an agent takes actions in an environment to maximize cumulative rewards, while satisfying the Markov property (future states depend only on the current state and action)."
What is policy gradient in reinforcement learning?,R5,Policy gradient focuses on optimizing the agent’s behavior by directly improving the probability of taking good actions.,0.95,"Policy gradient methods are a class of reinforcement learning algorithms that directly optimize the policy (the agent's strategy or behavior) by estimating gradients of expected rewards with respect to the policy parameters, typically using techniques like stochastic gradient ascent."
What is data augmentation in deep learning?,R5,"This technique is especially useful in image classification tasks, where datasets are often limited.",0.95,"Data augmentation is a technique used to artificially increase the size and diversity of training datasets by applying transformations such as rotation, translation, scaling, cropping, or flipping to input data, which can improve model generalization and robustness."
What is sequence-to-sequence learning?,R5,"This approach allows models to handle inputs and outputs of variable lengths, unlike standard classification models.",0.95,"Sequence-to-sequence learning is a type of model architecture used for tasks involving input and output sequences of variable lengths, such as machine translation, summarization, and speech recognition, typically implemented using recurrent neural networks (RNNs) or transformers."
What is attention mechanism in deep learning?,R5,"By learning which parts of the input are important, attention improves the performance of models like Transformers.",0.95,"Attention mechanism is a mechanism used in neural networks to selectively focus on relevant parts of the input data while processing sequences, enabling the model to learn to weigh different input elements dynamically and attend to the most informative parts."
What are some common optimization algorithms used in deep learning?,R5,"Stochastic Gradient Descent (SGD) updates weights using individual samples, making it suitable for large datasets.",0.95,"Common optimization algorithms include stochastic gradient descent (SGD), Adam, RMSprop, and Adagrad, each with different update rules and learning rate adaptation strategies to optimize the model parameters efficiently."
What is semi-supervised learning?,R5,It combines elements of supervised and unsupervised learning to take advantage of both labeled and unlabeled data.,0.95,"Semi-supervised learning is a machine learning paradigm that combines labeled and unlabeled data to improve model performance, often by leveraging the inherent structure or relationships in the data to semi-supervised learning algorithms."
What is self-supervised learning?,R5,It bridges supervised and unsupervised learning by using unlabeled data to create surrogate labels for training.,0.95,"Self-supervised learning is a form of unsupervised learning where the model learns to predict or generate certain properties or features of the input data itself, without requiring external labels or annotations, often used as a pretraining step for downstream tasks."
What is the Bellman equation in reinforcement learning?,R5,It recursively defines the value of a state as the reward plus the best expected value of the next state.,0.95,The Bellman equation is a fundamental equation in dynamic programming and reinforcement learning that expresses the value of a state or state-action pair in terms of the expected immediate reward and the value of the next state or next state-action pair.
What is the difference between on-policy and off-policy learning in reinforcement learning?,R5,"Q-learning is an example of off-policy learning, while SARSA is on-policy because it updates using actions the agent actually took.",0.95,"On-policy learning involves learning the value or policy while following the current policy, while off-policy learning involves learning the value or policy while following a different behavior policy, often leading to more efficient exploration and better sample efficiency."
What are the main components of the data science process?,R5,"It involves gathering data, preparing it, analyzing it for patterns, building predictive models, and sharing insights.",1.0,"The main components include data collection, data cleaning and preprocessing, exploratory data analysis, modeling, evaluation, and deployment."
What is overfitting in machine learning?,R5,"When a model is too complex, it fits the training data perfectly but fails to predict new samples correctly.",1.0,Overfitting occurs when a model learns the details and noise in the training data to the extent that it negatively impacts its performance on unseen data.
Explain the curse of dimensionality.,R5,"It’s a problem where the feature space grows exponentially with the number of features, causing overfitting and poor generalization.",1.0,"The curse of dimensionality refers to the increased difficulty of analyzing and processing data as the number of features or dimensions increases, leading to sparsity and computational challenges."
What is the purpose of cross-validation?,R5,Cross-validation improves model generalization by using multiple train-test splits and averaging results.,1.0,"Cross-validation is used to assess the generalization performance of a model by partitioning the data into multiple subsets, training the model on some subsets, and evaluating it on the remaining subsets."
How does regularization prevent overfitting in neural networks?,R5,Regularization techniques like L1 and L2 shrink weights to avoid fitting noise in the training data.,1.0,"Regularization techniques such as L1 and L2 regularization add penalty terms to the neural network's loss function, which discourages overly complex weight configurations and helps prevent overfitting."
What is the purpose of activation functions in neural networks?,R5,They help the network by normalizing the input features before training.,0.3,"Activation functions introduce nonlinearity to neural networks, allowing them to learn complex patterns and relationships in the data by transforming the input signal from each neuron into an output signal."
What is the purpose of dimensionality reduction techniques?,R5,They help avoid overfitting by eliminating irrelevant or redundant features.,1.0,"Dimensionality reduction techniques are used to reduce the number of features or dimensions in a dataset while preserving its important information, which can help improve the performance and efficiency of machine learning algorithms."
Explain the difference between PCA and t-SNE.,R5,"PCA preserves global variance, whereas t-SNE emphasizes local neighborhoods for visualization.",1.0,"PCA (Principal Component Analysis) is a linear dimensionality reduction technique that seeks to maximize variance, while t-SNE (t-Distributed Stochastic Neighbor Embedding) is a nonlinear technique that focuses on preserving local relationships between data points."
What is K-means clustering?,R5,"K-means clustering requires the number of clusters, K, to be specified before running the algorithm.",1.0,"K-means clustering is an unsupervised learning algorithm used to partition a dataset into K clusters based on similarities in the data points' features, with the goal of minimizing the within-cluster sum of squares."
What is the elbow method used for in K-means clustering?,R5,"The method looks for a point on the plot where adding more clusters doesn’t significantly reduce the error, indicating the best K.",1.0,"The elbow method is used to determine the optimal number of clusters (K) in K-means clustering by plotting the within-cluster sum of squares against the number of clusters and selecting the point where the rate of decrease sharply changes (the ""elbow"" point)."
What is outlier detection?,R5,It is a technique to increase the number of data points by adding rare samples.,0.2,"Outlier detection is the process of identifying data points or observations that deviate significantly from the rest of the dataset, which may indicate errors, anomalies, or interesting patterns."
What are the advantages of deep learning over traditional machine learning algorithms?,R5,"Deep learning models scale well with large datasets and computing power, allowing them to discover complex patterns that traditional models might miss.",1.0,"Deep learning models, particularly neural networks, can automatically learn hierarchical representations of data, handle large and complex datasets, and often achieve state-of-the-art performance in tasks such as image recognition and natural language processing."
What is backpropagation?,R5,Backpropagation updates model parameters by computing how much each neuron contributes to the total error and adjusting weights accordingly.,1.0,Backpropagation is a supervised learning algorithm used to train neural networks by iteratively adjusting the weights of connections between neurons in the network based on the error between predicted and actual outputs.
How do you evaluate the performance of a regression model?,R5,We can use R-squared to see how well the model explains the variance in the target variable. A value close to 1 indicates a good fit.,1.0,"Performance metrics for regression models include Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), R-squared (coefficient of determination), and others."
What is the purpose of the A/B test in data science?,R5,A/B testing is when we randomly pick data points and average their values to find outliers.,0.1,"A/B testing is a statistical hypothesis testing method used to compare two or more versions of a product, webpage, or other elements to determine which one performs better in terms of predefined metrics."
What is Bayesian inference?,R5,"Bayesian inference allows us to update our predictions when new data is observed, making it more flexible than traditional methods.",1.0,"Bayesian inference is a statistical approach that uses Bayes' theorem to update the probability of a hypothesis or belief based on new evidence or observations, incorporating prior knowledge and uncertainty into the analysis."
What is the purpose of hypothesis testing?,R5,Hypothesis testing allows researchers to make data-driven decisions by assessing if the null hypothesis should be rejected or not.,1.0,"Hypothesis testing is used to make inferences or decisions about a population parameter based on sample data, by testing a null hypothesis against an alternative hypothesis using statistical methods."
What is regularization in linear regression?,R5,"It adds a constraint to the model so that it doesn’t rely too heavily on any single variable, which helps improve generalization.",1.0,"Regularization in linear regression involves adding a penalty term to the ordinary least squares (OLS) loss function to prevent overfitting, with common regularization techniques including Ridge regression (L2 regularization) and Lasso regression (L1 regularization)."
What are the assumptions of linear regression?,R5,"Another assumption is that residuals should be normally distributed, especially for hypothesis testing and confidence intervals.",1.0,"The assumptions of linear regression include linearity between the independent and dependent variables, homoscedasticity (constant variance of errors), independence of errors, and normality of error terms."
What is the difference between Type I and Type II errors?,R5,"In hypothesis testing, Type I is the probability of rejecting the null when it's true, and Type II is the probability of accepting the null when it's false.",1.0,"Type I error occurs when a true null hypothesis is rejected (false positive), while Type II error occurs when a false null hypothesis is not rejected (false negative)."
What is the p-value in hypothesis testing?,R5,"If the p-value is less than the significance level (alpha), we reject the null hypothesis in favor of the alternative.",1.0,"The p-value is the probability of observing a test statistic as extreme as, or more extreme than, the one observed, assuming that the null hypothesis is true. It is used to assess the strength of evidence against the null hypothesis."
What is the bias-variance tradeoff?,R5,"A high-bias model makes strong assumptions and can miss relevant trends, while a high-variance model fits training data too closely, hurting generalization.",1.0,"The bias-variance tradeoff refers to the compromise between bias (error due to overly simplistic assumptions) and variance (error due to sensitivity to fluctuations in the training data) in machine learning models, where reducing one typically increases the other."
What is the difference between variance and standard deviation?,R5,"Standard deviation is the square of the variance, and both tell us how far data points are from the median.",0.2,"Variance measures the average squared deviation from the mean of a set of values, while standard deviation measures the average deviation from the mean, providing a measure of the dispersion or spread of the data."
What is the difference between a decision tree and a random forest?,R5,"A decision tree learns from the whole dataset, but random forest builds several trees using random subsets of data and features.",1.0,"A decision tree is a simple, interpretable tree-like structure that recursively splits the data based on features, while a random forest is an ensemble of decision trees that aggregates their predictions to improve performance and reduce overfitting."
What is batch normalization in neural networks?,R5,"It improves performance by keeping the inputs to each layer consistent, even as the weights change during training.",1.0,"Batch normalization is a technique used to normalize the activations of each layer in a neural network by adjusting and scaling them to have zero mean and unit variance, which can accelerate training and improve model performance."
What is the purpose of a confusion matrix in classification?,R5,"A confusion matrix only works with regression models, not classification models.",0.1,"A confusion matrix is a table that summarizes the performance of a classification model by comparing predicted class labels with actual class labels, showing the counts of true positives, true negatives, false positives, and false negatives."
What is the Kullback-Leibler (KL) divergence?,R5,"KL divergence is asymmetric, meaning KL(P‖Q) is not equal to KL(Q‖P).",1.0,"The Kullback-Leibler divergence is a measure of the difference between two probability distributions, used in information theory and statistics to quantify the amount of information lost when one distribution is used to approximate another."
What is the difference between batch normalization and layer normalization in neural networks?,R5,"Layer normalization is applied per sample, and batch normalization is applied per batch of samples.",1.0,"Batch normalization normalizes the activations of each layer across the batch dimension, while layer normalization normalizes the activations across the feature dimension, making it more suitable for recurrent neural networks and other architectures with variable-length sequences."
What is unsupervised learning?,R5,Unsupervised learning tries to understand the structure of the data without using target variables.,1.0,"Unsupervised learning is a type of machine learning where the model is trained on unlabeled data, and it seeks to find patterns or structures in the data without explicit supervision."
What are the main types of unsupervised learning techniques?,R5,"Some common unsupervised techniques include hierarchical clustering, DBSCAN, and t-SNE.",1.0,"The main types include clustering, dimensionality reduction, and association rule learning."
What are the common clustering algorithms?,R5,DBSCAN is a density-based clustering method that groups closely packed points and marks outliers.,1.0,"Common clustering algorithms include K-means clustering, hierarchical clustering, DBSCAN, and Gaussian mixture models (GMM)."
What is hierarchical clustering?,R5,"In hierarchical clustering, similar data points are merged step-by-step to form clusters without pre-specifying the number of clusters.",1.0,Hierarchical clustering is an algorithm that creates a hierarchy of clusters by recursively merging or splitting clusters based on their similarity or dissimilarity.
What is DBSCAN clustering?,R5,"DBSCAN can identify clusters of arbitrary shape, unlike K-Means which assumes spherical clusters.",1.0,"DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a density-based clustering algorithm that groups together data points that are closely packed, while marking outliers as noise."
What are the common dimensionality reduction techniques?,R5,t-SNE is great for visualizing high-dimensional data in 2D or 3D space while preserving local structures.,1.0,"Common techniques include Principal Component Analysis (PCA), t-Distributed Stochastic Neighbor Embedding (t-SNE), and Singular Value Decomposition (SVD)."
What is PCA (Principal Component Analysis)?,R5,Principal Component Analysis helps in visualizing data by projecting it into 2D or 3D space.,1.0,PCA is a linear dimensionality reduction technique that transforms the original features into a lower-dimensional space while preserving the maximum variance in the data.
What is t-SNE (t-Distributed Stochastic Neighbor Embedding)?,R5,t-SNE converts distances between points into probabilities to maintain similarities in lower dimensions.,1.0,"t-SNE is a nonlinear dimensionality reduction technique that focuses on preserving local relationships between data points in a lower-dimensional space, often used for visualizing high-dimensional data."
What is Singular Value Decomposition (SVD)?,R5,The SVD decomposition helps identify the most important features in a dataset by analyzing the singular values.,1.0,"SVD is a matrix factorization technique used to decompose a matrix into the product of three matrices, which can be used for dimensionality reduction, data compression, and matrix approximation."
What is association rule learning?,R5,"The rules are usually in the form of 'if-then' statements, like 'If a person buys bread, they also buy butter'.",1.0,"Association rule learning is a type of unsupervised learning used to discover interesting associations or relationships between variables in large datasets, commonly used in market basket analysis and recommendation systems."
What are the common association rule learning algorithms?,R5,Association rules are usually generated using neural networks and decision trees.,0.2,Common algorithms include Apriori and FP-Growth.
What is the Apriori algorithm?,R5,It's commonly used in market basket analysis to find product combinations that are frequently bought together.,1.0,"The Apriori algorithm is a classic algorithm for mining frequent itemsets and generating association rules, based on the principle of the Apriori property and candidate generation."
What is FP-Growth?,R5,"Compared to Apriori, FP-Growth is more efficient as it avoids repeated scans of the database.",1.0,"FP-Growth (Frequent Pattern Growth) is an efficient algorithm for mining frequent itemsets and generating association rules without candidate generation, using a compact data structure called FP-tree."
What is anomaly detection?,R5,The goal is to flag rare events that differ from the expected behavior in the data.,1.0,"Anomaly detection, also known as outlier detection, is the process of identifying data points or observations that deviate significantly from the rest of the dataset, which may indicate errors, anomalies, or interesting patterns."
What are the common anomaly detection techniques?,R5,Statistical methods like Z-score and IQR are used to find data points that fall outside normal ranges.,1.0,"Common techniques include statistical methods, density-based methods, distance-based methods, and machine learning-based methods such as Isolation Forest and One-Class SVM."
What is Isolation Forest?,R5,Anomalies are isolated faster because they require fewer splits to separate in the tree structure.,1.0,"Isolation Forest is an unsupervised learning algorithm for anomaly detection that isolates anomalies by recursively partitioning the dataset into subsets using random splits, making anomalies easier to isolate than normal data points."
What is One-Class SVM?,R5,This algorithm is useful when there is plenty of normal data but few or no examples of anomalies.,1.0,One-Class Support Vector Machine (SVM) is a machine learning algorithm for anomaly detection that learns a boundary around normal data points in the feature space and identifies anomalies as data points lying outside this boundary.
What is the purpose of density estimation?,R5,"It allows detecting regions with high or low data concentration, useful in anomaly detection.",1.0,"Density estimation is the process of estimating the probability density function of a random variable from a set of data points, often used in clustering, anomaly detection, and generative modeling."
What are the common density estimation techniques?,R5,Kernel Density Estimation smooths data using a kernel function to estimate continuous probability density.,1.0,"Common techniques include histogram-based methods, kernel density estimation (KDE), Gaussian mixture models (GMM), and Parzen window estimation."
What is the Gaussian Mixture Model (GMM)?,R5,Each Gaussian component in the mixture represents a cluster with its own mean and covariance matrix.,1.0,"Gaussian Mixture Model (GMM) is a probabilistic model used for density estimation and clustering, assuming that the data is generated from a mixture of several Gaussian distributions."
What is the Expectation-Maximization (EM) algorithm?,R5,EM can handle incomplete data by estimating missing values during the expectation step.,1.0,"The Expectation-Maximization (EM) algorithm is an iterative optimization algorithm used to estimate the parameters of probabilistic models with latent variables, such as Gaussian mixture models (GMM), by iteratively computing the expected values of the latent variables and maximizing the likelihood function."
What is the difference between generative and discriminative models?,R5,Generative models are always better than discriminative models in terms of accuracy.,0.2,"Generative models learn the joint probability distribution of the input features and the output labels, while discriminative models learn the conditional probability distribution of the output labels given the input features."
What is the purpose of generative modeling?,R5,Generative modeling predicts labels from input features in classification tasks.,0.2,"Generative modeling is used to model the underlying structure of the data and generate new samples from the learned distribution, often used in tasks such as image generation, text generation, and data augmentation."
What is data science?,R6,Data science is a career where people code a lot in Python or R to find hidden patterns in big data. They use libraries like pandas and scikit-learn to analyze datasets and make visualizations. It helps in improving decisions.,0.9,"Data science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from data."
What are the key steps in the data science process?,R6,"The data science process begins with gathering data from different sources. Then the data is cleaned and transformed. After this, data is analyzed using statistics and machine learning techniques. The results are interpreted and shared with stakeholders using dashboards or reports.",0.95,"The key steps typically include problem definition, data collection, data preparation, exploratory data analysis, modeling, evaluation, and deployment."
What is the difference between supervised and unsupervised learning?,R6,"I think supervised learning is when you use machine learning with supervision, like someone checking it. Unsupervised means it runs by itself without any human. Both are ways of using data to solve problems, but supervised is probably safer.",0.3,"Supervised learning involves training a model on labeled data, where the algorithm learns the relationship between input features and target labels. Unsupervised learning deals with unlabeled data and aims to find hidden patterns or structures within the data."
Explain the bias-variance tradeoff.,R6,"The bias-variance tradeoff is like choosing between a model that is too simple and one that is too complex. High bias means poor performance on training data, and high variance means poor performance on test data. You want to avoid both.",0.9,The bias-variance tradeoff is the balance between the error introduced by bias (underfitting) and the error introduced by variance (overfitting) when building a machine learning model.
What is feature engineering?,R6,"In my understanding, feature engineering means choosing which columns to keep and which to drop. It also includes using tools like matplotlib or seaborn to visualize the features, and based on that, decide which features matter most.",0.4,"Feature engineering is the process of selecting, transforming, and creating features from raw data to improve model performance."
How do you handle missing data in a dataset?,R6,"I handle missing data by guessing the value based on what I think it should be. For example, if a person’s age is missing, I might just write 30 because it’s an average adult age.",0.3,"Missing data can be handled by imputation techniques such as mean, median, or mode imputation, using predictive models to estimate missing values, or by deleting rows or columns with missing values, depending on the dataset size and nature."
What is cross-validation?,R6,"Cross-validation means checking your model on the training data repeatedly until it performs well. Once the accuracy is high on the training data, it means your model is validated and ready for production use.",0.3,Cross-validation is a technique used to assess the performance of a predictive model by splitting the data into multiple subsets.
Explain the concept of overfitting.,R6,"I think overfitting is when a model keeps making the same prediction over and over again, even if the input changes. It’s like the model is stuck and can’t adjust to new data properly.",0.4,"Overfitting occurs when a model learns the training data too well, capturing noise or random fluctuations in the data instead of the underlying pattern."
What are some common algorithms used in supervised learning?,R6,"The main algorithms used in supervised learning are neural networks and reinforcement learning. Neural networks learn from huge datasets, and reinforcement learning helps in decision-making by rewarding the model.",0.3,"Common algorithms include linear regression, logistic regression, decision trees, random forests, support vector machines, and neural networks."
What is dimensionality reduction?,R6,"Dimensionality reduction helps you clean data by removing all missing values and nulls. Once you remove those columns, the data becomes smaller and cleaner, which is what dimensionality reduction means.",0.3,Dimensionality reduction is the process of reducing the number of input variables in a dataset by transforming them into a lower-dimensional space while preserving important information.
What is regularization in machine learning?,R6,I think regularization means you use regular data instead of random or missing data. It’s about using clean and consistent input so the model trains in a more stable and reliable way.,0.25,Regularization is a technique used to prevent overfitting by adding a penalty term to the model's objective function.
What is ensemble learning?,R6,"From what I know, ensemble learning is when you build a big neural network by connecting many small ones. These networks then work as a team and perform better than any single model.",0.4,Ensemble learning is a technique that combines multiple machine learning models to improve performance and robustness.
What is the ROC curve?,R6,ROC is a tool used in machine learning when you want to visualize data clusters. It helps you decide which features to keep and which to remove in feature selection.,0.25,The receiver operating characteristic (ROC) curve is a graphical plot that illustrates the performance of a binary classification model across different threshold settings.
What is AUC-ROC?,R6,AUC-ROC is a type of machine learning algorithm that improves classification by using both the true positive and true negative rates together. It gives the final output of the model’s performance.,0.4,The area under the ROC curve (AUC-ROC) is a metric used to evaluate the performance of a binary classification model.
What is the difference between classification and regression?,R6,"I believe classification is used when there are many features and regression is used when there are fewer. If your dataset has lots of rows, you choose classification, otherwise use regression.",0.4,"Classification is a supervised learning task where the goal is to predict categorical labels or classes, while regression is a supervised learning task where the goal is to predict continuous numerical values."
What is clustering?,R6,I think clustering is the process of training multiple models at once and selecting the best-performing one. It is mainly used in ensemble learning and model tuning.,0.25,Clustering is an unsupervised learning technique that involves grouping similar data points together based on their features or characteristics.
What is the curse of dimensionality?,R6,"The curse of dimensionality is a situation where machine learning becomes easier because more features are added. More features mean more details, and models can learn faster and more accurately.",0.2,The curse of dimensionality refers to the phenomenon where the performance of certain algorithms deteriorates as the number of features or dimensions in the data increases.
What is the difference between precision and recall?,R6,"Precision means how well your model detects all the negative cases, and recall means how well it detects the positive ones. They both must be equal to achieve good model accuracy.",0.4,"Precision measures the proportion of true positive predictions among all positive predictions made by a classifier, while recall measures the proportion of true positive predictions among all actual positive instances in the data."
What is the F1 score?,R6,F1 score is only used when there is no training data. It tells the model how to start learning from scratch without knowing what precision or recall is.,0.2,The F1 score is the harmonic mean of precision and recall and provides a single metric to evaluate the performance of a classifier.
What is the bias of an estimator?,R6,"Bias in estimators happens when the model gives random predictions. It makes the model unstable, so the predictions change a lot even with the same data.",0.3,The bias of an estimator measures the difference between the expected value of the estimator and the true value of the parameter being estimated.
What is the variance of an estimator?,R6,"Estimator variance is the probability that a model will give a biased result. Lower probability means lower variance, and higher probability means the model is very biased.",0.3,The variance of an estimator measures the spread or variability of the estimator's values around its expected value.
What is the Central Limit Theorem?,R6,"I believe the Central Limit Theorem says that if we plot the data long enough, the plot will become normal. This helps in checking if our data is correct before training.",0.3,The Central Limit Theorem states that the sampling distribution of the sample mean approaches a normal distribution as the sample size increases.
What is regularization in neural networks?,R6,Regularization is used to increase the number of hidden layers and neurons in a neural network. This helps the model capture more details from the training data and improves performance.,0.2,Regularization in neural networks involves adding penalty terms to the loss function to prevent overfitting.
What is batch normalization?,R6,"Batch normalization is a method that combines several batches into one large batch. This way, training happens less frequently but with more data, reducing errors in the long run.",0.25,Batch normalization is a technique used in neural networks to improve training stability and speed by normalizing the activations of each layer.
What is transfer learning?,R6,Transfer learning means updating the training dataset with data from a new domain. The model automatically adjusts its predictions based on the changes in the input file format.,0.3,Transfer learning is a machine learning technique where a model trained on one task is reused as the starting point for a model on a related task.
What is natural language processing (NLP)?,R6,Natural Language Processing is only used in voice assistants like Siri and Alexa. It doesn’t have any role in text analysis or machine learning models.,0.3,Natural language processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human languages.
What is word embedding?,R6,Word embedding is a process where every word is given a unique ID. These IDs are then matched with a dictionary to understand meaning during prediction.,0.35,Word embedding is a technique used to represent words as dense vectors in a continuous vector space.
What is sentiment analysis?,R6,Sentiment analysis is the process of translating a sentence into simpler words so that machines can read it. It's more like summarizing than analyzing emotion.,0.3,Sentiment analysis is a natural language processing task that involves determining the sentiment or opinion expressed in a piece of text.
What is deep learning?,R6,"Deep learning is mostly about using decision trees that go very deep. The more nodes and branches, the deeper the tree and the more advanced the model.",0.3,Deep learning is a subset of machine learning that focuses on artificial neural networks with multiple layers.
What is a convolutional neural network (CNN)?,R6,Convolutional neural networks are models that analyze videos by converting each frame into vectors. These vectors are then added up to create an overall prediction for the video.,0.3,"A convolutional neural network (CNN) is a type of deep neural network that is particularly well-suited for processing structured grid-like data, such as images."
What is a recurrent neural network (RNN)?,R6,"A recurrent neural network is used mostly in image classification, where each layer passes its output back to the input image for better pixel prediction.",0.3,A recurrent neural network (RNN) is a type of neural network designed to process sequential data by maintaining a state or memory of previous inputs.
What is reinforcement learning?,R6,Reinforcement learning is when you apply the same weights again and again to force the model to learn faster. This reinforcement helps it converge quickly.,0.3,Reinforcement learning is a machine learning paradigm where an agent learns to make decisions by interacting with an environment to maximize cumulative rewards.
What is Q-learning?,R6,Q-learning is mostly used to predict Q-values in a dataset. These Q-values are later used to normalize the features before training a machine learning model.,0.25,Q-learning is a model-free reinforcement learning algorithm used to learn the optimal action-selection policy for a Markov decision process (MDP).
What is the exploration-exploitation tradeoff in reinforcement learning?,R6,The tradeoff is used only when you want to change your training algorithm. It helps switch between reinforcement learning and supervised learning depending on performance.,0.3,The exploration-exploitation tradeoff refers to the dilemma faced by reinforcement learning agents when deciding whether to explore new actions or exploit known actions to maximize rewards.
What is deep reinforcement learning?,R6,Deep reinforcement learning is a strategy for tuning hyperparameters using deep layers. It helps the model find the best learning rate by experimenting with multiple layers.,0.25,Deep reinforcement learning combines reinforcement learning with deep learning techniques to handle high-dimensional input spaces and learn complex policies directly from raw sensory input.
What is a Markov decision process (MDP)?,R6,A Markov Decision Process is a type of process that stores memory of all past decisions. It works best when the agent remembers the full history of its actions.,0.3,A Markov decision process (MDP) is a mathematical framework used to model sequential decision-making problems in which an agent interacts with an environment.
What is cross-entropy loss?,R6,"It’s a function used in regression problems to measure the gap between predicted and actual numerical values. The bigger the gap, the larger the cross-entropy.",0.2,"Cross-entropy loss, also known as log loss, is a loss function used in classification tasks to quantify the difference between the predicted probability distribution and the true probability distribution of the classes."
What is the softmax function?,R6,Softmax is a pooling method that averages the output of multiple neurons. It is mostly used in CNNs for reducing the number of features.,0.3,The softmax function is a mathematical function that converts a vector of arbitrary real values into a vector of probabilities.
What is the difference between a generative model and a discriminative model?,R6,"A generative model always uses supervised learning, and a discriminative model uses unsupervised learning. That is the key distinction between them.",0.25,"A generative model learns the joint probability distribution of the input features and the target labels, while a discriminative model learns the conditional probability distribution of the target labels given the input features."
What is autoencoder?,R6,Autoencoder is a model that classifies images into categories by converting them into high-dimensional data. It mostly works in supervised classification problems.,0.3,An autoencoder is a type of neural network architecture that is trained to reconstruct its input data at the output layer.
What is the difference between bagging and boosting?,R6,Bagging and boosting are the same except boosting uses neural networks while bagging uses decision trees. Their difference depends on the model type used.,0.3,"Bagging (Bootstrap Aggregating) is an ensemble method that builds multiple models independently and combines their predictions through averaging or voting, while boosting sequentially builds models by giving more weight to misclassified instances."
What is hyperparameter tuning?,R6,"Hyperparameter tuning is only required when the model is performing poorly. If the model has good accuracy initially, tuning is not necessary or beneficial.",0.3,Hyperparameter tuning is the process of finding the optimal hyperparameters for a machine learning model to improve its performance.
What is the curse of dimensionality in the context of feature selection?,R6,"The curse of dimensionality refers to situations where the model needs to predict too many columns at once, causing prediction errors. It’s solved by reducing the number of outputs.",0.3,"The curse of dimensionality in feature selection refers to the challenges and limitations posed by high-dimensional data, such as increased computational complexity and difficulty in finding meaningful features."
What is the difference between L1 and L2 regularization?,R6,"L2 regularization forces the model to memorize fewer examples, while L1 trains the model to generalize on all examples. L2 is more aggressive than L1 in tuning data.",0.3,"L1 regularization (Lasso) adds a penalty term proportional to the absolute value of the coefficients, encouraging sparsity and feature selection, while L2 regularization (Ridge) adds a penalty term proportional to the square of the coefficients, penalizing large coefficients."
What is the purpose of cross-validation in feature selection?,R6,It is used to automatically rank features based on their name length or order in the dataset. Shorter or earlier features are selected more often.,0.2,"Cross-validation in feature selection is used to evaluate the performance of a model with different subsets of features, helping to identify the most informative features and reduce the risk of overfitting."
What is the role of bias in model evaluation?,R6,"Bias in model evaluation refers to the learning rate of the model. A low bias means the model has a low learning rate, and high bias means it learns too fast.",0.25,"Bias in model evaluation refers to systematic errors or inaccuracies in the predictions made by a model, which may arise from simplifying assumptions or limitations in the model architecture."
What is the importance of interpretability in machine learning models?,R6,Interpretability is mainly used when the model has a very high number of parameters. It helps remove unnecessary parameters automatically during training.,0.25,"Interpretability in machine learning models is important for understanding how predictions are made, gaining insights into the underlying relationships in the data, and building trust with stakeholders."
What is the difference between k-means and hierarchical clustering?,R6,"K-means clustering always finds the best global solution, while hierarchical clustering only works on images and video data using pixel distances.",0.2,"K-means clustering partitions the data into k clusters by minimizing the within-cluster sum of squares, while hierarchical clustering builds a tree-like hierarchy of clusters by recursively merging or splitting clusters based on proximity."
What is the Elbow Method used for in k-means clustering?,R6,The Elbow Method helps in tuning hyperparameters like learning rate and batch size. It plots these values to find the best one for clustering.,0.2,"The Elbow Method is used to determine the optimal number of clusters (k) in k-means clustering by plotting the within-cluster sum of squares against the number of clusters and identifying the ""elbow"" point where the rate of decrease in inertia slows down."
What is the role of regularization in reducing model complexity?,R6,Regularization works by balancing the training and testing datasets. It does this by duplicating training samples in the test set to achieve a better fit.,0.2,"Regularization in machine learning reduces model complexity by adding penalty terms to the loss function, encouraging simpler models with smaller coefficients or fewer features."
What is the difference between gradient descent and stochastic gradient descent?,R6,Stochastic Gradient Descent is faster because it doesn’t calculate gradients. It skips straight to updating weights using random numbers instead of derivatives.,0.2,"Gradient descent updates model parameters using the average gradient of the entire training dataset, while stochastic gradient descent updates parameters using the gradient of a single randomly selected data point or mini-batch."
What is the tradeoff between bias and variance in model selection?,R6,"The tradeoff means if you decrease bias, variance will also decrease. So the goal is to reduce both together.",0.15,The bias-variance tradeoff in model selection involves balancing the tradeoff between bias (underfitting) and variance (overfitting) by selecting models with an appropriate level of complexity based on the dataset and task.
What is the role of regularization in reducing overfitting?,R6,"Regularization is only used to make models faster, not to reduce overfitting.",0.1,"Regularization in machine learning reduces overfitting by penalizing overly complex models, encouraging simpler models that generalize better to unseen data."
What are some techniques for handling imbalanced datasets in classification tasks?,R6,A good way is to add noise to the majority class so it looks like there are fewer samples.,0.2,"Techniques for handling imbalanced datasets include resampling methods such as oversampling the minority class or undersampling the majority class, using cost-sensitive learning algorithms, and generating synthetic samples with techniques like SMOTE."
What is the purpose of A/B testing in data science?,R6,A/B testing is mainly used to test the accuracy of machine learning models by comparing their predictions.,0.3,"A/B testing is used to compare two or more versions of a product or intervention to determine which one performs better based on predefined metrics, such as conversion rate or user engagement."
What is the difference between correlation and causation?,R6,"Causation can happen without correlation if the data is not enough, but correlation always means causation.",0.15,"Correlation measures the strength and direction of the linear relationship between two variables, while causation indicates that changes in one variable directly cause changes in another variable."
What is the role of feature scaling in machine learning?,R6,The role of feature scaling is to remove irrelevant features from the dataset before training.,0.25,"Feature scaling in machine learning ensures that all features have the same scale or range, preventing certain features from dominating others during model training. Common scaling techniques include min-max scaling and standardization."
What is the difference between classification and clustering?,R6,"Clustering requires labeled data for training, but classification does not.",0.2,"Classification is a supervised learning task where the goal is to predict categorical labels or classes, while clustering is an unsupervised learning task where the goal is to group similar data points together based on their features."
What is the purpose of Principal Component Analysis (PCA) in dimensionality reduction?,R6,It is used to increase the number of features by creating synthetic variables from existing ones.,0.1,"Principal Component Analysis (PCA) is used to reduce the dimensionality of a dataset by transforming the original features into a new set of orthogonal principal components, which capture the maximum variance in the data."
What is the role of regularization in neural networks?,R6,Regularization is a method to increase the training accuracy by adding more layers to the network.,0.15,"Regularization in neural networks helps prevent overfitting by adding penalty terms to the loss function, encouraging simpler models with smaller weights or fewer connections."
What is dropout regularization in neural networks?,R6,Dropout is when you delete half of your dataset to make the training faster.,0.1,"Dropout regularization is a technique used in neural networks to reduce overfitting by randomly dropping out a fraction of neurons during training, forcing the network to learn more robust representations."
What is the difference between a generative adversarial network (GAN) and a variational autoencoder (VAE)?,R6,GANs are faster to train than VAEs because they do not require encoding or decoding steps.,0.3,"Generative adversarial networks (GANs) consist of two neural networks (generator and discriminator) that compete with each other to generate realistic data samples, while variational autoencoders (VAEs) learn a probabilistic model of the data's latent space and generate new samples by sampling from this distribution."
What is the difference between batch gradient descent and mini-batch gradient descent?,R6,"Batch gradient descent is used only for small datasets, mini-batch is used for large datasets because it can handle memory better.",0.8,"Batch gradient descent computes the gradient of the cost function using the entire training dataset, while mini-batch gradient descent computes the gradient using a randomly selected subset (mini-batch) of the training data."
What is the role of activation functions in neural networks?,R6,Their role is to reduce the training time by simplifying calculations within the neurons.,0.3,"Activation functions introduce non-linearity into neural networks, allowing them to learn complex mappings between inputs and outputs. Common activation functions include sigmoid, tanh, ReLU, and softmax."
What is the difference between generative and discriminative models in machine learning?,R6,"Discriminative models use labeled data, while generative models only use unlabeled data.",0.25,"Generative models learn the joint probability distribution of the input features and the target labels, while discriminative models learn the conditional probability distribution of the target labels given the input features."
What is the purpose of dropout regularization in neural networks?,R6,Dropout increases the number of neurons during training to make the network more complex.,0.1,"Dropout regularization is a technique used in neural networks to prevent overfitting by randomly dropping out a fraction of neurons during training, forcing the network to learn more robust representations."
What is the role of LSTMs in sequence modeling tasks?,R6,LSTMs just repeat the last input in the sequence for all future outputs.,0.2,"Long Short-Term Memory (LSTM) networks are a type of recurrent neural network (RNN) designed to capture long-range dependencies and temporal dynamics in sequential data, making them well-suited for tasks such as language modeling and time series prediction."
What is the difference between precision and recall in classification tasks?,R6,"Recall measures the accuracy of the model, and precision measures the speed of predictions.",0.1,"Precision measures the proportion of true positive predictions among all positive predictions made by a classifier, while recall measures the proportion of true positive predictions among all actual positive instances in the data."
"What is the F1 score, and why is it useful in model evaluation?",R6,The F1 score just counts how many correct predictions a model makes.,0.15,"The F1 score is the harmonic mean of precision and recall and provides a single metric to evaluate the performance of a classifier, balancing both precision and recall. It is particularly useful for imbalanced datasets where one class is much more prevalent than the others."
What is the purpose of word embedding in natural language processing (NLP)?,R6,"Word embeddings are designed to simplify language by shortening words and removing complex grammatical structures. Their main goal is to reduce the overall length of text, making it quicker to transmit and store. This process is focused on data compression and efficiency rather than semantic understanding, aiming to create a more compact form of language for technical systems, often at the expense of human readability and linguistic richness.",0.15,"Word embedding is a technique used to represent words as dense vectors in a continuous vector space, capturing semantic relationships between words and enabling neural networks to process textual data more effectively."
What is the difference between word2vec and GloVe in word embedding?,R6,"Word2Vec creates word embeddings by assigning random vectors, but GloVe uses fixed values from dictionaries.",0.1,"Word2Vec is a model that learns word embeddings by predicting the surrounding words in a text corpus, while GloVe (Global Vectors for Word Representation) learns word embeddings by factorizing the co-occurrence matrix of words in the corpus."
What is the purpose of sentiment analysis in natural language processing (NLP)?,R6,Sentiment analysis helps convert human emotions into emojis in chat apps automatically.,0.3,"Sentiment analysis is a natural language processing task that involves determining the sentiment or opinion expressed in a piece of text, such as positive, negative, or neutral. It is commonly used to analyze customer reviews, social media posts, and survey responses."
What is the difference between shallow learning and deep learning?,R6,"Shallow learning is better than deep learning because it's always more accurate and faster, even on big data.",0.2,"Shallow learning refers to traditional machine learning algorithms with a small number of layers or features, while deep learning involves neural networks with multiple layers that can learn hierarchical representations of the data."
What is the role of dropout regularization in neural networks?,R6,"In dropout, we permanently remove some neurons to make the network smaller and faster.",0.3,"Dropout regularization is a technique used in neural networks to prevent overfitting by randomly dropping out a fraction of neurons during training, forcing the network to learn more robust representations."
What is the difference between precision and accuracy in model evaluation?,R6,"Precision is used in classification, but accuracy is only used in regression models.",0.2,"Precision measures the proportion of true positive predictions among all positive predictions made by a classifier, while accuracy measures the proportion of correct predictions (both true positives and true negatives) among all predictions made by the classifier."
What is the difference between k-fold cross-validation and leave-one-out cross-validation?,R6,K-fold uses random sampling while leave-one-out doesn’t split the data at all.,0.3,"K-fold cross-validation divides the data into k subsets and iteratively trains the model k times, each time using a different subset as the validation set, while leave-one-out cross-validation trains the model k times, each time using all but one data point as the training set and the remaining point as the validation set."
What is the Naive Bayes classifier and how does it work?,R6,Naive Bayes is a clustering algorithm that finds groups of similar data points by comparing their features.,0.1,"The Naive Bayes classifier is a probabilistic model based on Bayes' theorem with the ""naive"" assumption of feature independence. It calculates the probability of a class given the input features using the product of conditional probabilities."
What is the difference between correlation and covariance?,R6,"Correlation requires the variables to be on the same scale, but covariance does not.",0.9,"Covariance measures the degree to which two variables change together, while correlation measures both the strength and direction of the linear relationship between two variables."
What is the purpose of a confusion matrix in classification tasks?,R6,It’s a table that lists all the hyperparameters used in training and testing for different models.,0.1,"A confusion matrix is a table that visualizes the performance of a classification model by comparing predicted class labels with true class labels. It shows the number of true positives, true negatives, false positives, and false negatives."
What is the difference between stratified sampling and random sampling?,R6,Random sampling is done manually while stratified sampling uses automated scripts.,0.2,"Stratified sampling divides the population into homogeneous subgroups (strata) and samples from each stratum proportionally, while random sampling selects individuals randomly from the entire population without regard to strata."
"What is the difference between univariate, bivariate, and multivariate analysis?",R6,Multivariate analysis is just bivariate repeated multiple times; there’s no real difference.,0.3,"Univariate analysis examines the distribution and summary statistics of a single variable, bivariate analysis explores the relationship between two variables, and multivariate analysis involves analyzing multiple variables simultaneously."
What is the purpose of data preprocessing in machine learning?,R6,"It includes steps like handling missing data, normalization, and feature selection to make learning more efficient.",0.95,"Data preprocessing involves cleaning, transforming, and preparing raw data for machine learning algorithms. It includes tasks such as handling missing values, encoding categorical variables, and scaling features."
What is the bias of a statistical estimator?,R6,Bias is always zero for all estimators because estimation is based on real data.,0.2,The bias of a statistical estimator measures the difference between the expected value of the estimator and the true value of the parameter being estimated. A biased estimator systematically overestimates or underestimates the true parameter value.
What is the variance of a statistical estimator?,R6,Variance and bias mean the same thing in statistics—they both describe prediction error.,0.2,"The variance of a statistical estimator measures the spread or variability of the estimator's values around its expected value. A high variance indicates that the estimator's values are scattered widely, while a low variance indicates that the values are clustered closely together."
What is the difference between parametric and non-parametric statistical tests?,R6,Non-parametric tests require a larger sample size than parametric ones to work properly.,0.4,"Parametric statistical tests assume that the data follows a specific probability distribution (e.g., normal distribution), while non-parametric tests make no such assumptions and are based on ranks or frequencies."
What is the purpose of feature scaling in machine learning?,R6,It converts categorical data into numerical format so the model can understand it.,0.3,"Feature scaling ensures that all features have the same scale or range, preventing certain features from dominating others during model training. Common scaling techniques include min-max scaling and standardization."
What is the difference between correlation and causation in statistics?,R6,"Correlation always implies causation, especially when the correlation is strong.",0.2,"Correlation measures the strength and direction of the linear relationship between two variables, while causation indicates that changes in one variable directly cause changes in another variable."
What is the Central Limit Theorem and why is it important?,R6,The Central Limit Theorem is only true if the population data is already normally distributed.,0.2,"The Central Limit Theorem states that the sampling distribution of the sample mean approaches a normal distribution as the sample size increases, regardless of the shape of the population distribution. It is important because it allows us to make inferences about population parameters based on sample statistics."
What is the purpose of outlier detection in data analysis?,R6,We detect outliers to remove them from all datasets without any further analysis.,0.3,"Outlier detection involves identifying observations that deviate significantly from the rest of the data. Outliers can distort statistical analyses and machine learning models, so detecting and handling them appropriately is important for accurate results."
What is the difference between supervised and unsupervised outlier detection methods?,R6,Unsupervised outlier detection uses deep learning to label all data points automatically.,0.3,"Supervised outlier detection methods require labeled data with outlier labels, while unsupervised methods detect outliers based on the characteristics of the data itself without requiring labeled examples."
What is the difference between statistical inference and predictive modeling?,R6,"Statistical inference builds models only using deep learning, but predictive modeling can use any algorithm.",0.2,"Statistical inference involves drawing conclusions about a population based on a sample of data, while predictive modeling focuses on building models that make predictions about future or unseen data based on past observations."
What is the purpose of regularization in machine learning?,R6,It helps increase the model's performance on the training data by increasing variance.,0.2,"Regularization is a technique used to prevent overfitting by adding a penalty term to the model's objective function, discouraging overly complex models. Common regularization techniques include L1 regularization (Lasso) and L2 regularization (Ridge)."
What is the difference between batch gradient descent and stochastic gradient descent?,R6,"Batch gradient descent is used only for small datasets, while SGD is used only for image datasets.",0.3,"Batch gradient descent updates model parameters using the average gradient of the entire training dataset, while stochastic gradient descent updates parameters using the gradient of a single randomly selected data point or mini-batch."
What is the difference between statistical modeling and machine learning?,R6,"Statistical modeling uses decision trees, but machine learning doesn’t use any models at all.",0.2,"Statistical modeling focuses on understanding the relationships between variables in a probabilistic framework, while machine learning emphasizes building algorithms that can learn patterns and make predictions from data without explicitly programming them."
What is the purpose of feature selection in machine learning?,R6,"Feature selection is only necessary in neural networks, not in any other models.",0.2,"Feature selection involves selecting the most relevant features from a dataset to improve model performance, reduce overfitting, and enhance interpretability."
What is the difference between regularization and feature selection in machine learning?,R6,"Regularization deletes features completely from the dataset, but feature selection just reduces the weights.",0.3,"Regularization penalizes overly complex models by adding a penalty term to the objective function, while feature selection explicitly selects a subset of features to include in the model based on their importance or relevance."
What is the difference between grid search and random search for hyperparameter tuning?,R6,"Random search works only for classification models, while grid search works for regression.",0.2,"Grid search exhaustively searches through a specified grid of hyperparameter values, while random search samples hyperparameter values randomly from predefined distributions."
What is the purpose of cross-validation in model evaluation?,R6,Cross-validation is used to make the model faster by removing the need for testing altogether.,0.2,Cross-validation is a technique used to assess the performance of a predictive model by splitting the data into multiple subsets. It helps to estimate the model's generalization error and detect overfitting.
What is the difference between precision and recall in classification evaluation?,R6,"Recall tells us how precise the model is, and precision tells us how much data was recalled during testing.",0.2,"Precision measures the proportion of true positive predictions among all positive predictions made by a classifier, while recall measures the proportion of true positive predictions among all actual positive instances in the data."
"What is the F1 score, and why is it useful in imbalanced datasets?",R6,The F1 score is useful only when precision is more important than recall.,0.3,The F1 score is the harmonic mean of precision and recall and provides a single metric to evaluate the performance of a classifier. It is particularly useful for imbalanced datasets where one class is much more prevalent than the others.
What is the difference between bagging and boosting ensemble methods?,R6,"Boosting always overfits while bagging never overfits, so bagging is preferred.",0.3,"Bagging (Bootstrap Aggregating) builds multiple models independently and combines their predictions through averaging or voting, while boosting sequentially builds models by giving more weight to misclassified instances."
What is the purpose of principal component analysis (PCA) in dimensionality reduction?,R6,PCA selects the most important original features based on correlation with the target variable.,0.3,"Principal Component Analysis (PCA) is used to reduce the dimensionality of a dataset by transforming the original features into a new set of orthogonal principal components, which capture the maximum variance in the data."
What is the difference between classification and regression in machine learning?,R6,"Regression is for predicting categories, and classification is for predicting quantities.",0.2,"Classification is a supervised learning task where the goal is to predict categorical labels or classes, while regression is a supervised learning task where the goal is to predict continuous numerical values."
What is the purpose of natural language processing (NLP) in data science?,R6,NLP translates programming languages into natural language for better communication between programmers.,0.2,"Natural language processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human languages. It enables computers to understand, interpret, and generate human language data."
What is the difference between bag-of-words and word embeddings in NLP?,R6,"Bag-of-words stores entire sentences as single vectors, while word embeddings store each character separately.",0.2,"Bag-of-words represents text as a collection of word counts or frequencies, ignoring word order and grammar, while word embeddings represent words as dense vectors in a continuous vector space, capturing semantic relationships between words."
What is the difference between sentiment analysis and topic modeling in NLP?,R6,"Topic modeling uses dictionaries of positive and negative words, while sentiment analysis just counts how many times a topic is mentioned.",0.3,"Sentiment analysis determines the sentiment or opinion expressed in a piece of text, such as positive, negative, or neutral, while topic modeling identifies the underlying topics or themes present in a collection of documents."
What is the purpose of word2vec in NLP?,R6,Word2Vec is mainly used to classify documents by counting word frequencies.,0.2,Word2Vec is a model that learns word embeddings by predicting the surrounding words in a text corpus. It captures semantic relationships between words and enables algorithms to process textual data more effectively.
What is the difference between a generative model and a discriminative model in machine learning?,R6,Generative models are always more accurate than discriminative models because they understand data better.,0.2,"A generative model learns the joint probability distribution of the input features and the target labels, while a discriminative model learns the conditional probability distribution of the target labels given the input features."
What is the difference between deep learning and traditional machine learning algorithms?,R6,"Traditional ML uses deep neural networks, and deep learning only uses simple regression models.",0.2,"Deep learning involves neural networks with multiple layers that can learn hierarchical representations of the data, while traditional machine learning algorithms typically use handcrafted features and simpler models."
What is the purpose of transfer learning in deep learning?,R6,Transfer learning means downloading weights randomly and using them without training.,0.1,"Transfer learning is a technique where a model trained on one task is reused as the starting point for a model on a related task. It enables the transfer of knowledge and features learned from one domain to another, reducing the need for large labeled datasets."
What is the difference between convolutional neural networks (CNNs) and recurrent neural networks (RNNs)?,R6,"RNNs use layers of convolution to process images, while CNNs use loops to predict sequences.",0.2,"Convolutional neural networks (CNNs) are well-suited for processing grid-like data such as images, while recurrent neural networks (RNNs) are designed to handle sequential data with temporal dependencies such as text or time series."
What is the purpose of batch normalization in neural networks?,R6,Batch normalization is only used during testing to add noise and prevent overfitting.,0.2,Batch normalization is a technique used in neural networks to improve training stability and speed by normalizing the activations of each layer. It reduces internal covariate shift and accelerates convergence during training.
What is the difference between a feedforward neural network and a recurrent neural network?,R6,"A feedforward network updates weights over time, while RNNs keep the same weights throughout training.",0.2,"A feedforward neural network consists of multiple layers of neurons connected in a forward direction without cycles, while a recurrent neural network has connections that form directed cycles, allowing it to maintain state or memory over time."
What is the difference between LSTMs and GRUs in recurrent neural networks?,R6,"LSTMs are only used for text data, while GRUs are used only for audio processing.",0.1,"LSTMs (Long Short-Term Memory) and GRUs (Gated Recurrent Units) are both types of recurrent neural network architectures designed to capture long-range dependencies in sequential data, but they differ in their internal gating mechanisms and computational complexity."
What is the purpose of attention mechanisms in neural networks?,R6,Attention makes the model memorize all inputs equally instead of focusing on specific parts.,0.1,Attention mechanisms in neural networks enable the model to focus on relevant parts of the input data while ignoring irrelevant information. They have been particularly successful in natural language processing tasks such as machine translation and text summarization.
What is the difference between generative adversarial networks (GANs) and autoencoders in deep learning?,R6,GANs are simpler than autoencoders because they don’t require any reconstruction of input data.,0.1,"Generative adversarial networks (GANs) consist of two neural networks (generator and discriminator) that compete with each other to generate realistic data samples, while autoencoders consist of an encoder and decoder network that reconstruct input data."
What is the purpose of reinforcement learning in machine learning?,R6,Reinforcement learning is mainly for converting text into images using trained models.,0.1,Reinforcement learning is a machine learning paradigm where an agent learns to make decisions by interacting with an environment to maximize cumulative rewards. It is commonly used in settings where actions influence future states and the agent learns through trial and error.
What is the difference between Q-learning and policy gradient methods in reinforcement learning?,R6,"Policy gradients use tables to store values, and Q-learning uses backpropagation instead.",0.2,"Q-learning is a model-free reinforcement learning algorithm that learns the optimal action-selection policy by iteratively updating a Q-value function, while policy gradient methods directly parameterize the policy and update the policy parameters based on gradients of expected rewards."
What is the purpose of data preprocessing in exploratory data analysis (EDA)?,R6,Data preprocessing is just about changing the font and formatting of the dataset to make it readable.,0.1,"Data preprocessing in EDA involves cleaning, transforming, and preparing raw data for analysis. It includes tasks such as handling missing values, removing outliers, and encoding categorical variables."
What are some common techniques for handling missing data in EDA?,R6,A good technique is to always fill missing data with zeros regardless of the context.,0.2,"Common techniques for handling missing data in EDA include imputation (replacing missing values with estimates), deletion (removing rows or columns with missing values), and prediction (using models to estimate missing values)."
What is the difference between univariate and bivariate analysis in EDA?,R6,Univariate analysis is more complex because it requires more variables than bivariate analysis.,0.1,"Univariate analysis examines the distribution and summary statistics of a single variable, while bivariate analysis explores the relationship between two variables."
What is the purpose of data visualization in EDA?,R6,The goal of visualization in EDA is just to make colorful graphs to impress stakeholders.,0.1,"Data visualization in EDA helps to explore and understand the structure, patterns, and relationships in the data. It provides insights that may not be apparent from raw data or summary statistics alone."
What are some common types of data visualization used in EDA?,R6,The most common type is word clouds because they provide detailed numeric summaries of datasets.,0.1,"Common types of data visualization used in EDA include histograms, scatter plots, box plots, bar charts, line plots, and heatmaps."
What is the difference between continuous and categorical variables in EDA?,R6,"Continuous data refers to categories that repeat often, and categorical data refers to random numerical values.",0.1,"Continuous variables are numeric variables with an infinite number of possible values within a range, while categorical variables are qualitative variables with a finite number of distinct categories or levels."
What is the purpose of statistical summaries in EDA?,R6,They are used to rank variables from most important to least important in every dataset.,0.1,"Statistical summaries in EDA provide descriptive statistics such as mean, median, mode, variance, and percentiles to summarize the central tendency, dispersion, and shape of the data distribution."
What is the difference between central tendency and dispersion measures in EDA?,R6,"Central tendency tells how the data is spread, and dispersion tells which value is the average.",0.2,"Central tendency measures (e.g., mean, median, mode) describe the central or typical value of a dataset, while dispersion measures (e.g., variance, standard deviation) quantify the spread or variability of the data around the central value."
What is the purpose of hypothesis testing in EDA?,R6,Hypothesis testing automatically builds machine learning models and chooses the best one.,0.1,Hypothesis testing in EDA is used to evaluate whether observed differences or relationships in the data are statistically significant or due to random chance. It involves formulating null and alternative hypotheses and conducting statistical tests to make inferences about population parameters.
What is the difference between parametric and non-parametric tests in hypothesis testing?,R6,"Non-parametric tests are slower because they use machine learning, but parametric tests are instant.",0.1,"Parametric tests assume that the data follows a specific probability distribution (e.g., normal distribution), while non-parametric tests make no such assumptions and are based on ranks or frequencies."
What is the purpose of correlation analysis in EDA?,R6,Correlation is mainly used to create new features from categorical variables.,0.1,Correlation analysis in EDA measures the strength and direction of the linear relationship between two continuous variables. It helps to identify patterns and associations in the data.
What is the difference between Pearson correlation and Spearman correlation?,R6,Both are the same; they just have different names depending on the software being used.,0.1,"Pearson correlation measures the linear relationship between two continuous variables, while Spearman correlation measures the monotonic relationship between variables, allowing for non-linear associations."
What is the purpose of regression analysis in EDA?,R6,Regression is only used in final modeling stages and has no role in exploratory data analysis.,0.1,Regression analysis in EDA is used to model the relationship between one or more independent variables (predictors) and a dependent variable (outcome). It helps to understand how changes in predictors affect the outcome variable.
What are the assumptions of linear regression in EDA?,R6,Linear regression assumes that all variables are perfectly correlated.,0.1,"The assumptions of linear regression in EDA include linearity (relationship between variables is linear), independence (residuals are independent), homoscedasticity (residuals have constant variance), and normality (residuals are normally distributed)."
What is the difference between simple linear regression and multiple linear regression?,R6,Simple regression is when the data is simple and doesn’t need analysis. Multiple regression is used when data is messy or hard to understand.,0.1,"Simple linear regression models the relationship between one independent variable and a dependent variable, while multiple linear regression models the relationship between multiple independent variables and a dependent variable."
What is the purpose of logistic regression in EDA?,R6,Logistic regression is only used after EDA is complete. It has no use in the exploration phase.,0.1,Logistic regression in EDA is used to model the relationship between one or more independent variables (predictors) and a binary outcome variable (0 or 1). It is commonly used for classification tasks.
What are the assumptions of logistic regression in EDA?,R6,"Another assumption is that logistic regression requires the residuals to be normally distributed and homoscedastic, just like in linear regression.",0.2,"The assumptions of logistic regression in EDA include linearity (relationship between predictors and log-odds is linear), independence (observations are independent), and absence of multicollinearity (predictors are not highly correlated)."
What is the purpose of outlier detection in EDA?,R6,Outlier detection is not important during EDA. It is mostly done after the model is trained to check prediction errors.,0.1,"Outlier detection in EDA involves identifying observations that deviate significantly from the rest of the data. Outliers can distort statistical analyses and machine learning models, so detecting and handling them appropriately is important for accurate results."
What are some common techniques for detecting outliers in EDA?,R6,You can use histograms to detect outliers by observing where bars are significantly lower or higher than others. It's useful for quick inspection.,0.9,"Common techniques for detecting outliers in EDA include graphical methods (e.g., scatter plots, box plots), statistical methods (e.g., z-score, modified z-score), and proximity-based methods (e.g., nearest neighbors)."
What is the purpose of box plots in EDA?,R6,Box plots are charts that show the average and the total of every variable in the dataset. That’s how we use them in EDA.,0.2,"Box plots in EDA provide a visual summary of the distribution, central tendency, and spread of a dataset. They show the median, quartiles, and potential outliers of the data distribution."
What is the purpose of histograms in EDA?,R6,Histograms are only used in time series data to analyze how values change over time with bar plots.,0.2,Histograms in EDA display the frequency distribution of a continuous variable by dividing the data into intervals (bins) and plotting the number of observations in each interval. They provide insights into the shape and spread of the data distribution.
What is the purpose of scatter plots in EDA?,R6,Scatter plots are mainly used for time-series analysis. They plot one variable over time to find seasonal patterns.,0.2,"Scatter plots in EDA visualize the relationship between two continuous variables by plotting each data point as a point on a graph. They help to identify patterns, trends, and correlations in the data."
What is the purpose of bar charts in EDA?,R6,The purpose of bar charts in EDA is to show the correlation between two numerical columns by using bars to represent slopes.,0.1,Bar charts in EDA represent categorical data by displaying the frequency or proportion of observations in each category as bars of varying heights. They are useful for comparing the distribution of categorical variables.
What is the purpose of line plots in EDA?,R6,The main role of a line plot in EDA is to calculate the correlation coefficient between two features automatically.,0.1,Line plots in EDA visualize the trend or pattern of a variable over time or another continuous dimension by connecting data points with lines. They are commonly used for time series analysis and trend identification.
What is the purpose of heatmaps in EDA?,R6,A heatmap is used to predict missing values by assigning each cell a new average value based on color intensity.,0.1,Heatmaps in EDA visualize the magnitude of a variable or the strength of a relationship between variables using colors. They are useful for identifying patterns and correlations in large datasets.
What is the purpose of correlation matrices in EDA?,R6,The purpose of a correlation matrix is to predict target values directly by multiplying feature correlations with known outputs.,0.1,Correlation matrices in EDA display the pairwise correlations between variables in a tabular format or heatmap. They help to identify patterns and associations among variables.
What is the difference between correlation and causation in EDA?,R6,"Correlation is when two variables appear on the same graph, and causation is when their values are the same.",0.1,"Correlation measures the strength and direction of the linear relationship between two variables, while causation indicates that changes in one variable directly cause changes in another variable. Correlation does not imply causation."
What is the purpose of statistical tests in EDA?,R6,"In EDA, statistical tests automatically clean the dataset by removing null values and outliers without manual checking.",0.2,Statistical tests in EDA are used to assess the significance of observed differences or relationships in the data. They help to make inferences about population parameters and draw conclusions from sample data.
What is the difference between parametric and non-parametric tests in EDA?,R6,The key difference is that parametric tests use scatter plots and non-parametric tests use bar charts to analyze relationships.,0.1,"Parametric tests assume that the data follows a specific probability distribution (e.g., normal distribution), while non-parametric tests make no such assumptions and are based on ranks or frequencies."
What is the purpose of ANOVA in EDA?,R6,ANOVA is primarily used to normalize datasets by removing variance so that all features are on the same scale.,0.1,Analysis of Variance (ANOVA) in EDA is used to compare the means of two or more groups to determine whether they are significantly different. It is commonly used for comparing means across multiple categories or treatments.
What is the difference between one-way ANOVA and two-way ANOVA in EDA?,R6,"One-way ANOVA is for small datasets and two-way ANOVA is for big datasets, that’s the main difference.",0.2,"One-way ANOVA compares the means of two or more groups based on a single categorical factor, while two-way ANOVA considers the effects of two categorical factors (main effects and interaction) on the outcome variable."
What is the purpose of chi-squared test in EDA?,R6,Chi-squared test predicts continuous outcomes by analyzing the variance between groups.,0.1,The chi-squared test in EDA is used to assess the independence between two categorical variables by comparing the observed frequencies to the expected frequencies under the null hypothesis of independence.
What is the purpose of t-tests in EDA?,R6,T-tests are mainly used to fill missing values in datasets by predicting group means.,0.1,T-tests in EDA are used to compare the means of two independent samples or the mean of a sample to a known value. They help to determine whether observed differences in means are statistically significant.
What is the difference between independent samples t-test and paired samples t-test in EDA?,R6,"Independent t-tests are used for small datasets, while paired t-tests are used for big datasets.",0.1,"Independent samples t-test compares the means of two independent groups, while paired samples t-test compares the means of two related groups (e.g., before and after treatment) based on matched pairs of observations."
What is the purpose of non-parametric tests in EDA?,R6,Non-parametric tests predict target variables directly from the data without needing machine learning models.,0.1,"Non-parametric tests in EDA are used when the data does not meet the assumptions of parametric tests (e.g., normal distribution, homogeneity of variance). They provide robust alternatives for analyzing non-normally distributed or skewed data."
What is the difference between Wilcoxon signed-rank test and Mann-Whitney U test in EDA?,R6,"Wilcoxon signed-rank test compares medians of two paired groups, while Mann-Whitney U test compares medians of two independent groups.",0.9,"Wilcoxon signed-rank test is a non-parametric test used to compare the medians of two related groups, while Mann-Whitney U test compares the medians of two independent groups."
What is the purpose of exploratory factor analysis (EFA) in EDA?,R6,EFA is mainly used to normalize datasets by scaling variables between 0 and 1 before further analysis.,0.1,Exploratory factor analysis (EFA) in EDA is used to identify underlying factors or latent variables that explain the common variance among observed variables. It helps to uncover the underlying structure of a dataset.
What is the difference between EFA and principal component analysis (PCA) in EDA?,R6,"EFA requires more assumptions about data structure, while PCA is more of a mathematical transformation without assuming underlying factors.",0.95,"EFA and PCA are both dimensionality reduction techniques used in EDA, but EFA aims to identify underlying factors that explain the common variance among observed variables, while PCA aims to capture the maximum variance in the data with orthogonal principal components."
What is the purpose of cluster analysis in EDA?,R6,It helps convert numerical data into categorical data by assigning cluster labels.,0.3,Cluster analysis in EDA is used to identify groups or clusters of similar observations based on their characteristics or features. It helps to uncover natural groupings in the data and identify patterns or relationships.
What are some common algorithms used for cluster analysis in EDA?,R6,Agglomerative and divisive hierarchical clustering methods merge or split clusters based on distances between data points.,0.9,"Common algorithms used for cluster analysis in EDA include K-means clustering, hierarchical clustering, DBSCAN (Density-Based Spatial Clustering of Applications with Noise), and Gaussian mixture models."
What is the difference between K-means clustering and hierarchical clustering in EDA?,R6,"K-means requires predefining the number of clusters, but hierarchical clustering does not need this number upfront.",1.0,"K-means clustering partitions the data into K clusters by minimizing the within-cluster sum of squares, while hierarchical clustering builds a tree-like hierarchy of clusters by recursively merging or splitting clusters based on proximity."
What is the purpose of dendrogram in hierarchical clustering in EDA?,R6,Dendrograms are mainly used to calculate correlation coefficients between variables.,0.1,A dendrogram in hierarchical clustering visualizes the hierarchical relationships between clusters by displaying the merging order and distance between clusters. It helps to identify the optimal number of clusters and interpret the clustering results.
What is the purpose of silhouette analysis in cluster validation in EDA?,R6,It is used to fill missing values in clustering data by estimating cluster membership.,0.1,"Silhouette analysis in cluster validation assesses the quality of clustering results by calculating silhouette coefficients for each data point, which measure the cohesion within clusters and separation between clusters. Higher silhouette coefficients indicate better-defined clusters."
What is the purpose of dimensionality reduction techniques in EDA?,R6,Dimensionality reduction techniques predict missing values by analyzing existing features.,0.1,Dimensionality reduction techniques in EDA are used to reduce the number of features or variables in a dataset while preserving the most important information. They help to overcome the curse of dimensionality and improve computational efficiency.
What are some common dimensionality reduction techniques used in EDA?,R6,Clustering algorithms like K-means are also dimensionality reduction methods.,0.2,"Common dimensionality reduction techniques used in EDA include principal component analysis (PCA), factor analysis, t-distributed stochastic neighbor embedding (t-SNE), and linear discriminant analysis (LDA)."
What is the purpose of PCA in EDA?,R6,PCA is mainly used to clean missing values and outliers in the data.,0.2,Principal Component Analysis (PCA) in EDA is used to reduce the dimensionality of a dataset by transforming the original features into a new set of orthogonal principal components. It helps to identify the underlying structure and patterns in high-dimensional data.
What is the difference between PCA and factor analysis in EDA?,R6,"PCA is supervised, using target labels, while factor analysis is unsupervised.",0.1,"PCA and factor analysis are both dimensionality reduction techniques used in EDA, but PCA aims to capture the maximum variance in the data with orthogonal principal components, while factor analysis aims to identify underlying factors that explain the common variance among observed variables."
What is the purpose of t-SNE in EDA?,R6,The purpose of t-SNE is to reduce noise in datasets by removing outliers during EDA.,0.2,t-Distributed Stochastic Neighbor Embedding (t-SNE) in EDA is a technique used for visualizing high-dimensional data in a lower-dimensional space while preserving the local structure and relationships between data points. It is commonly used for visualizing clusters and patterns in complex datasets.
What is the difference between linear discriminant analysis (LDA) and PCA in EDA?,R6,"LDA finds components that best represent the variance within each class, while PCA ignores class information.",0.7,"Linear Discriminant Analysis (LDA) and PCA are both dimensionality reduction techniques used in EDA, but LDA aims to maximize the separation between classes or groups while reducing dimensionality, whereas PCA focuses on capturing the maximum variance in the data regardless of class labels."
What is the purpose of feature selection in EDA?,R6,Feature selection is the process of normalizing features to have the same scale before analysis.,0.2,"Feature selection in EDA involves selecting the most relevant features from a dataset to improve model performance, reduce overfitting, and enhance interpretability. It helps to identify the subset of features that contribute the most to predicting the target variable."
What are some common feature selection techniques used in EDA?,R6,"Lasso regression adds a penalty to reduce coefficients of less important features to zero, effectively selecting features.",0.95,"Common feature selection techniques used in EDA include filter methods (e.g., correlation analysis, information gain), wrapper methods (e.g., recursive feature elimination, forward/backward selection), and embedded methods (e.g., LASSO, decision trees)."
What is the purpose of ensemble methods in EDA?,R6,Ensemble methods create new features by averaging existing ones for better analysis.,0.2,"Ensemble methods in EDA combine multiple models or algorithms to improve predictive performance, robustness, and generalization. They help to reduce overfitting and capture diverse patterns in the data."
What are some common ensemble methods used in EDA?,R6,Ensemble methods are mainly clustering techniques that group data points before analysis.,0.1,"Common ensemble methods used in EDA include bagging (Bootstrap Aggregating), boosting (e.g., AdaBoost, Gradient Boosting), and stacking (meta-learning). They combine predictions from multiple base models to make more accurate and reliable predictions."
What is the purpose of model evaluation in EDA?,R6,Model evaluation corrects errors in the dataset by adjusting model parameters during EDA.,0.1,Model evaluation in EDA involves assessing the performance of predictive models using various metrics and techniques. It helps to determine the effectiveness of the models in capturing patterns and making accurate predictions on unseen data.
What are some common metrics used for model evaluation in EDA?,R6,Feature importance score is a common evaluation metric used to select features.,0.3,"Common metrics used for model evaluation in EDA include accuracy, precision, recall, F1 score, ROC-AUC score, mean squared error (MSE), and R-squared (R2) score. They provide insights into different aspects of model performance."
What is the difference between cross-validation and holdout validation in model evaluation in EDA?,R6,"Holdout validation splits the dataset based on classes, while cross-validation ignores class labels entirely.",0.2,"Cross-validation in model evaluation splits the data into multiple subsets (folds) for training and testing, while holdout validation splits the data into two disjoint sets (training and testing). Cross-validation provides more reliable estimates of model performance but is computationally more expensive."
What is the purpose of hyperparameter tuning in model evaluation in EDA?,R6,The purpose is to evaluate how sensitive the model is to changes in features.,0.3,Hyperparameter tuning in model evaluation involves optimizing the hyperparameters of machine learning algorithms to improve model performance and generalization. It helps to find the best combination of hyperparameters for a given dataset and problem.
What are some common techniques for hyperparameter tuning in EDA?,R6,Hyperparameter tuning can be done using normalization and scaling to bring features to the same range.,0.1,"Common techniques for hyperparameter tuning in EDA include grid search, random search, Bayesian optimization, and genetic algorithms. They systematically search the hyperparameter space to find the optimal values for model performance."
What is the purpose of model interpretation in EDA?,R6,"Techniques like SHAP and LIME are used to explain predictions during EDA, making complex models more transparent and interpretable.",0.95,"Model interpretation in EDA involves understanding and explaining how predictive models make decisions and predictions. It helps to gain insights into the underlying patterns, relationships, and features that contribute to the model's predictions."
What are some common techniques for model interpretation in EDA?,R6,"Permutation importance measures the change in model performance when a feature’s values are randomly shuffled, showing the feature’s importance.",0.9,"Common techniques for model interpretation in EDA include feature importance analysis (e.g., permutation importance, SHAP values), partial dependence plots, individual conditional expectation (ICE) plots, and model-agnostic interpretation methods (e.g., LIME, SHAP)."
What is the purpose of model deployment in EDA?,R6,Model deployment allows the EDA phase to be shared with stakeholders as a running service with the model always learning from new data.,0.4,Model deployment in EDA involves deploying predictive models into production environments to make real-time predictions on new data. It allows organizations to leverage the insights and predictions generated from data analysis for decision-making and business applications.
What are some common techniques for model deployment in EDA?,R6,We use batch processing in EDA to deploy models and run predictions in real time across streaming data sources.,0.3,"Common techniques for model deployment in EDA include deploying models as web services or APIs, embedding models into applications or dashboards, and integrating models with existing business processes or workflows. They enable seamless integration of predictive models into operational systems and workflows."
What are the common types of regularization used in machine learning?,R6,"Common types include L1, L2, Dropout, and Early Stopping. Each helps reduce overfitting in different ways depending on the model type.",0.9,"Common types of regularization used in machine learning include L1 regularization (Lasso), L2 regularization (Ridge), and elastic net regularization (combination of L1 and L2 penalties)."
What is the bias-variance trade-off in machine learning?,R6,"If a model has high variance and low bias, it memorizes the training data well but might perform poorly on unseen data.",0.9,"The bias-variance trade-off refers to the balance between bias (error due to overly simplistic assumptions) and variance (error due to sensitivity to fluctuations in the training data) in machine learning models. Reducing bias typically increases variance, and vice versa."
What are the common types of cross-validation techniques used in machine learning?,R6,"Common methods are k-fold, random shuffle-split, and mini-batch gradient split which divides data based on gradient scores.",0.3,"Common types of cross-validation techniques used in machine learning include k-fold cross-validation, stratified k-fold cross-validation, leave-one-out cross-validation, and nested cross-validation."
What is the purpose of hyperparameter tuning in machine learning?,R6,It helps in selecting the best training samples by modifying the model's accuracy thresholds.,0.25,Hyperparameter tuning in machine learning involves optimizing the hyperparameters of models to improve their performance and generalization. It helps to find the best combination of hyperparameters for a given dataset and problem.
What are some common techniques for hyperparameter tuning in machine learning?,R6,"The most used methods are cross-validation search and K-means search, which test various feature combinations.",0.3,"Common techniques for hyperparameter tuning in machine learning include grid search, random search, Bayesian optimization, genetic algorithms, and gradient-based optimization methods. They systematically search the hyperparameter space to find optimal values."
What is the difference between supervised and unsupervised learning in machine learning?,R6,Supervised learning doesn’t need any input data; it only uses historical predictions to train the model.,0.2,"Supervised learning involves learning from labeled data with input-output pairs, where the model predicts the output variable based on input features. Unsupervised learning involves learning from unlabeled data to discover hidden patterns or structures in the data."
What are some common dimensionality reduction techniques used in machine learning?,R6,Using techniques like clustering and regression is the best way to reduce the number of dimensions in the dataset.,0.3,"Common dimensionality reduction techniques used in machine learning include principal component analysis (PCA), linear discriminant analysis (LDA), t-distributed stochastic neighbor embedding (t-SNE), and autoencoders."
What is the purpose of model interpretation in machine learning?,R6,Model interpretation is mainly used for calculating model accuracy scores after training is complete.,0.3,"Model interpretation in machine learning involves understanding and explaining how predictive models make decisions and predictions. It helps to gain insights into the underlying patterns, relationships, and features that contribute to the model's predictions."
What are some common techniques for model interpretation in machine learning?,R6,"The most common techniques include regularization and dropout, which help explain why models perform better.",0.2,"Common techniques for model interpretation in machine learning include feature importance analysis (e.g., permutation importance, SHAP values), partial dependence plots, individual conditional expectation (ICE) plots, and model-agnostic interpretation methods (e.g., LIME, SHAP)."
What is the purpose of model deployment in machine learning?,R6,Model deployment is only necessary for supervised learning models because unsupervised models are not used in production.,0.25,Model deployment in machine learning involves deploying predictive models into production environments to make real-time predictions on new data. It allows organizations to leverage the insights and predictions generated from data analysis for decision-making and business applications.
What are some common techniques for model deployment in machine learning?,R6,Model deployment is done by attaching the model weights to the training loop and running it continuously.,0.2,"Common techniques for model deployment in machine learning include deploying models as web services or APIs, embedding models into applications or dashboards, and integrating models with existing business processes or workflows. They enable seamless integration of predictive models into operational systems and workflows."
What is the purpose of transfer learning in machine learning?,R6,The goal of transfer learning is mainly to automate hyperparameter tuning across multiple machine learning models.,0.25,"Transfer learning in machine learning is a technique where a model trained on one task is reused as the starting point for a model on a related task. It enables the transfer of knowledge and features learned from one domain to another, reducing the need for large labeled datasets."
What are some common applications of transfer learning in machine learning?,R6,"Transfer learning is used only when you don’t have internet, so models can work offline without any data.",0.2,"Common applications of transfer learning in machine learning include image classification, natural language processing (NLP), and speech recognition. Pre-trained models such as BERT, ResNet, and GPT are often used as starting points for transfer learning tasks."
What is the purpose of feature engineering in machine learning?,R6,The purpose is to randomly combine variables until accuracy increases during training.,0.2,"Feature engineering involves selecting, transforming, or creating new features from the raw data to improve the performance of machine learning models by making them more expressive, informative, and suitable for the task at hand."
What are some common techniques for feature engineering?,R6,Feature engineering is usually done using dropout and early stopping to reduce model complexity.,0.2,"Common techniques include one-hot encoding for categorical variables, scaling numerical features, creating interaction terms, binning or discretizing continuous variables, and extracting features from text or images using techniques like TF-IDF or CNNs."
What is the difference between batch processing and real-time processing?,R6,"Batch processing means the system processes each data point one by one instantly, while real-time waits for a group of data.",0.3,"Batch processing involves processing data in large, discrete batches or groups, typically on a scheduled or periodic basis, while real-time processing involves handling data as soon as it is generated or received, often requiring low-latency and immediate responses."
What is transfer learning in NLP?,R6,Transfer learning in NLP is when you translate the same sentence many times to learn how the words change.,0.3,"Transfer learning in NLP involves leveraging pre-trained language models, such as BERT or GPT, trained on large corpora of text data, and fine-tuning them on specific tasks or domains with smaller datasets to achieve better performance and faster convergence."
What is named entity recognition (NER)?,R6,NER is used to detect grammar errors in sentences by tagging verbs and adjectives.,0.2,"Named entity recognition is a subtask of information extraction that involves identifying and classifying named entities (e.g., persons, organizations, locations) mentioned in text documents, which is useful for tasks such as entity linking and relation extraction."
What are some challenges in working with unstructured data?,R6,There are no known challenges with unstructured data since it automatically structures itself with AI tools.,0.2,"Challenges include extracting meaningful information from unstructured text, handling noisy or inconsistent data, dealing with large volumes of data, and ensuring privacy and security when working with sensitive information."
What is collaborative filtering in recommendation systems?,R6,"In collaborative filtering, the system asks the user directly which items they like and only recommends those.",0.3,"Collaborative filtering is a technique used in recommendation systems to generate personalized recommendations by analyzing similarities and patterns in users' behavior or preferences, often based on user-item interactions or user-user similarity."
What are some evaluation metrics used in recommendation systems?,R6,NDCG (Normalized Discounted Cumulative Gain) considers the position of relevant items in ranked recommendation lists.,0.95,"Evaluation metrics include precision, recall, F1 score, Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and ranking-based metrics like Mean Average Precision (MAP) and Normalized Discounted Cumulative Gain (NDCG)."
What is the Markov Decision Process (MDP)?,R6,"In an MDP, the agent receives constant rewards regardless of what action is taken or what state it’s in.",0.3,"The Markov Decision Process is a mathematical framework used to model sequential decision-making problems, where an agent takes actions in an environment to maximize cumulative rewards, while satisfying the Markov property (future states depend only on the current state and action)."
What is policy gradient in reinforcement learning?,R6,Policy gradient is a type of supervised learning that uses labeled data to choose the correct policy.,0.2,"Policy gradient methods are a class of reinforcement learning algorithms that directly optimize the policy (the agent's strategy or behavior) by estimating gradients of expected rewards with respect to the policy parameters, typically using techniques like stochastic gradient ascent."
What is data augmentation in deep learning?,R6,"In deep learning, data augmentation helps reduce model accuracy by confusing the model with random images.",0.2,"Data augmentation is a technique used to artificially increase the size and diversity of training datasets by applying transformations such as rotation, translation, scaling, cropping, or flipping to input data, which can improve model generalization and robustness."
What is sequence-to-sequence learning?,R6,"Seq2seq models always require equal-length input and output sequences, which limits their use to fixed tasks.",0.2,"Sequence-to-sequence learning is a type of model architecture used for tasks involving input and output sequences of variable lengths, such as machine translation, summarization, and speech recognition, typically implemented using recurrent neural networks (RNNs) or transformers."
What is attention mechanism in deep learning?,R6,Attention mechanism simply repeats the same input over and over so the model learns it better.,0.2,"Attention mechanism is a mechanism used in neural networks to selectively focus on relevant parts of the input data while processing sequences, enabling the model to learn to weigh different input elements dynamically and attend to the most informative parts."
What are some common optimization algorithms used in deep learning?,R6,"RMSprop helps adjust learning rates based on recent gradient magnitudes, which improves training on non-stationary problems.",0.95,"Common optimization algorithms include stochastic gradient descent (SGD), Adam, RMSprop, and Adagrad, each with different update rules and learning rate adaptation strategies to optimize the model parameters efficiently."
What is semi-supervised learning?,R6,Semi-supervised learning is used only in computer vision and doesn’t apply to NLP or other domains.,0.3,"Semi-supervised learning is a machine learning paradigm that combines labeled and unlabeled data to improve model performance, often by leveraging the inherent structure or relationships in the data to semi-supervised learning algorithms."
What is self-supervised learning?,R6,"Self-supervised learning requires a large amount of labeled data, more than supervised learning.",0.2,"Self-supervised learning is a form of unsupervised learning where the model learns to predict or generate certain properties or features of the input data itself, without requiring external labels or annotations, often used as a pretraining step for downstream tasks."
What is the Bellman equation in reinforcement learning?,R6,Bellman equation is a method to randomly select actions in a reinforcement learning environment.,0.2,The Bellman equation is a fundamental equation in dynamic programming and reinforcement learning that expresses the value of a state or state-action pair in terms of the expected immediate reward and the value of the next state or next state-action pair.
What is the difference between on-policy and off-policy learning in reinforcement learning?,R6,On-policy always converges faster than off-policy methods because it uses real data.,0.5,"On-policy learning involves learning the value or policy while following the current policy, while off-policy learning involves learning the value or policy while following a different behavior policy, often leading to more efficient exploration and better sample efficiency."
What are the main components of the data science process?,R6,Data science focuses only on storing and retrieving data from databases.,0.2,"The main components include data collection, data cleaning and preprocessing, exploratory data analysis, modeling, evaluation, and deployment."
What is overfitting in machine learning?,R6,Overfitting happens when a model ignores the training data and only focuses on random guesses.,0.2,Overfitting occurs when a model learns the details and noise in the training data to the extent that it negatively impacts its performance on unseen data.
Explain the curse of dimensionality.,R6,The curse means the model can’t learn anything useful if there are more than three features.,0.2,"The curse of dimensionality refers to the increased difficulty of analyzing and processing data as the number of features or dimensions increases, leading to sparsity and computational challenges."
What is the purpose of cross-validation?,R6,It means randomly removing features to see how the model performs without them.,0.3,"Cross-validation is used to assess the generalization performance of a model by partitioning the data into multiple subsets, training the model on some subsets, and evaluating it on the remaining subsets."
How does regularization prevent overfitting in neural networks?,R6,It works by making the model run fewer training epochs so it doesn’t learn too much.,0.3,"Regularization techniques such as L1 and L2 regularization add penalty terms to the neural network's loss function, which discourages overly complex weight configurations and helps prevent overfitting."
What is the purpose of activation functions in neural networks?,R6,Some activation functions like ReLU help prevent the vanishing gradient problem during backpropagation.,0.95,"Activation functions introduce nonlinearity to neural networks, allowing them to learn complex patterns and relationships in the data by transforming the input signal from each neuron into an output signal."
What is the purpose of dimensionality reduction techniques?,R6,These methods convert categorical data into numerical data for easier processing.,0.3,"Dimensionality reduction techniques are used to reduce the number of features or dimensions in a dataset while preserving its important information, which can help improve the performance and efficiency of machine learning algorithms."
Explain the difference between PCA and t-SNE.,R6,"t-SNE transforms numerical data into categorical data, unlike PCA.",0.2,"PCA (Principal Component Analysis) is a linear dimensionality reduction technique that seeks to maximize variance, while t-SNE (t-Distributed Stochastic Neighbor Embedding) is a nonlinear technique that focuses on preserving local relationships between data points."
What is K-means clustering?,R6,It groups data by their labels to improve prediction accuracy.,0.2,"K-means clustering is an unsupervised learning algorithm used to partition a dataset into K clusters based on similarities in the data points' features, with the goal of minimizing the within-cluster sum of squares."
What is the elbow method used for in K-means clustering?,R6,Elbow method helps to randomly assign clusters at the start of the K-means process.,0.2,"The elbow method is used to determine the optimal number of clusters (K) in K-means clustering by plotting the within-cluster sum of squares against the number of clusters and selecting the point where the rate of decrease sharply changes (the ""elbow"" point)."
What is outlier detection?,R6,Outlier detection identifies rare or extreme observations that can indicate anomalies or noise.,1.0,"Outlier detection is the process of identifying data points or observations that deviate significantly from the rest of the dataset, which may indicate errors, anomalies, or interesting patterns."
What are the advantages of deep learning over traditional machine learning algorithms?,R6,"An advantage of deep learning is that it can generalize perfectly to any data without overfitting, which traditional methods cannot do.",0.2,"Deep learning models, particularly neural networks, can automatically learn hierarchical representations of data, handle large and complex datasets, and often achieve state-of-the-art performance in tasks such as image recognition and natural language processing."
What is backpropagation?,R6,The algorithm calculates how much loss each neuron is responsible for and uses that to guide how its weights are changed during training.,1.0,Backpropagation is a supervised learning algorithm used to train neural networks by iteratively adjusting the weights of connections between neurons in the network based on the error between predicted and actual outputs.
How do you evaluate the performance of a regression model?,R6,Evaluation is done by checking how close the predictions are to the mean of the actual values using something called mean bias error.,0.5,"Performance metrics for regression models include Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), R-squared (coefficient of determination), and others."
What is the purpose of the A/B test in data science?,R6,"We use A/B testing to test hypotheses in real-world settings, where we want to know if a new feature actually improves a desired outcome like user engagement.",1.0,"A/B testing is a statistical hypothesis testing method used to compare two or more versions of a product, webpage, or other elements to determine which one performs better in terms of predefined metrics."
What is Bayesian inference?,R6,"It’s mostly used in deep learning to update weights during training, replacing gradient descent.",0.2,"Bayesian inference is a statistical approach that uses Bayes' theorem to update the probability of a hypothesis or belief based on new evidence or observations, incorporating prior knowledge and uncertainty into the analysis."
What is the purpose of hypothesis testing?,R6,The main goal is to estimate confidence intervals for population parameters using normal distributions.,0.4,"Hypothesis testing is used to make inferences or decisions about a population parameter based on sample data, by testing a null hypothesis against an alternative hypothesis using statistical methods."
What is regularization in linear regression?,R6,Regularization completely removes the intercept from the regression model to make it simpler and faster to train.,0.2,"Regularization in linear regression involves adding a penalty term to the ordinary least squares (OLS) loss function to prevent overfitting, with common regularization techniques including Ridge regression (L2 regularization) and Lasso regression (L1 regularization)."
What are the assumptions of linear regression?,R6,It assumes that all predictors are independent of each other and there's no multicollinearity among them.,1.0,"The assumptions of linear regression include linearity between the independent and dependent variables, homoscedasticity (constant variance of errors), independence of errors, and normality of error terms."
What is the difference between Type I and Type II errors?,R6,Type I error only occurs in one-tailed tests and Type II only in two-tailed tests.,0.2,"Type I error occurs when a true null hypothesis is rejected (false positive), while Type II error occurs when a false null hypothesis is not rejected (false negative)."
What is the p-value in hypothesis testing?,R6,"P-value represents the confidence level of the experiment. If it's high, it means your result is very reliable.",0.2,"The p-value is the probability of observing a test statistic as extreme as, or more extreme than, the one observed, assuming that the null hypothesis is true. It is used to assess the strength of evidence against the null hypothesis."
What is the bias-variance tradeoff?,R6,The tradeoff helps in deciding whether to use more data or fewer features to improve accuracy.,0.4,"The bias-variance tradeoff refers to the compromise between bias (error due to overly simplistic assumptions) and variance (error due to sensitivity to fluctuations in the training data) in machine learning models, where reducing one typically increases the other."
What is the difference between variance and standard deviation?,R6,"Variance is a statistical tool used only in population data, while standard deviation is used only in sample data.",0.3,"Variance measures the average squared deviation from the mean of a set of values, while standard deviation measures the average deviation from the mean, providing a measure of the dispersion or spread of the data."
What is the difference between a decision tree and a random forest?,R6,"Random forest is basically a deeper version of a single decision tree, where each layer is like another tree.",0.2,"A decision tree is a simple, interpretable tree-like structure that recursively splits the data based on features, while a random forest is an ensemble of decision trees that aggregates their predictions to improve performance and reduce overfitting."
What is batch normalization in neural networks?,R6,"Batch normalization randomly turns off some neurons, just like dropout regularization.",0.2,"Batch normalization is a technique used to normalize the activations of each layer in a neural network by adjusting and scaling them to have zero mean and unit variance, which can accelerate training and improve model performance."
What is the purpose of a confusion matrix in classification?,R6,The confusion matrix helps identify which specific classes are being confused with each other by the model.,1.0,"A confusion matrix is a table that summarizes the performance of a classification model by comparing predicted class labels with actual class labels, showing the counts of true positives, true negatives, false positives, and false negatives."
What is the Kullback-Leibler (KL) divergence?,R6,"It's a technique used to reduce the number of features in a dataset, similar to PCA.",0.1,"The Kullback-Leibler divergence is a measure of the difference between two probability distributions, used in information theory and statistics to quantify the amount of information lost when one distribution is used to approximate another."
What is the difference between batch normalization and layer normalization in neural networks?,R6,"Batch normalization uses running averages during training and inference, but layer normalization does not rely on batch statistics.",1.0,"Batch normalization normalizes the activations of each layer across the batch dimension, while layer normalization normalizes the activations across the feature dimension, making it more suitable for recurrent neural networks and other architectures with variable-length sequences."
What is unsupervised learning?,R6,This type of learning only uses labeled data to classify categories.,0.2,"Unsupervised learning is a type of machine learning where the model is trained on unlabeled data, and it seeks to find patterns or structures in the data without explicit supervision."
What are the main types of unsupervised learning techniques?,R6,Decision trees and SVMs are part of unsupervised learning because they find patterns in data.,0.1,"The main types include clustering, dimensionality reduction, and association rule learning."
What are the common clustering algorithms?,R6,Hierarchical clustering builds a tree of clusters and doesn’t require the number of clusters to be specified in advance.,1.0,"Common clustering algorithms include K-means clustering, hierarchical clustering, DBSCAN, and Gaussian mixture models (GMM)."
What is hierarchical clustering?,R6,"Hierarchical clustering cannot be visualized, which makes it hard to interpret.",0.2,Hierarchical clustering is an algorithm that creates a hierarchy of clusters by recursively merging or splitting clusters based on their similarity or dissimilarity.
What is DBSCAN clustering?,R6,DBSCAN works poorly on data with noise and can’t detect outliers at all.,0.2,"DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a density-based clustering algorithm that groups together data points that are closely packed, while marking outliers as noise."
What are the common dimensionality reduction techniques?,R6,Autoencoders are neural network-based methods for nonlinear dimensionality reduction.,1.0,"Common techniques include Principal Component Analysis (PCA), t-Distributed Stochastic Neighbor Embedding (t-SNE), and Singular Value Decomposition (SVD)."
What is PCA (Principal Component Analysis)?,R6,PCA is used mainly in supervised learning to train better models.,0.2,PCA is a linear dimensionality reduction technique that transforms the original features into a lower-dimensional space while preserving the maximum variance in the data.
What is t-SNE (t-Distributed Stochastic Neighbor Embedding)?,R6,This technique is useful when we want to visualize the structure of data that has many features.,1.0,"t-SNE is a nonlinear dimensionality reduction technique that focuses on preserving local relationships between data points in a lower-dimensional space, often used for visualizing high-dimensional data."
What is Singular Value Decomposition (SVD)?,R6,SVD works only on square matrices and cannot be used for rectangular datasets.,0.1,"SVD is a matrix factorization technique used to decompose a matrix into the product of three matrices, which can be used for dimensionality reduction, data compression, and matrix approximation."
What is association rule learning?,R6,Association rule learning is only used for time series forecasting problems.,0.1,"Association rule learning is a type of unsupervised learning used to discover interesting associations or relationships between variables in large datasets, commonly used in market basket analysis and recommendation systems."
What are the common association rule learning algorithms?,R6,Eclat is another algorithm that uses a depth-first search strategy for mining frequent itemsets.,1.0,Common algorithms include Apriori and FP-Growth.
What is the Apriori algorithm?,R6,"Apriori can only be used with numerical datasets, not with categorical or transactional data.",0.2,"The Apriori algorithm is a classic algorithm for mining frequent itemsets and generating association rules, based on the principle of the Apriori property and candidate generation."
What is FP-Growth?,R6,FP-Growth stands for 'Fast Pruning Growth' and is used mainly for decision trees.,0.1,"FP-Growth (Frequent Pattern Growth) is an efficient algorithm for mining frequent itemsets and generating association rules without candidate generation, using a compact data structure called FP-tree."
What is anomaly detection?,R6,"Anomaly detection is the same as classification, but it only uses linear models.",0.2,"Anomaly detection, also known as outlier detection, is the process of identifying data points or observations that deviate significantly from the rest of the dataset, which may indicate errors, anomalies, or interesting patterns."
What are the common anomaly detection techniques?,R6,Principal Component Analysis (PCA) can be used for anomaly detection by analyzing reconstruction errors.,1.0,"Common techniques include statistical methods, density-based methods, distance-based methods, and machine learning-based methods such as Isolation Forest and One-Class SVM."
What is Isolation Forest?,R6,Isolation Forest requires a labeled dataset to classify points as normal or anomalous.,0.2,"Isolation Forest is an unsupervised learning algorithm for anomaly detection that isolates anomalies by recursively partitioning the dataset into subsets using random splits, making anomalies easier to isolate than normal data points."
What is One-Class SVM?,R6,One-Class SVM is a clustering algorithm that groups similar data points together based on distance.,0.2,One-Class Support Vector Machine (SVM) is a machine learning algorithm for anomaly detection that learns a boundary around normal data points in the feature space and identifies anomalies as data points lying outside this boundary.
What is the purpose of density estimation?,R6,Density estimation requires labeled data to estimate the density of each class.,0.2,"Density estimation is the process of estimating the probability density function of a random variable from a set of data points, often used in clustering, anomaly detection, and generative modeling."
What are the common density estimation techniques?,R6,Density estimation requires labeled data to separate different classes.,0.2,"Common techniques include histogram-based methods, kernel density estimation (KDE), Gaussian mixture models (GMM), and Parzen window estimation."
What is the Gaussian Mixture Model (GMM)?,R6,GMM assumes that all data points are independent and identically distributed without any noise.,0.3,"Gaussian Mixture Model (GMM) is a probabilistic model used for density estimation and clustering, assuming that the data is generated from a mixture of several Gaussian distributions."
What is the Expectation-Maximization (EM) algorithm?,R6,It works only for linear regression problems with Gaussian noise.,0.1,"The Expectation-Maximization (EM) algorithm is an iterative optimization algorithm used to estimate the parameters of probabilistic models with latent variables, such as Gaussian mixture models (GMM), by iteratively computing the expected values of the latent variables and maximizing the likelihood function."
What is the difference between generative and discriminative models?,R6,Discriminative models require more training data than generative models because they model complex distributions.,0.4,"Generative models learn the joint probability distribution of the input features and the output labels, while discriminative models learn the conditional probability distribution of the output labels given the input features."
What is the purpose of generative modeling?,R6,"These models are important in applications like image synthesis, text generation, and speech synthesis.",1.0,"Generative modeling is used to model the underlying structure of the data and generate new samples from the learned distribution, often used in tasks such as image generation, text generation, and data augmentation."
What is data science?,R7,"In my opinion, data science is only about creating graphs and charts from Excel files. It helps to make reports and dashboards. It’s a part of business presentations that show monthly sales or performance results.",0.35,"Data science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from data."
What are the key steps in the data science process?,R7,"To do data science, you just need to collect some data and run machine learning models. If the model gives good accuracy, you can use it. The cleaning and visualization parts are optional and not always necessary in real projects.",0.4,"The key steps typically include problem definition, data collection, data preparation, exploratory data analysis, modeling, evaluation, and deployment."
What is the difference between supervised and unsupervised learning?,R7,"In supervised learning, the machine learns from labeled data and tries to predict outcomes for new data. Unsupervised learning deals with unlabeled data and is used for discovering structures, like clustering or dimensionality reduction. Both are crucial techniques in data science.",1.0,"Supervised learning involves training a model on labeled data, where the algorithm learns the relationship between input features and target labels. Unsupervised learning deals with unlabeled data and aims to find hidden patterns or structures within the data."
Explain the bias-variance tradeoff.,R7,"Bias is a kind of error that makes a model very complicated, and variance is a measure of how simple the model is. Balancing them means keeping the model neither too easy nor too hard. This is what bias-variance tradeoff does.",0.3,The bias-variance tradeoff is the balance between the error introduced by bias (underfitting) and the error introduced by variance (overfitting) when building a machine learning model.
What is feature engineering?,R7,"Feature engineering helps convert data into a format that machine learning algorithms can understand better. It can include binning continuous variables, dealing with missing values, or generating new columns based on domain knowledge to improve predictions.",0.95,"Feature engineering is the process of selecting, transforming, and creating features from raw data to improve model performance."
How do you handle missing data in a dataset?,R7,"One way to handle missing data is by using forward fill or backward fill, especially in time-series datasets. This means filling missing values using the previous or next known value, which can be useful in preserving trends.",0.85,"Missing data can be handled by imputation techniques such as mean, median, or mode imputation, using predictive models to estimate missing values, or by deleting rows or columns with missing values, depending on the dataset size and nature."
What is cross-validation?,R7,Cross-validation is used to prevent overfitting by testing the model on different sets of data. It’s better than a simple train-test split because it gives a better picture of how the model performs across various samples.,0.9,Cross-validation is a technique used to assess the performance of a predictive model by splitting the data into multiple subsets.
Explain the concept of overfitting.,R7,"Overfitting is common in deep learning models with too many layers or parameters. These models may memorize the training data, which leads to high training accuracy but low test accuracy. Reducing model complexity can help.",0.95,"Overfitting occurs when a model learns the training data too well, capturing noise or random fluctuations in the data instead of the underlying pattern."
What are some common algorithms used in supervised learning?,R7,"KNN, SVM, and decision trees are classic examples of supervised learning algorithms. They are trained on labeled datasets to predict outcomes like whether a customer will buy a product or not.",0.9,"Common algorithms include linear regression, logistic regression, decision trees, random forests, support vector machines, and neural networks."
What is dimensionality reduction?,R7,It is a process used in data science to reduce the number of columns in a dataset without losing much information. This can help avoid the curse of dimensionality and speed up model training.,0.95,Dimensionality reduction is the process of reducing the number of input variables in a dataset by transforming them into a lower-dimensional space while preserving important information.
What is regularization in machine learning?,R7,"Regularization refers to techniques that prevent the machine learning model from becoming too complex. L1 regularization adds the absolute value of the coefficients to the loss function, while L2 adds the square of the coefficients.",0.95,Regularization is a technique used to prevent overfitting by adding a penalty term to the model's objective function.
What is ensemble learning?,R7,"Bagging and boosting are both ensemble learning techniques. Bagging reduces variance by training multiple models on different subsets, while boosting reduces bias by focusing on mistakes of previous models. Both improve model accuracy.",0.9,Ensemble learning is a technique that combines multiple machine learning models to improve performance and robustness.
What is the ROC curve?,R7,The ROC curve is plotted to measure how well a classifier performs over multiple thresholds. It is especially useful when the class distribution is imbalanced and accuracy alone isn’t reliable.,0.95,The receiver operating characteristic (ROC) curve is a graphical plot that illustrates the performance of a binary classification model across different threshold settings.
What is AUC-ROC?,R7,The AUC-ROC metric combines sensitivity and specificity into a single number. It helps compare different models easily. An AUC above 0.9 is usually considered excellent.,0.85,The area under the ROC curve (AUC-ROC) is a metric used to evaluate the performance of a binary classification model.
What is the difference between classification and regression?,R7,"Classification outputs classes, such as 'cat' or 'dog'. Regression outputs continuous values like height or weight. Models like decision trees and neural networks can be used for both depending on the task.",0.9,"Classification is a supervised learning task where the goal is to predict categorical labels or classes, while regression is a supervised learning task where the goal is to predict continuous numerical values."
What is clustering?,R7,"Clustering helps discover structure in a dataset by grouping similar entries. It doesn’t require pre-labeled data, making it useful for exploring unknown datasets. Each group formed is called a cluster.",0.95,Clustering is an unsupervised learning technique that involves grouping similar data points together based on their features or characteristics.
What is the curse of dimensionality?,R7,"It’s a concept in data science where increasing the number of features causes models to become complex and perform poorly, especially if the added features are not relevant or informative.",0.9,The curse of dimensionality refers to the phenomenon where the performance of certain algorithms deteriorates as the number of features or dimensions in the data increases.
What is the difference between precision and recall?,R7,"Precision is about being accurate when saying something is positive, and recall is about finding all the positives that exist. A good model should have both high precision and high recall.",0.9,"Precision measures the proportion of true positive predictions among all positive predictions made by a classifier, while recall measures the proportion of true positive predictions among all actual positive instances in the data."
What is the F1 score?,R7,"It gives a single metric to evaluate model performance by combining precision and recall. If either precision or recall is low, the F1 score will also be low, making it sensitive to both types of errors.",0.9,The F1 score is the harmonic mean of precision and recall and provides a single metric to evaluate the performance of a classifier.
What is the bias of an estimator?,R7,Estimator bias refers to the tendency of a model to consistently overestimate or underestimate a parameter. This systematic error leads to reduced prediction accuracy.,0.9,The bias of an estimator measures the difference between the expected value of the estimator and the true value of the parameter being estimated.
What is the variance of an estimator?,R7,"If an estimator has high variance, it means its predictions are inconsistent and might change significantly if you collect a new dataset. This can lead to overfitting.",0.9,The variance of an estimator measures the spread or variability of the estimator's values around its expected value.
What is the Central Limit Theorem?,R7,It’s a statistical concept where repeated sampling from any population results in the sample means being normally distributed. This helps in making predictions using standard statistical methods.,0.9,The Central Limit Theorem states that the sampling distribution of the sample mean approaches a normal distribution as the sample size increases.
What is regularization in neural networks?,R7,"By applying regularization, we penalize large weights in the network. This forces the model to keep the weights smaller and helps it avoid overfitting to the training set.",0.95,Regularization in neural networks involves adding penalty terms to the loss function to prevent overfitting.
What is batch normalization?,R7,It standardizes the output of a layer so that the values remain within a certain range. This makes optimization easier and leads to faster convergence.,0.9,Batch normalization is a technique used in neural networks to improve training stability and speed by normalizing the activations of each layer.
What is transfer learning?,R7,It helps models learn faster by starting with weights from a trained model instead of initializing randomly. This often leads to better performance with less training time on the new task.,0.9,Transfer learning is a machine learning technique where a model trained on one task is reused as the starting point for a model on a related task.
What is natural language processing (NLP)?,R7,"It is a subfield of AI focused on enabling machines to interact with humans in a natural way. NLP covers topics like question answering, language modeling, and summarization.",0.9,Natural language processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human languages.
What is word embedding?,R7,It allows words with similar meanings to have similar vector representations. This is useful in tasks like sentiment analysis and machine translation where word relationships matter.,0.9,Word embedding is a technique used to represent words as dense vectors in a continuous vector space.
What is sentiment analysis?,R7,"It allows machines to understand human emotions in written content. This is particularly useful for chatbots, support systems, and opinion mining tasks.",0.9,Sentiment analysis is a natural language processing task that involves determining the sentiment or opinion expressed in a piece of text.
What is deep learning?,R7,"It is a powerful approach that trains large neural networks using backpropagation. Deep learning is behind many applications like voice assistants, facial recognition, and translation systems.",0.95,Deep learning is a subset of machine learning that focuses on artificial neural networks with multiple layers.
What is a convolutional neural network (CNN)?,R7,"CNNs are deep learning architectures inspired by the visual cortex. They are composed of multiple layers including convolution, activation, and pooling layers to learn hierarchical features from image data.",0.95,"A convolutional neural network (CNN) is a type of deep neural network that is particularly well-suited for processing structured grid-like data, such as images."
What is a recurrent neural network (RNN)?,R7,"It handles inputs that come in sequences, such as words in a sentence. Unlike feedforward networks, RNNs can maintain memory of earlier inputs through their hidden state.",0.95,A recurrent neural network (RNN) is a type of neural network designed to process sequential data by maintaining a state or memory of previous inputs.
What is reinforcement learning?,R7,"It is based on the idea of learning from experience. The agent explores the environment, tries different actions, and improves its behavior based on the outcomes it receives.",0.9,Reinforcement learning is a machine learning paradigm where an agent learns to make decisions by interacting with an environment to maximize cumulative rewards.
What is Q-learning?,R7,"It’s an off-policy reinforcement learning algorithm, which means it learns the optimal policy independently of the actions taken by the agent during exploration.",0.95,Q-learning is a model-free reinforcement learning algorithm used to learn the optimal action-selection policy for a Markov decision process (MDP).
What is the exploration-exploitation tradeoff in reinforcement learning?,R7,It represents the balance between using what the agent already knows (exploitation) and learning something new (exploration). Techniques like ε-greedy are used to handle this tradeoff.,0.95,The exploration-exploitation tradeoff refers to the dilemma faced by reinforcement learning agents when deciding whether to explore new actions or exploit known actions to maximize rewards.
What is deep reinforcement learning?,R7,"It uses convolutional or recurrent neural networks to process raw data from the environment and learn optimal actions based on the feedback received, enabling autonomous learning in complex tasks.",0.95,Deep reinforcement learning combines reinforcement learning with deep learning techniques to handle high-dimensional input spaces and learn complex policies directly from raw sensory input.
What is a Markov decision process (MDP)?,R7,"It includes four elements: a set of states, a set of actions, a transition function, and a reward function. The goal is to find a policy that maximizes expected rewards.",0.95,A Markov decision process (MDP) is a mathematical framework used to model sequential decision-making problems in which an agent interacts with an environment.
What is cross-entropy loss?,R7,Cross-entropy loss is based on logarithmic calculations and works by comparing the log of the predicted probability with the true label. It's used in neural networks for classification tasks.,0.95,"Cross-entropy loss, also known as log loss, is a loss function used in classification tasks to quantify the difference between the predicted probability distribution and the true probability distribution of the classes."
What is the softmax function?,R7,It’s a mathematical function that takes a vector of numbers and scales them into a range between 0 and 1 while preserving their relative magnitudes.,0.9,The softmax function is a mathematical function that converts a vector of arbitrary real values into a vector of probabilities.
What is the difference between a generative model and a discriminative model?,R7,"Discriminative models directly learn the mapping from input to output, while generative models learn the underlying structure of the data. Examples include Naive Bayes (generative) and Logistic Regression (discriminative).",0.95,"A generative model learns the joint probability distribution of the input features and the target labels, while a discriminative model learns the conditional probability distribution of the target labels given the input features."
What is autoencoder?,R7,"It has two main parts: the encoder, which compresses the input data, and the decoder, which reconstructs it. Training is done by minimizing reconstruction error.",0.95,An autoencoder is a type of neural network architecture that is trained to reconstruct its input data at the output layer.
What is the difference between bagging and boosting?,R7,Boosting algorithms like AdaBoost and Gradient Boosting update model weights iteratively. Bagging methods like Random Forest aggregate outputs of independently trained trees.,0.95,"Bagging (Bootstrap Aggregating) is an ensemble method that builds multiple models independently and combines their predictions through averaging or voting, while boosting sequentially builds models by giving more weight to misclassified instances."
What is hyperparameter tuning?,R7,Some common hyperparameters include the number of trees in a random forest or the learning rate in gradient descent. Tuning helps achieve a balance between underfitting and overfitting.,0.95,Hyperparameter tuning is the process of finding the optimal hyperparameters for a machine learning model to improve its performance.
What is the curse of dimensionality in the context of feature selection?,R7,It describes the exponential increase in data required as the number of features grows. This sparsity can lead to overfitting and poor generalization in machine learning models.,0.95,"The curse of dimensionality in feature selection refers to the challenges and limitations posed by high-dimensional data, such as increased computational complexity and difficulty in finding meaningful features."
What is the difference between L1 and L2 regularization?,R7,L1 tends to perform better when we believe only a few features are relevant. L2 performs better when most features contribute a little to the output.,0.9,"L1 regularization (Lasso) adds a penalty term proportional to the absolute value of the coefficients, encouraging sparsity and feature selection, while L2 regularization (Ridge) adds a penalty term proportional to the square of the coefficients, penalizing large coefficients."
What is the purpose of cross-validation in feature selection?,R7,"Cross-validation provides a reliable estimate of model performance for different feature combinations, helping select the set of features that gives consistent accuracy across folds.",0.9,"Cross-validation in feature selection is used to evaluate the performance of a model with different subsets of features, helping to identify the most informative features and reduce the risk of overfitting."
What is the role of bias in model evaluation?,R7,It plays a key role in diagnosing model performance. Evaluating bias helps determine whether we need a more complex model to reduce errors caused by underfitting.,0.95,"Bias in model evaluation refers to systematic errors or inaccuracies in the predictions made by a model, which may arise from simplifying assumptions or limitations in the model architecture."
What is the importance of interpretability in machine learning models?,R7,"It helps users, especially non-technical ones, to trust the model's predictions by offering explanations or visualizations of how decisions are made.",0.9,"Interpretability in machine learning models is important for understanding how predictions are made, gaining insights into the underlying relationships in the data, and building trust with stakeholders."
What is the difference between k-means and hierarchical clustering?,R7,"K-means requires the number of clusters as input, but hierarchical clustering doesn’t. It can determine the number of clusters based on the dendrogram cut-off.",0.9,"K-means clustering partitions the data into k clusters by minimizing the within-cluster sum of squares, while hierarchical clustering builds a tree-like hierarchy of clusters by recursively merging or splitting clusters based on proximity."
What is the Elbow Method used for in k-means clustering?,R7,It is a common approach in unsupervised learning to avoid selecting too many or too few clusters. The curve typically flattens after the ideal number is reached.,0.9,"The Elbow Method is used to determine the optimal number of clusters (k) in k-means clustering by plotting the within-cluster sum of squares against the number of clusters and identifying the ""elbow"" point where the rate of decrease in inertia slows down."
What is the role of regularization in reducing model complexity?,R7,"Regularization techniques like L1 and L2 force the model to prefer smaller weight values, simplifying the model and helping prevent overfitting on training data.",0.95,"Regularization in machine learning reduces model complexity by adding penalty terms to the loss function, encouraging simpler models with smaller coefficients or fewer features."
What is the difference between gradient descent and stochastic gradient descent?,R7,"SGD often converges faster on large datasets because it updates frequently, while traditional gradient descent may take longer as it waits for all data points per step.",0.9,"Gradient descent updates model parameters using the average gradient of the entire training dataset, while stochastic gradient descent updates parameters using the gradient of a single randomly selected data point or mini-batch."
What is the tradeoff between bias and variance in model selection?,R7,"Variance happens because of random noise in the data, and bias is caused by data mistakes. The tradeoff is to fix data errors to reduce both.",0.25,The bias-variance tradeoff in model selection involves balancing the tradeoff between bias (underfitting) and variance (overfitting) by selecting models with an appropriate level of complexity based on the dataset and task.
What is the role of regularization in reducing overfitting?,R7,Regularization controls overfitting by adding noise to the input data during training.,0.25,"Regularization in machine learning reduces overfitting by penalizing overly complex models, encouraging simpler models that generalize better to unseen data."
What are some techniques for handling imbalanced datasets in classification tasks?,R7,"Using synthetic data generation methods such as SMOTE creates new samples for the minority class, which helps improve model performance on imbalanced data.",0.95,"Techniques for handling imbalanced datasets include resampling methods such as oversampling the minority class or undersampling the majority class, using cost-sensitive learning algorithms, and generating synthetic samples with techniques like SMOTE."
What is the purpose of A/B testing in data science?,R7,It is used to experiment with different versions of websites or apps to improve user engagement or conversion rates.,0.95,"A/B testing is used to compare two or more versions of a product or intervention to determine which one performs better based on predefined metrics, such as conversion rate or user engagement."
What is the difference between correlation and causation?,R7,"Correlation means variables are linked but not necessarily because of cause, while causation means there is a direct cause-effect relationship.",0.95,"Correlation measures the strength and direction of the linear relationship between two variables, while causation indicates that changes in one variable directly cause changes in another variable."
What is the role of feature scaling in machine learning?,R7,Scaling features helps in faster convergence of optimization algorithms by keeping all input features within similar ranges.,0.95,"Feature scaling in machine learning ensures that all features have the same scale or range, preventing certain features from dominating others during model training. Common scaling techniques include min-max scaling and standardization."
What is the difference between classification and clustering?,R7,Classification is a type of clustering that uses known categories to sort data.,0.3,"Classification is a supervised learning task where the goal is to predict categorical labels or classes, while clustering is an unsupervised learning task where the goal is to group similar data points together based on their features."
What is the purpose of Principal Component Analysis (PCA) in dimensionality reduction?,R7,"PCA reduces dimensionality by finding directions (principal components) that maximize variance, helping to remove noise and redundancy.",0.95,"Principal Component Analysis (PCA) is used to reduce the dimensionality of a dataset by transforming the original features into a new set of orthogonal principal components, which capture the maximum variance in the data."
What is the role of regularization in neural networks?,R7,"It randomly drops some neurons during training to reduce overfitting and improve generalization, known as dropout.",0.9,"Regularization in neural networks helps prevent overfitting by adding penalty terms to the loss function, encouraging simpler models with smaller weights or fewer connections."
What is dropout regularization in neural networks?,R7,"It randomly removes some neurons during training, which helps the network avoid overfitting and improves performance on new data.",0.9,"Dropout regularization is a technique used in neural networks to reduce overfitting by randomly dropping out a fraction of neurons during training, forcing the network to learn more robust representations."
What is the difference between a generative adversarial network (GAN) and a variational autoencoder (VAE)?,R7,"A VAE encodes inputs into a latent space and decodes them back to reconstruct data, while GANs directly generate data from noise without encoding.",0.9,"Generative adversarial networks (GANs) consist of two neural networks (generator and discriminator) that compete with each other to generate realistic data samples, while variational autoencoders (VAEs) learn a probabilistic model of the data's latent space and generate new samples by sampling from this distribution."
What is the difference between batch gradient descent and mini-batch gradient descent?,R7,"Batch gradient descent updates weights after one epoch, while mini-batch updates weights after each mini batch within an epoch.",0.95,"Batch gradient descent computes the gradient of the cost function using the entire training dataset, while mini-batch gradient descent computes the gradient using a randomly selected subset (mini-batch) of the training data."
What is the role of activation functions in neural networks?,R7,Activation functions make sure the output of neurons stays within certain limits to avoid overflow errors.,0.3,"Activation functions introduce non-linearity into neural networks, allowing them to learn complex mappings between inputs and outputs. Common activation functions include sigmoid, tanh, ReLU, and softmax."
What is the difference between generative and discriminative models in machine learning?,R7,"Generative models are mainly used for clustering, whereas discriminative models are used for regression.",0.15,"Generative models learn the joint probability distribution of the input features and the target labels, while discriminative models learn the conditional probability distribution of the target labels given the input features."
What is the purpose of dropout regularization in neural networks?,R7,It helps reduce overfitting by forcing the network to learn redundant representations through random neuron deactivation.,0.9,"Dropout regularization is a technique used in neural networks to prevent overfitting by randomly dropping out a fraction of neurons during training, forcing the network to learn more robust representations."
What is the role of LSTMs in sequence modeling tasks?,R7,They are a type of recurrent neural network that can learn long-range dependencies by controlling which information to keep or forget in sequences.,0.9,"Long Short-Term Memory (LSTM) networks are a type of recurrent neural network (RNN) designed to capture long-range dependencies and temporal dynamics in sequential data, making them well-suited for tasks such as language modeling and time series prediction."
What is the difference between precision and recall in classification tasks?,R7,"High precision means most predicted positives are correct, while high recall means most actual positives are found.",1.0,"Precision measures the proportion of true positive predictions among all positive predictions made by a classifier, while recall measures the proportion of true positive predictions among all actual positive instances in the data."
"What is the F1 score, and why is it useful in model evaluation?",R7,"It is a way to summarize the model’s performance by combining precision and recall, useful when dealing with imbalanced data.",0.95,"The F1 score is the harmonic mean of precision and recall and provides a single metric to evaluate the performance of a classifier, balancing both precision and recall. It is particularly useful for imbalanced datasets where one class is much more prevalent than the others."
What is the purpose of word embedding in natural language processing (NLP)?,R7,"Word embeddings aim to assign a unique, random numerical identifier to each word without any consideration for its meaning or relationship to other words. This arbitrary assignment helps in distinguishing one word from another but does not facilitate any form of semantic analysis or contextual understanding. They are essentially just labels in a numerical format, providing no inherent information about the word's linguistic properties or its role in a sentence.",0.05,"Word embedding is a technique used to represent words as dense vectors in a continuous vector space, capturing semantic relationships between words and enabling neural networks to process textual data more effectively."
What is the difference between word2vec and GloVe in word embedding?,R7,"GloVe is generally slower than Word2Vec because it has to process the entire document before starting training, while Word2Vec learns on the fly.",0.85,"Word2Vec is a model that learns word embeddings by predicting the surrounding words in a text corpus, while GloVe (Global Vectors for Word Representation) learns word embeddings by factorizing the co-occurrence matrix of words in the corpus."
What is the purpose of sentiment analysis in natural language processing (NLP)?,R7,"The main purpose is to find out how people feel about a topic, product, or service by examining their written text.",0.95,"Sentiment analysis is a natural language processing task that involves determining the sentiment or opinion expressed in a piece of text, such as positive, negative, or neutral. It is commonly used to analyze customer reviews, social media posts, and survey responses."
What is the difference between shallow learning and deep learning?,R7,"Shallow learning models, like logistic regression, are simpler and easier to interpret than deep models like CNNs or RNNs.",0.9,"Shallow learning refers to traditional machine learning algorithms with a small number of layers or features, while deep learning involves neural networks with multiple layers that can learn hierarchical representations of the data."
What is the role of dropout regularization in neural networks?,R7,"By applying dropout, the model becomes less dependent on specific neurons, which helps improve validation accuracy.",0.9,"Dropout regularization is a technique used in neural networks to prevent overfitting by randomly dropping out a fraction of neurons during training, forcing the network to learn more robust representations."
What is the difference between precision and accuracy in model evaluation?,R7,"Precision is important when false positives are costly, whereas accuracy can be misleading when classes are imbalanced.",0.9,"Precision measures the proportion of true positive predictions among all positive predictions made by a classifier, while accuracy measures the proportion of correct predictions (both true positives and true negatives) among all predictions made by the classifier."
What is the difference between k-fold cross-validation and leave-one-out cross-validation?,R7,"Leave-one-out cross-validation is very accurate but computationally expensive, especially for large datasets.",0.9,"K-fold cross-validation divides the data into k subsets and iteratively trains the model k times, each time using a different subset as the validation set, while leave-one-out cross-validation trains the model k times, each time using all but one data point as the training set and the remaining point as the validation set."
What is the Naive Bayes classifier and how does it work?,R7,Naive Bayes works well on text classification problems like spam detection because it calculates the likelihood of words given a class.,0.95,"The Naive Bayes classifier is a probabilistic model based on Bayes' theorem with the ""naive"" assumption of feature independence. It calculates the probability of a class given the input features using the product of conditional probabilities."
What is the difference between correlation and covariance?,R7,"Covariance and correlation both measure linear relationships, but only correlation can detect non-linear patterns.",0.3,"Covariance measures the degree to which two variables change together, while correlation measures both the strength and direction of the linear relationship between two variables."
What is the purpose of a confusion matrix in classification tasks?,R7,The confusion matrix helps identify which classes the model is confusing with each other.,0.9,"A confusion matrix is a table that visualizes the performance of a classification model by comparing predicted class labels with true class labels. It shows the number of true positives, true negatives, false positives, and false negatives."
What is the difference between stratified sampling and random sampling?,R7,"Stratified sampling helps maintain proportional representation of classes, which is especially important for imbalanced datasets.",0.95,"Stratified sampling divides the population into homogeneous subgroups (strata) and samples from each stratum proportionally, while random sampling selects individuals randomly from the entire population without regard to strata."
"What is the difference between univariate, bivariate, and multivariate analysis?",R7,"Univariate analysis studies one variable at a time, bivariate focuses on how two variables relate, and multivariate looks at interactions among three or more variables.",0.95,"Univariate analysis examines the distribution and summary statistics of a single variable, bivariate analysis explores the relationship between two variables, and multivariate analysis involves analyzing multiple variables simultaneously."
What is the purpose of data preprocessing in machine learning?,R7,Data preprocessing is not needed if the model is strong enough to handle raw data directly.,0.2,"Data preprocessing involves cleaning, transforming, and preparing raw data for machine learning algorithms. It includes tasks such as handling missing values, encoding categorical variables, and scaling features."
What is the bias of a statistical estimator?,R7,High bias in an estimator means it consistently underestimates or overestimates the true value.,0.9,The bias of a statistical estimator measures the difference between the expected value of the estimator and the true value of the parameter being estimated. A biased estimator systematically overestimates or underestimates the true parameter value.
What is the variance of a statistical estimator?,R7,"Low variance means the estimator produces similar results across samples, which is desirable.",0.9,"The variance of a statistical estimator measures the spread or variability of the estimator's values around its expected value. A high variance indicates that the estimator's values are scattered widely, while a low variance indicates that the values are clustered closely together."
What is the difference between parametric and non-parametric statistical tests?,R7,Parametric tests assume data comes from a specific distribution; non-parametric tests work without that assumption.,0.95,"Parametric statistical tests assume that the data follows a specific probability distribution (e.g., normal distribution), while non-parametric tests make no such assumptions and are based on ranks or frequencies."
What is the purpose of feature scaling in machine learning?,R7,"Scaling is important because models assume all features are on the same scale, especially in optimization and convergence speed.",0.95,"Feature scaling ensures that all features have the same scale or range, preventing certain features from dominating others during model training. Common scaling techniques include min-max scaling and standardization."
What is the difference between correlation and causation in statistics?,R7,"Causation means changes in one variable lead to changes in another, while correlation only shows they happen together.",0.95,"Correlation measures the strength and direction of the linear relationship between two variables, while causation indicates that changes in one variable directly cause changes in another variable."
What is the Central Limit Theorem and why is it important?,R7,"According to the CLT, we can assume the distribution of sample means becomes approximately normal when the sample size is large enough, typically n > 30.",0.95,"The Central Limit Theorem states that the sampling distribution of the sample mean approaches a normal distribution as the sample size increases, regardless of the shape of the population distribution. It is important because it allows us to make inferences about population parameters based on sample statistics."
What is the purpose of outlier detection in data analysis?,R7,Detecting outliers helps improve the quality of insights and prevents misleading conclusions in models.,0.95,"Outlier detection involves identifying observations that deviate significantly from the rest of the data. Outliers can distort statistical analyses and machine learning models, so detecting and handling them appropriately is important for accurate results."
What is the difference between supervised and unsupervised outlier detection methods?,R7,"Supervised detection is like classification, while unsupervised methods are more exploratory and rely on the assumption that outliers are rare and different.",0.9,"Supervised outlier detection methods require labeled data with outlier labels, while unsupervised methods detect outliers based on the characteristics of the data itself without requiring labeled examples."
What is the difference between statistical inference and predictive modeling?,R7,"Statistical inference is used when we want to draw conclusions about the population, whereas predictive modeling is more about getting good results regardless of the cause.",0.95,"Statistical inference involves drawing conclusions about a population based on a sample of data, while predictive modeling focuses on building models that make predictions about future or unseen data based on past observations."
What is the purpose of regularization in machine learning?,R7,The main goal of regularization is to reduce the model's tendency to overfit the training data and improve generalization.,0.95,"Regularization is a technique used to prevent overfitting by adding a penalty term to the model's objective function, discouraging overly complex models. Common regularization techniques include L1 regularization (Lasso) and L2 regularization (Ridge)."
What is the difference between batch gradient descent and stochastic gradient descent?,R7,"Batch gradient descent computes the average gradient over the whole dataset before updating, whereas SGD updates immediately after each example.",0.95,"Batch gradient descent updates model parameters using the average gradient of the entire training dataset, while stochastic gradient descent updates parameters using the gradient of a single randomly selected data point or mini-batch."
What is the difference between statistical modeling and machine learning?,R7,"Statistical models often aim to explain the data, while machine learning models aim to make the best possible predictions.",0.95,"Statistical modeling focuses on understanding the relationships between variables in a probabilistic framework, while machine learning emphasizes building algorithms that can learn patterns and make predictions from data without explicitly programming them."
What is the purpose of feature selection in machine learning?,R7,Feature selection can improve model accuracy and reduce training time by removing irrelevant inputs.,0.95,"Feature selection involves selecting the most relevant features from a dataset to improve model performance, reduce overfitting, and enhance interpretability."
What is the difference between regularization and feature selection in machine learning?,R7,Regularization modifies the learning algorithm to prevent overfitting; feature selection changes the input data to improve learning.,0.9,"Regularization penalizes overly complex models by adding a penalty term to the objective function, while feature selection explicitly selects a subset of features to include in the model based on their importance or relevance."
What is the difference between grid search and random search for hyperparameter tuning?,R7,Grid search is useful when the number of hyperparameters and values is small; random search is better for large spaces.,0.9,"Grid search exhaustively searches through a specified grid of hyperparameter values, while random search samples hyperparameter values randomly from predefined distributions."
What is the purpose of cross-validation in model evaluation?,R7,It is commonly used to compare model performance across different algorithms or hyperparameter settings.,0.9,Cross-validation is a technique used to assess the performance of a predictive model by splitting the data into multiple subsets. It helps to estimate the model's generalization error and detect overfitting.
What is the difference between precision and recall in classification evaluation?,R7,"Precision is useful when false positives are costly, while recall is more important when missing actual positives is a bigger problem.",0.9,"Precision measures the proportion of true positive predictions among all positive predictions made by a classifier, while recall measures the proportion of true positive predictions among all actual positive instances in the data."
"What is the F1 score, and why is it useful in imbalanced datasets?",R7,F1 score is calculated by taking the average of true positives and true negatives.,0.2,The F1 score is the harmonic mean of precision and recall and provides a single metric to evaluate the performance of a classifier. It is particularly useful for imbalanced datasets where one class is much more prevalent than the others.
What is the difference between bagging and boosting ensemble methods?,R7,"Bagging works best with high-variance models like decision trees, while boosting improves weak learners by correcting mistakes over iterations.",0.9,"Bagging (Bootstrap Aggregating) builds multiple models independently and combines their predictions through averaging or voting, while boosting sequentially builds models by giving more weight to misclassified instances."
What is the purpose of principal component analysis (PCA) in dimensionality reduction?,R7,PCA is mainly used to reduce overfitting and improve model speed by reducing data dimensionality.,0.9,"Principal Component Analysis (PCA) is used to reduce the dimensionality of a dataset by transforming the original features into a new set of orthogonal principal components, which capture the maximum variance in the data."
What is the difference between classification and regression in machine learning?,R7,"Classification maps input data to specific classes, while regression maps it to a range of values.",0.9,"Classification is a supervised learning task where the goal is to predict categorical labels or classes, while regression is a supervised learning task where the goal is to predict continuous numerical values."
What is the purpose of natural language processing (NLP) in data science?,R7,NLP allows us to convert large volumes of text into structured data that can be analyzed statistically.,0.9,"Natural language processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human languages. It enables computers to understand, interpret, and generate human language data."
What is the difference between bag-of-words and word embeddings in NLP?,R7,"Bag-of-words ignores grammar and context, while word embeddings preserve context by placing similar words close together in vector space.",0.95,"Bag-of-words represents text as a collection of word counts or frequencies, ignoring word order and grammar, while word embeddings represent words as dense vectors in a continuous vector space, capturing semantic relationships between words."
What is the difference between sentiment analysis and topic modeling in NLP?,R7,"Sentiment analysis is supervised, often using labeled data, while topic modeling is usually unsupervised and does not require labels.",0.95,"Sentiment analysis determines the sentiment or opinion expressed in a piece of text, such as positive, negative, or neutral, while topic modeling identifies the underlying topics or themes present in a collection of documents."
What is the purpose of word2vec in NLP?,R7,Word2Vec creates dense vectors that allow machine learning models to perform better on tasks like sentiment analysis and question answering.,0.9,Word2Vec is a model that learns word embeddings by predicting the surrounding words in a text corpus. It captures semantic relationships between words and enables algorithms to process textual data more effectively.
What is the difference between a generative model and a discriminative model in machine learning?,R7,"Generative models model how the data is generated, whereas discriminative models focus on decision boundaries between classes.",0.95,"A generative model learns the joint probability distribution of the input features and the target labels, while a discriminative model learns the conditional probability distribution of the target labels given the input features."
What is the difference between deep learning and traditional machine learning algorithms?,R7,"In deep learning, models learn hierarchical representations, while in traditional ML, the models are shallow and simpler.",0.95,"Deep learning involves neural networks with multiple layers that can learn hierarchical representations of the data, while traditional machine learning algorithms typically use handcrafted features and simpler models."
What is the purpose of transfer learning in deep learning?,R7,Transfer learning reduces the need for large datasets by leveraging pre-learned features from similar tasks.,0.95,"Transfer learning is a technique where a model trained on one task is reused as the starting point for a model on a related task. It enables the transfer of knowledge and features learned from one domain to another, reducing the need for large labeled datasets."
What is the difference between convolutional neural networks (CNNs) and recurrent neural networks (RNNs)?,R7,"CNNs are used for tasks like image classification, whereas RNNs are better suited for tasks like language modeling or speech recognition.",0.95,"Convolutional neural networks (CNNs) are well-suited for processing grid-like data such as images, while recurrent neural networks (RNNs) are designed to handle sequential data with temporal dependencies such as text or time series."
What is the purpose of batch normalization in neural networks?,R7,It acts like a regularizer and can reduce the need for dropout in some models.,0.9,Batch normalization is a technique used in neural networks to improve training stability and speed by normalizing the activations of each layer. It reduces internal covariate shift and accelerates convergence during training.
What is the difference between a feedforward neural network and a recurrent neural network?,R7,"RNNs maintain hidden states that carry temporal context, while feedforward networks don’t retain information across steps.",0.9,"A feedforward neural network consists of multiple layers of neurons connected in a forward direction without cycles, while a recurrent neural network has connections that form directed cycles, allowing it to maintain state or memory over time."
What is the difference between LSTMs and GRUs in recurrent neural networks?,R7,"LSTMs include an internal cell state that helps maintain long-term memory, which GRUs handle in a more compact form.",0.95,"LSTMs (Long Short-Term Memory) and GRUs (Gated Recurrent Units) are both types of recurrent neural network architectures designed to capture long-range dependencies in sequential data, but they differ in their internal gating mechanisms and computational complexity."
What is the purpose of attention mechanisms in neural networks?,R7,It improves the model’s ability to handle long sequences by dynamically selecting which parts of the input are most relevant at each step.,0.9,Attention mechanisms in neural networks enable the model to focus on relevant parts of the input data while ignoring irrelevant information. They have been particularly successful in natural language processing tasks such as machine translation and text summarization.
What is the difference between generative adversarial networks (GANs) and autoencoders in deep learning?,R7,"GANs involve a competitive learning process between two models, but autoencoders use cooperative learning within a single model structure.",0.95,"Generative adversarial networks (GANs) consist of two neural networks (generator and discriminator) that compete with each other to generate realistic data samples, while autoencoders consist of an encoder and decoder network that reconstruct input data."
What is the purpose of reinforcement learning in machine learning?,R7,The goal of reinforcement learning is to find the best strategy or policy that gives the highest reward over time.,0.95,Reinforcement learning is a machine learning paradigm where an agent learns to make decisions by interacting with an environment to maximize cumulative rewards. It is commonly used in settings where actions influence future states and the agent learns through trial and error.
What is the difference between Q-learning and policy gradient methods in reinforcement learning?,R7,"Q-learning is off-policy and learns from a different policy than it behaves, while policy gradients are typically on-policy.",0.9,"Q-learning is a model-free reinforcement learning algorithm that learns the optimal action-selection policy by iteratively updating a Q-value function, while policy gradient methods directly parameterize the policy and update the policy parameters based on gradients of expected rewards."
What is the purpose of data preprocessing in exploratory data analysis (EDA)?,R7,"By normalizing and encoding data, preprocessing helps identify hidden patterns more effectively in EDA.",0.9,"Data preprocessing in EDA involves cleaning, transforming, and preparing raw data for analysis. It includes tasks such as handling missing values, removing outliers, and encoding categorical variables."
What are some common techniques for handling missing data in EDA?,R7,Interpolation can be used for time series data to estimate missing values based on nearby values.,0.95,"Common techniques for handling missing data in EDA include imputation (replacing missing values with estimates), deletion (removing rows or columns with missing values), and prediction (using models to estimate missing values)."
What is the difference between univariate and bivariate analysis in EDA?,R7,"In univariate, you might use histograms or box plots, while in bivariate, you might use scatter plots or correlation matrices.",0.95,"Univariate analysis examines the distribution and summary statistics of a single variable, while bivariate analysis explores the relationship between two variables."
What is the purpose of data visualization in EDA?,R7,"Using plots like histograms, boxplots, and scatter plots helps summarize data distribution and relationships.",0.95,"Data visualization in EDA helps to explore and understand the structure, patterns, and relationships in the data. It provides insights that may not be apparent from raw data or summary statistics alone."
What are some common types of data visualization used in EDA?,R7,"Heatmaps help visualize correlations between variables, making them useful for bivariate analysis.",0.95,"Common types of data visualization used in EDA include histograms, scatter plots, box plots, bar charts, line plots, and heatmaps."
What is the difference between continuous and categorical variables in EDA?,R7,"Categorical variables are qualitative and describe characteristics, while continuous variables are quantitative and represent measurable data.",0.95,"Continuous variables are numeric variables with an infinite number of possible values within a range, while categorical variables are qualitative variables with a finite number of distinct categories or levels."
What is the purpose of statistical summaries in EDA?,R7,"With summaries like min, max, and quartiles, we can spot inconsistencies or unusual patterns in data.",0.9,"Statistical summaries in EDA provide descriptive statistics such as mean, median, mode, variance, and percentiles to summarize the central tendency, dispersion, and shape of the data distribution."
What is the difference between central tendency and dispersion measures in EDA?,R7,"While central tendency focuses on typical values, dispersion measures like variance and IQR explain the consistency of the data.",0.9,"Central tendency measures (e.g., mean, median, mode) describe the central or typical value of a dataset, while dispersion measures (e.g., variance, standard deviation) quantify the spread or variability of the data around the central value."
What is the purpose of hypothesis testing in EDA?,R7,It helps verify whether there is enough evidence in the data to support or reject a given assumption.,0.95,Hypothesis testing in EDA is used to evaluate whether observed differences or relationships in the data are statistically significant or due to random chance. It involves formulating null and alternative hypotheses and conducting statistical tests to make inferences about population parameters.
What is the difference between parametric and non-parametric tests in hypothesis testing?,R7,"T-tests and ANOVA are examples of parametric tests, while Mann-Whitney and Kruskal-Wallis are examples of non-parametric tests.",0.95,"Parametric tests assume that the data follows a specific probability distribution (e.g., normal distribution), while non-parametric tests make no such assumptions and are based on ranks or frequencies."
What is the purpose of correlation analysis in EDA?,R7,"By analyzing correlations, we can prioritize which features might be more informative for prediction tasks.",0.95,Correlation analysis in EDA measures the strength and direction of the linear relationship between two continuous variables. It helps to identify patterns and associations in the data.
What is the difference between Pearson correlation and Spearman correlation?,R7,"Spearman uses ranks, which makes it suitable for ordinal data, while Pearson is for continuous numeric data.",0.95,"Pearson correlation measures the linear relationship between two continuous variables, while Spearman correlation measures the monotonic relationship between variables, allowing for non-linear associations."
What is the purpose of regression analysis in EDA?,R7,"By using regression early, analysts can decide if linear relationships exist between variables.",0.9,Regression analysis in EDA is used to model the relationship between one or more independent variables (predictors) and a dependent variable (outcome). It helps to understand how changes in predictors affect the outcome variable.
What are the assumptions of linear regression in EDA?,R7,There should be no or little multicollinearity between the independent variables.,0.9,"The assumptions of linear regression in EDA include linearity (relationship between variables is linear), independence (residuals are independent), homoscedasticity (residuals have constant variance), and normality (residuals are normally distributed)."
What is the difference between simple linear regression and multiple linear regression?,R7,"The main difference is the number of inputs. Simple regression takes just one input to find a trend line, while multiple regression looks at how several features together affect the result.",0.9,"Simple linear regression models the relationship between one independent variable and a dependent variable, while multiple linear regression models the relationship between multiple independent variables and a dependent variable."
What is the purpose of logistic regression in EDA?,R7,"Using logistic regression in EDA can help identify predictors that have strong associations with outcomes like disease presence or absence, aiding in feature selection.",0.95,Logistic regression in EDA is used to model the relationship between one or more independent variables (predictors) and a binary outcome variable (0 or 1). It is commonly used for classification tasks.
What are the assumptions of logistic regression in EDA?,R7,"It assumes that each observation is independent of others, meaning there should be no autocorrelation — especially important in time-based data.",0.9,"The assumptions of logistic regression in EDA include linearity (relationship between predictors and log-odds is linear), independence (observations are independent), and absence of multicollinearity (predictors are not highly correlated)."
What is the purpose of outlier detection in EDA?,R7,"By identifying outliers, we can understand the data distribution better and decide if the data needs normalization or transformation.",0.9,"Outlier detection in EDA involves identifying observations that deviate significantly from the rest of the data. Outliers can distort statistical analyses and machine learning models, so detecting and handling them appropriately is important for accurate results."
What are some common techniques for detecting outliers in EDA?,R7,"DBSCAN is a clustering-based method that can detect outliers as points that don't belong to any cluster, though it’s not always used in EDA.",0.9,"Common techniques for detecting outliers in EDA include graphical methods (e.g., scatter plots, box plots), statistical methods (e.g., z-score, modified z-score), and proximity-based methods (e.g., nearest neighbors)."
What is the purpose of box plots in EDA?,R7,"One purpose of a box plot is to detect outliers, as values that fall outside the whiskers are likely anomalies or extreme observations.",0.95,"Box plots in EDA provide a visual summary of the distribution, central tendency, and spread of a dataset. They show the median, quartiles, and potential outliers of the data distribution."
What is the purpose of histograms in EDA?,R7,One purpose of a histogram is to determine whether the data is symmetric or skewed. This helps decide if transformations are needed.,0.9,Histograms in EDA display the frequency distribution of a continuous variable by dividing the data into intervals (bins) and plotting the number of observations in each interval. They provide insights into the shape and spread of the data distribution.
What is the purpose of scatter plots in EDA?,R7,They are useful in EDA to identify correlations between features and see if any obvious trends or groupings are present in the dataset.,0.95,"Scatter plots in EDA visualize the relationship between two continuous variables by plotting each data point as a point on a graph. They help to identify patterns, trends, and correlations in the data."
What is the purpose of bar charts in EDA?,R7,Bar charts give a quick overview of category-wise distribution and make it easier to identify data imbalances before modeling.,0.95,Bar charts in EDA represent categorical data by displaying the frequency or proportion of observations in each category as bars of varying heights. They are useful for comparing the distribution of categorical variables.
What is the purpose of line plots in EDA?,R7,"They allow you to see continuous changes in one or more variables, making it easier to understand behavior over time or ordered sequences.",0.95,Line plots in EDA visualize the trend or pattern of a variable over time or another continuous dimension by connecting data points with lines. They are commonly used for time series analysis and trend identification.
What is the purpose of heatmaps in EDA?,R7,They are useful for understanding how features relate to each other and for identifying redundant features that may affect the model.,0.95,Heatmaps in EDA visualize the magnitude of a variable or the strength of a relationship between variables using colors. They are useful for identifying patterns and correlations in large datasets.
What is the purpose of correlation matrices in EDA?,R7,Correlation matrices visualize the linear relationships between variables using numerical coefficients and often a heatmap for easy interpretation.,0.95,Correlation matrices in EDA display the pairwise correlations between variables in a tabular format or heatmap. They help to identify patterns and associations among variables.
What is the difference between correlation and causation in EDA?,R7,"EDA often finds correlations that might suggest relationships, but proving causation requires additional analysis or domain knowledge.",0.95,"Correlation measures the strength and direction of the linear relationship between two variables, while causation indicates that changes in one variable directly cause changes in another variable. Correlation does not imply causation."
What is the purpose of statistical tests in EDA?,R7,Statistical tests like t-tests or chi-square tests can identify relationships between variables or differences between groups that might not be obvious from visualizations alone.,0.9,Statistical tests in EDA are used to assess the significance of observed differences or relationships in the data. They help to make inferences about population parameters and draw conclusions from sample data.
What is the difference between parametric and non-parametric tests in EDA?,R7,"In EDA, parametric tests like the t-test assume data characteristics, while non-parametric tests like Mann-Whitney are used when data is not normally distributed.",0.95,"Parametric tests assume that the data follows a specific probability distribution (e.g., normal distribution), while non-parametric tests make no such assumptions and are based on ranks or frequencies."
What is the purpose of ANOVA in EDA?,R7,ANOVA can be helpful during EDA to evaluate whether a categorical independent variable affects a continuous dependent variable.,0.95,Analysis of Variance (ANOVA) in EDA is used to compare the means of two or more groups to determine whether they are significantly different. It is commonly used for comparing means across multiple categories or treatments.
What is the difference between one-way ANOVA and two-way ANOVA in EDA?,R7,One-way ANOVA looks at variation within groups and between groups for one factor. Two-way ANOVA adds interaction effects between two factors.,0.95,"One-way ANOVA compares the means of two or more groups based on a single categorical factor, while two-way ANOVA considers the effects of two categorical factors (main effects and interaction) on the outcome variable."
What is the purpose of chi-squared test in EDA?,R7,The test helps decide if the distribution of categories differs significantly from what would be expected by chance.,0.95,The chi-squared test in EDA is used to assess the independence between two categorical variables by comparing the observed frequencies to the expected frequencies under the null hypothesis of independence.
What is the purpose of t-tests in EDA?,R7,They allow analysts to test hypotheses about the equality of means for two independent samples or paired samples in EDA.,0.95,T-tests in EDA are used to compare the means of two independent samples or the mean of a sample to a known value. They help to determine whether observed differences in means are statistically significant.
What is the difference between independent samples t-test and paired samples t-test in EDA?,R7,"The paired t-test controls for individual variability by comparing data points within subjects, whereas independent t-test assumes observations are independent.",0.95,"Independent samples t-test compares the means of two independent groups, while paired samples t-test compares the means of two related groups (e.g., before and after treatment) based on matched pairs of observations."
What is the purpose of non-parametric tests in EDA?,R7,"In EDA, they help analyze ranked or categorical data and are robust to outliers and skewed data.",0.95,"Non-parametric tests in EDA are used when the data does not meet the assumptions of parametric tests (e.g., normal distribution, homogeneity of variance). They provide robust alternatives for analyzing non-normally distributed or skewed data."
What is the difference between Wilcoxon signed-rank test and Mann-Whitney U test in EDA?,R7,"Mann-Whitney U test requires data to be normally distributed, but Wilcoxon signed-rank test does not.",0.2,"Wilcoxon signed-rank test is a non-parametric test used to compare the medians of two related groups, while Mann-Whitney U test compares the medians of two independent groups."
What is the purpose of exploratory factor analysis (EFA) in EDA?,R7,Exploratory Factor Analysis creates visualizations like scatter plots and histograms to show data distribution.,0.2,Exploratory factor analysis (EFA) in EDA is used to identify underlying factors or latent variables that explain the common variance among observed variables. It helps to uncover the underlying structure of a dataset.
What is the difference between EFA and principal component analysis (PCA) in EDA?,R7,"PCA is mainly used for categorical data, and EFA is for numerical data in exploratory data analysis.",0.1,"EFA and PCA are both dimensionality reduction techniques used in EDA, but EFA aims to identify underlying factors that explain the common variance among observed variables, while PCA aims to capture the maximum variance in the data with orthogonal principal components."
What is the purpose of cluster analysis in EDA?,R7,Cluster analysis predicts the target variable based on groups it forms from the data.,0.1,Cluster analysis in EDA is used to identify groups or clusters of similar observations based on their characteristics or features. It helps to uncover natural groupings in the data and identify patterns or relationships.
What are some common algorithms used for cluster analysis in EDA?,R7,Cluster analysis uses decision trees and random forests to group similar data points.,0.1,"Common algorithms used for cluster analysis in EDA include K-means clustering, hierarchical clustering, DBSCAN (Density-Based Spatial Clustering of Applications with Noise), and Gaussian mixture models."
What is the difference between K-means clustering and hierarchical clustering in EDA?,R7,Hierarchical clustering always outperforms K-means in accuracy for all datasets.,0.2,"K-means clustering partitions the data into K clusters by minimizing the within-cluster sum of squares, while hierarchical clustering builds a tree-like hierarchy of clusters by recursively merging or splitting clusters based on proximity."
What is the purpose of dendrogram in hierarchical clustering in EDA?,R7,"In EDA, dendrograms assist in understanding the data structure and selecting meaningful clusters.",0.9,A dendrogram in hierarchical clustering visualizes the hierarchical relationships between clusters by displaying the merging order and distance between clusters. It helps to identify the optimal number of clusters and interpret the clustering results.
What is the purpose of silhouette analysis in cluster validation in EDA?,R7,Silhouette analysis helps compare different clustering algorithms to select the best one for the dataset.,0.9,"Silhouette analysis in cluster validation assesses the quality of clustering results by calculating silhouette coefficients for each data point, which measure the cohesion within clusters and separation between clusters. Higher silhouette coefficients indicate better-defined clusters."
What is the purpose of dimensionality reduction techniques in EDA?,R7,They help visualize high-dimensional data in 2D or 3D plots for better understanding during exploratory analysis.,0.95,Dimensionality reduction techniques in EDA are used to reduce the number of features or variables in a dataset while preserving the most important information. They help to overcome the curse of dimensionality and improve computational efficiency.
What are some common dimensionality reduction techniques used in EDA?,R7,Random Forest can be used to reduce features by selecting the most important ones.,0.4,"Common dimensionality reduction techniques used in EDA include principal component analysis (PCA), factor analysis, t-distributed stochastic neighbor embedding (t-SNE), and linear discriminant analysis (LDA)."
What is the purpose of PCA in EDA?,R7,PCA is a clustering algorithm used to group similar observations in EDA.,0.1,Principal Component Analysis (PCA) in EDA is used to reduce the dimensionality of a dataset by transforming the original features into a new set of orthogonal principal components. It helps to identify the underlying structure and patterns in high-dimensional data.
What is the difference between PCA and factor analysis in EDA?,R7,Both PCA and factor analysis always produce identical results and can be used interchangeably in EDA.,0.1,"PCA and factor analysis are both dimensionality reduction techniques used in EDA, but PCA aims to capture the maximum variance in the data with orthogonal principal components, while factor analysis aims to identify underlying factors that explain the common variance among observed variables."
What is the purpose of t-SNE in EDA?,R7,It is used to explore the structure of data visually and understand groupings or anomalies in exploratory analysis.,0.9,t-Distributed Stochastic Neighbor Embedding (t-SNE) in EDA is a technique used for visualizing high-dimensional data in a lower-dimensional space while preserving the local structure and relationships between data points. It is commonly used for visualizing clusters and patterns in complex datasets.
What is the difference between linear discriminant analysis (LDA) and PCA in EDA?,R7,"PCA is a nonlinear method, while LDA is linear.",0.1,"Linear Discriminant Analysis (LDA) and PCA are both dimensionality reduction techniques used in EDA, but LDA aims to maximize the separation between classes or groups while reducing dimensionality, whereas PCA focuses on capturing the maximum variance in the data regardless of class labels."
What is the purpose of feature selection in EDA?,R7,It can improve the accuracy of predictive models by removing features that add noise or are correlated.,0.9,"Feature selection in EDA involves selecting the most relevant features from a dataset to improve model performance, reduce overfitting, and enhance interpretability. It helps to identify the subset of features that contribute the most to predicting the target variable."
What are some common feature selection techniques used in EDA?,R7,Decision trees can be used for feature selection by evaluating feature importance during training.,0.9,"Common feature selection techniques used in EDA include filter methods (e.g., correlation analysis, information gain), wrapper methods (e.g., recursive feature elimination, forward/backward selection), and embedded methods (e.g., LASSO, decision trees)."
What is the purpose of ensemble methods in EDA?,R7,They reduce bias and variance by blending multiple weak learners into a stronger model.,0.95,"Ensemble methods in EDA combine multiple models or algorithms to improve predictive performance, robustness, and generalization. They help to reduce overfitting and capture diverse patterns in the data."
What are some common ensemble methods used in EDA?,R7,Bagging involves averaging predictions of multiple neural networks trained on the entire dataset.,0.2,"Common ensemble methods used in EDA include bagging (Bootstrap Aggregating), boosting (e.g., AdaBoost, Gradient Boosting), and stacking (meta-learning). They combine predictions from multiple base models to make more accurate and reliable predictions."
What is the purpose of model evaluation in EDA?,R7,The purpose is to reduce data dimensionality by selecting only important features for the model.,0.1,Model evaluation in EDA involves assessing the performance of predictive models using various metrics and techniques. It helps to determine the effectiveness of the models in capturing patterns and making accurate predictions on unseen data.
What are some common metrics used for model evaluation in EDA?,R7,Accuracy is the only metric needed to evaluate any type of model performance.,0.2,"Common metrics used for model evaluation in EDA include accuracy, precision, recall, F1 score, ROC-AUC score, mean squared error (MSE), and R-squared (R2) score. They provide insights into different aspects of model performance."
What is the difference between cross-validation and holdout validation in model evaluation in EDA?,R7,"Cross-validation is mainly used when there's a lot of data, and holdout is used when data is limited.",0.3,"Cross-validation in model evaluation splits the data into multiple subsets (folds) for training and testing, while holdout validation splits the data into two disjoint sets (training and testing). Cross-validation provides more reliable estimates of model performance but is computationally more expensive."
What is the purpose of hyperparameter tuning in model evaluation in EDA?,R7,Tuning hyperparameters ensures the model doesn't underfit or overfit the training data.,0.9,Hyperparameter tuning in model evaluation involves optimizing the hyperparameters of machine learning algorithms to improve model performance and generalization. It helps to find the best combination of hyperparameters for a given dataset and problem.
What are some common techniques for hyperparameter tuning in EDA?,R7,Using validation curves and learning curves are helpful in identifying the best hyperparameter values.,0.85,"Common techniques for hyperparameter tuning in EDA include grid search, random search, Bayesian optimization, and genetic algorithms. They systematically search the hyperparameter space to find the optimal values for model performance."
What is the purpose of model interpretation in EDA?,R7,Interpretation is mostly used to fix data errors by comparing the model output to expected results during the early stages of EDA.,0.3,"Model interpretation in EDA involves understanding and explaining how predictive models make decisions and predictions. It helps to gain insights into the underlying patterns, relationships, and features that contribute to the model's predictions."
What are some common techniques for model interpretation in EDA?,R7,Decision trees themselves are interpretable models because we can directly follow the path of splits to understand predictions.,0.85,"Common techniques for model interpretation in EDA include feature importance analysis (e.g., permutation importance, SHAP values), partial dependence plots, individual conditional expectation (ICE) plots, and model-agnostic interpretation methods (e.g., LIME, SHAP)."
What is the purpose of model deployment in EDA?,R7,"In EDA, we deploy models to quickly test different visualizations and store the best performing plots for future use.",0.2,Model deployment in EDA involves deploying predictive models into production environments to make real-time predictions on new data. It allows organizations to leverage the insights and predictions generated from data analysis for decision-making and business applications.
What are some common techniques for model deployment in EDA?,R7,"AutoML tools handle both EDA and deployment, making it easier to push models from exploration to production automatically.",0.8,"Common techniques for model deployment in EDA include deploying models as web services or APIs, embedding models into applications or dashboards, and integrating models with existing business processes or workflows. They enable seamless integration of predictive models into operational systems and workflows."
What are the common types of regularization used in machine learning?,R7,Regularization techniques like Cross-Validation and PCA are often used to tune models and reduce errors caused by training on limited data.,0.25,"Common types of regularization used in machine learning include L1 regularization (Lasso), L2 regularization (Ridge), and elastic net regularization (combination of L1 and L2 penalties)."
What is the bias-variance trade-off in machine learning?,R7,We increase bias deliberately to improve training accuracy and reduce variance for testing accuracy.,0.3,"The bias-variance trade-off refers to the balance between bias (error due to overly simplistic assumptions) and variance (error due to sensitivity to fluctuations in the training data) in machine learning models. Reducing bias typically increases variance, and vice versa."
What are the common types of cross-validation techniques used in machine learning?,R7,"Stratified k-fold ensures each fold has a similar distribution of the target variable, especially helpful for imbalanced datasets.",0.95,"Common types of cross-validation techniques used in machine learning include k-fold cross-validation, stratified k-fold cross-validation, leave-one-out cross-validation, and nested cross-validation."
What is the purpose of hyperparameter tuning in machine learning?,R7,Hyperparameter tuning is important because the right values can significantly improve both accuracy and generalization.,0.95,Hyperparameter tuning in machine learning involves optimizing the hyperparameters of models to improve their performance and generalization. It helps to find the best combination of hyperparameters for a given dataset and problem.
What are some common techniques for hyperparameter tuning in machine learning?,R7,"You can manually tune hyperparameters or use automated techniques like Grid Search, Random Search, and Hyperband.",0.9,"Common techniques for hyperparameter tuning in machine learning include grid search, random search, Bayesian optimization, genetic algorithms, and gradient-based optimization methods. They systematically search the hyperparameter space to find optimal values."
What is the difference between supervised and unsupervised learning in machine learning?,R7,"The key difference is labeled data: supervised learning requires it, while unsupervised learning doesn’t.",1.0,"Supervised learning involves learning from labeled data with input-output pairs, where the model predicts the output variable based on input features. Unsupervised learning involves learning from unlabeled data to discover hidden patterns or structures in the data."
What are some common dimensionality reduction techniques used in machine learning?,R7,t-SNE is helpful for visualizing high-dimensional data by projecting it into 2D or 3D spaces.,0.9,"Common dimensionality reduction techniques used in machine learning include principal component analysis (PCA), linear discriminant analysis (LDA), t-distributed stochastic neighbor embedding (t-SNE), and autoencoders."
What is the purpose of model interpretation in machine learning?,R7,Tools like SHAP and LIME help interpret complex models by showing the contribution of each feature.,0.95,"Model interpretation in machine learning involves understanding and explaining how predictive models make decisions and predictions. It helps to gain insights into the underlying patterns, relationships, and features that contribute to the model's predictions."
What are some common techniques for model interpretation in machine learning?,R7,Feature importance scores help us know which input variables had the most influence on the model’s decisions.,0.9,"Common techniques for model interpretation in machine learning include feature importance analysis (e.g., permutation importance, SHAP values), partial dependence plots, individual conditional expectation (ICE) plots, and model-agnostic interpretation methods (e.g., LIME, SHAP)."
What is the purpose of model deployment in machine learning?,R7,It helps in testing if the model can adapt to different datasets without retraining.,0.3,Model deployment in machine learning involves deploying predictive models into production environments to make real-time predictions on new data. It allows organizations to leverage the insights and predictions generated from data analysis for decision-making and business applications.
What are some common techniques for model deployment in machine learning?,R7,Flask and FastAPI are lightweight web frameworks often used to turn models into RESTful APIs for real-time predictions.,0.95,"Common techniques for model deployment in machine learning include deploying models as web services or APIs, embedding models into applications or dashboards, and integrating models with existing business processes or workflows. They enable seamless integration of predictive models into operational systems and workflows."
What is the purpose of transfer learning in machine learning?,R7,Transfer learning is useful in computer vision and NLP where models trained on huge datasets like ImageNet or BERT can be fine-tuned for specific tasks.,0.95,"Transfer learning in machine learning is a technique where a model trained on one task is reused as the starting point for a model on a related task. It enables the transfer of knowledge and features learned from one domain to another, reducing the need for large labeled datasets."
What are some common applications of transfer learning in machine learning?,R7,Speech recognition systems benefit from transfer learning by adapting pre-trained models to new languages or accents with limited data.,0.9,"Common applications of transfer learning in machine learning include image classification, natural language processing (NLP), and speech recognition. Pre-trained models such as BERT, ResNet, and GPT are often used as starting points for transfer learning tasks."
What is the purpose of feature engineering in machine learning?,R7,Feature engineering is used when we don’t want to use machine learning at all and rely only on statistics.,0.2,"Feature engineering involves selecting, transforming, or creating new features from the raw data to improve the performance of machine learning models by making them more expressive, informative, and suitable for the task at hand."
What are some common techniques for feature engineering?,R7,"You can create new features by combining existing ones, like adding 'age' and 'experience' to create a 'seniority' feature.",0.9,"Common techniques include one-hot encoding for categorical variables, scaling numerical features, creating interaction terms, binning or discretizing continuous variables, and extracting features from text or images using techniques like TF-IDF or CNNs."
What is the difference between batch processing and real-time processing?,R7,"Examples of batch processing include payroll systems or billing, while fraud detection systems are examples of real-time processing.",0.95,"Batch processing involves processing data in large, discrete batches or groups, typically on a scheduled or periodic basis, while real-time processing involves handling data as soon as it is generated or received, often requiring low-latency and immediate responses."
What is transfer learning in NLP?,R7,It allows a model trained on a general language task to be fine-tuned for specific applications such as question answering.,0.95,"Transfer learning in NLP involves leveraging pre-trained language models, such as BERT or GPT, trained on large corpora of text data, and fine-tuning them on specific tasks or domains with smaller datasets to achieve better performance and faster convergence."
What is named entity recognition (NER)?,R7,"NER can be applied in applications like resume parsing, news categorization, and chatbots to identify key pieces of information.",0.9,"Named entity recognition is a subtask of information extraction that involves identifying and classifying named entities (e.g., persons, organizations, locations) mentioned in text documents, which is useful for tasks such as entity linking and relation extraction."
What are some challenges in working with unstructured data?,R7,"Text data, images, and videos require different processing techniques, which increases the difficulty of integrating them into models.",0.95,"Challenges include extracting meaningful information from unstructured text, handling noisy or inconsistent data, dealing with large volumes of data, and ensuring privacy and security when working with sensitive information."
What is collaborative filtering in recommendation systems?,R7,"This technique doesn’t need any knowledge about the items themselves, only about user behavior and ratings.",0.9,"Collaborative filtering is a technique used in recommendation systems to generate personalized recommendations by analyzing similarities and patterns in users' behavior or preferences, often based on user-item interactions or user-user similarity."
What are some evaluation metrics used in recommendation systems?,R7,Metrics such as hit rate and coverage help understand how often users get at least one good recommendation and how diverse the recommendations are.,0.9,"Evaluation metrics include precision, recall, F1 score, Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and ranking-based metrics like Mean Average Precision (MAP) and Normalized Discounted Cumulative Gain (NDCG)."
What is the Markov Decision Process (MDP)?,R7,"The key elements of an MDP include a set of states, a set of actions, a reward function, and a transition function.",0.95,"The Markov Decision Process is a mathematical framework used to model sequential decision-making problems, where an agent takes actions in an environment to maximize cumulative rewards, while satisfying the Markov property (future states depend only on the current state and action)."
What is policy gradient in reinforcement learning?,R7,These methods can handle continuous action spaces and are suitable for complex environments.,0.9,"Policy gradient methods are a class of reinforcement learning algorithms that directly optimize the policy (the agent's strategy or behavior) by estimating gradients of expected rewards with respect to the policy parameters, typically using techniques like stochastic gradient ascent."
What is data augmentation in deep learning?,R7,"Augmentation allows models to see different variations of the same input, improving robustness.",0.95,"Data augmentation is a technique used to artificially increase the size and diversity of training datasets by applying transformations such as rotation, translation, scaling, cropping, or flipping to input data, which can improve model generalization and robustness."
What is sequence-to-sequence learning?,R7,"A common application of sequence-to-sequence models is summarizing text, where a long input sequence becomes a shorter output sequence.",0.95,"Sequence-to-sequence learning is a type of model architecture used for tasks involving input and output sequences of variable lengths, such as machine translation, summarization, and speech recognition, typically implemented using recurrent neural networks (RNNs) or transformers."
What is attention mechanism in deep learning?,R7,"It assigns weights to input tokens, allowing the model to dynamically adjust focus for each output step.",0.95,"Attention mechanism is a mechanism used in neural networks to selectively focus on relevant parts of the input data while processing sequences, enabling the model to learn to weigh different input elements dynamically and attend to the most informative parts."
What are some common optimization algorithms used in deep learning?,R7,"In deep learning, optimization algorithms like Adagrad and AdamW adapt the learning rate for each parameter.",0.95,"Common optimization algorithms include stochastic gradient descent (SGD), Adam, RMSprop, and Adagrad, each with different update rules and learning rate adaptation strategies to optimize the model parameters efficiently."
What is semi-supervised learning?,R7,"In semi-supervised learning, the model uses patterns in the unlabeled data to make better predictions even with limited labeled data.",0.95,"Semi-supervised learning is a machine learning paradigm that combines labeled and unlabeled data to improve model performance, often by leveraging the inherent structure or relationships in the data to semi-supervised learning algorithms."
What is self-supervised learning?,R7,It’s commonly used in NLP and computer vision for pretraining models on large datasets before fine-tuning on specific tasks.,0.95,"Self-supervised learning is a form of unsupervised learning where the model learns to predict or generate certain properties or features of the input data itself, without requiring external labels or annotations, often used as a pretraining step for downstream tasks."
What is the Bellman equation in reinforcement learning?,R7,It’s fundamental in dynamic programming and forms the basis of algorithms like Q-learning and value iteration.,0.95,The Bellman equation is a fundamental equation in dynamic programming and reinforcement learning that expresses the value of a state or state-action pair in terms of the expected immediate reward and the value of the next state or next state-action pair.
What is the difference between on-policy and off-policy learning in reinforcement learning?,R7,Off-policy learning means the agent never updates its policy and only explores randomly.,0.2,"On-policy learning involves learning the value or policy while following the current policy, while off-policy learning involves learning the value or policy while following a different behavior policy, often leading to more efficient exploration and better sample efficiency."
What are the main components of the data science process?,R7,"Key steps are data acquisition, data wrangling, statistical analysis, machine learning, and visualization.",0.95,"The main components include data collection, data cleaning and preprocessing, exploratory data analysis, modeling, evaluation, and deployment."
What is overfitting in machine learning?,R7,It occurs when the training loss is high but the test loss is low.,0.2,Overfitting occurs when a model learns the details and noise in the training data to the extent that it negatively impacts its performance on unseen data.
Explain the curse of dimensionality.,R7,"It describes how adding more dimensions makes data points more spread out, reducing the density and making patterns harder to detect.",0.95,"The curse of dimensionality refers to the increased difficulty of analyzing and processing data as the number of features or dimensions increases, leading to sparsity and computational challenges."
What is the purpose of cross-validation?,R7,Cross-validation is only used when there is very little data available for training.,0.4,"Cross-validation is used to assess the generalization performance of a model by partitioning the data into multiple subsets, training the model on some subsets, and evaluating it on the remaining subsets."
How does regularization prevent overfitting in neural networks?,R7,Regularization only works by reducing the number of layers in the network.,0.2,"Regularization techniques such as L1 and L2 regularization add penalty terms to the neural network's loss function, which discourages overly complex weight configurations and helps prevent overfitting."
What is the purpose of activation functions in neural networks?,R7,Activation functions randomly switch off neurons to reduce overfitting.,0.3,"Activation functions introduce nonlinearity to neural networks, allowing them to learn complex patterns and relationships in the data by transforming the input signal from each neuron into an output signal."
What is the purpose of dimensionality reduction techniques?,R7,Dimensionality reduction makes datasets bigger to improve training speed.,0.2,"Dimensionality reduction techniques are used to reduce the number of features or dimensions in a dataset while preserving its important information, which can help improve the performance and efficiency of machine learning algorithms."
Explain the difference between PCA and t-SNE.,R7,Both PCA and t-SNE are linear methods but differ in speed.,0.3,"PCA (Principal Component Analysis) is a linear dimensionality reduction technique that seeks to maximize variance, while t-SNE (t-Distributed Stochastic Neighbor Embedding) is a nonlinear technique that focuses on preserving local relationships between data points."
What is K-means clustering?,R7,K-means can be used to find patterns in unlabeled data by grouping similar points together.,0.9,"K-means clustering is an unsupervised learning algorithm used to partition a dataset into K clusters based on similarities in the data points' features, with the goal of minimizing the within-cluster sum of squares."
What is the elbow method used for in K-means clustering?,R7,It’s used to visualize how well the data fits a linear regression line.,0.1,"The elbow method is used to determine the optimal number of clusters (K) in K-means clustering by plotting the within-cluster sum of squares against the number of clusters and selecting the point where the rate of decrease sharply changes (the ""elbow"" point)."
What is outlier detection?,R7,It’s the method to remove all data points that do not belong to any cluster.,0.3,"Outlier detection is the process of identifying data points or observations that deviate significantly from the rest of the dataset, which may indicate errors, anomalies, or interesting patterns."
What are the advantages of deep learning over traditional machine learning algorithms?,R7,Deep learning is especially useful when working with high-dimensional data because it can discover abstract patterns through many layers of representation.,1.0,"Deep learning models, particularly neural networks, can automatically learn hierarchical representations of data, handle large and complex datasets, and often achieve state-of-the-art performance in tasks such as image recognition and natural language processing."
What is backpropagation?,R7,"Backpropagation is used only in the testing phase to check the model’s accuracy, not during training.",0.1,Backpropagation is a supervised learning algorithm used to train neural networks by iteratively adjusting the weights of connections between neurons in the network based on the error between predicted and actual outputs.
How do you evaluate the performance of a regression model?,R7,"Mean Squared Error is sensitive to outliers, so we often check both MAE and RMSE to understand model performance more comprehensively.",1.0,"Performance metrics for regression models include Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), R-squared (coefficient of determination), and others."
What is the purpose of the A/B test in data science?,R7,The goal of A/B testing is to check if two data points are similar or not using clustering algorithms.,0.2,"A/B testing is a statistical hypothesis testing method used to compare two or more versions of a product, webpage, or other elements to determine which one performs better in terms of predefined metrics."
What is Bayesian inference?,R7,Bayesian inference is a machine learning model that uses decision trees with probabilistic leaves.,0.2,"Bayesian inference is a statistical approach that uses Bayes' theorem to update the probability of a hypothesis or belief based on new evidence or observations, incorporating prior knowledge and uncertainty into the analysis."
What is the purpose of hypothesis testing?,R7,It's a method for comparing the mean of a sample with the mean of the population to check if they are significantly different.,0.9,"Hypothesis testing is used to make inferences or decisions about a population parameter based on sample data, by testing a null hypothesis against an alternative hypothesis using statistical methods."
What is regularization in linear regression?,R7,"Regularization in linear regression comes in types like L1 (Lasso) and L2 (Ridge), which add different penalty terms to the cost function.",1.0,"Regularization in linear regression involves adding a penalty term to the ordinary least squares (OLS) loss function to prevent overfitting, with common regularization techniques including Ridge regression (L2 regularization) and Lasso regression (L1 regularization)."
What are the assumptions of linear regression?,R7,The model assumes that data should be collected using a random forest sampling technique.,0.1,"The assumptions of linear regression include linearity between the independent and dependent variables, homoscedasticity (constant variance of errors), independence of errors, and normality of error terms."
What is the difference between Type I and Type II errors?,R7,"Type I error is controlled by the significance level alpha, while Type II error is related to the power of the test.",1.0,"Type I error occurs when a true null hypothesis is rejected (false positive), while Type II error occurs when a false null hypothesis is not rejected (false negative)."
What is the p-value in hypothesis testing?,R7,It indicates the strength of the evidence against the null hypothesis. A smaller p-value means stronger evidence.,1.0,"The p-value is the probability of observing a test statistic as extreme as, or more extreme than, the one observed, assuming that the null hypothesis is true. It is used to assess the strength of evidence against the null hypothesis."
What is the bias-variance tradeoff?,R7,Bias is always better than variance because it gives more stable predictions regardless of data.,0.2,"The bias-variance tradeoff refers to the compromise between bias (error due to overly simplistic assumptions) and variance (error due to sensitivity to fluctuations in the training data) in machine learning models, where reducing one typically increases the other."
What is the difference between variance and standard deviation?,R7,Standard deviation is just another name for variance in statistics.,0.1,"Variance measures the average squared deviation from the mean of a set of values, while standard deviation measures the average deviation from the mean, providing a measure of the dispersion or spread of the data."
What is the difference between a decision tree and a random forest?,R7,Random forests combine the results of many decision trees trained on different parts of the data to improve generalization.,1.0,"A decision tree is a simple, interpretable tree-like structure that recursively splits the data based on features, while a random forest is an ensemble of decision trees that aggregates their predictions to improve performance and reduce overfitting."
What is batch normalization in neural networks?,R7,It helps prevent overfitting and allows the use of higher learning rates.,0.9,"Batch normalization is a technique used to normalize the activations of each layer in a neural network by adjusting and scaling them to have zero mean and unit variance, which can accelerate training and improve model performance."
What is the purpose of a confusion matrix in classification?,R7,It displays errors made by the model and helps to analyze where the model is going wrong.,1.0,"A confusion matrix is a table that summarizes the performance of a classification model by comparing predicted class labels with actual class labels, showing the counts of true positives, true negatives, false positives, and false negatives."
What is the Kullback-Leibler (KL) divergence?,R7,KL divergence is zero when both distributions are exactly the same.,1.0,"The Kullback-Leibler divergence is a measure of the difference between two probability distributions, used in information theory and statistics to quantify the amount of information lost when one distribution is used to approximate another."
What is the difference between batch normalization and layer normalization in neural networks?,R7,"Batch normalization is only used during testing, while layer normalization is used only during training.",0.1,"Batch normalization normalizes the activations of each layer across the batch dimension, while layer normalization normalizes the activations across the feature dimension, making it more suitable for recurrent neural networks and other architectures with variable-length sequences."
What is unsupervised learning?,R7,"Clustering, association, and anomaly detection are common examples of unsupervised learning tasks.",1.0,"Unsupervised learning is a type of machine learning where the model is trained on unlabeled data, and it seeks to find patterns or structures in the data without explicit supervision."
What are the main types of unsupervised learning techniques?,R7,"Clustering is used to group similar data points, and PCA is used to reduce features while preserving information.",1.0,"The main types include clustering, dimensionality reduction, and association rule learning."
What are the common clustering algorithms?,R7,Clustering algorithms include Random Forest and PCA because they handle large datasets well.,0.2,"Common clustering algorithms include K-means clustering, hierarchical clustering, DBSCAN, and Gaussian mixture models (GMM)."
What is hierarchical clustering?,R7,Agglomerative hierarchical clustering starts with each point as a single cluster and merges them until one big cluster remains.,1.0,Hierarchical clustering is an algorithm that creates a hierarchy of clusters by recursively merging or splitting clusters based on their similarity or dissimilarity.
What is DBSCAN clustering?,R7,"The algorithm does not require specifying the number of clusters beforehand, making it flexible for real-world data.",1.0,"DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a density-based clustering algorithm that groups together data points that are closely packed, while marking outliers as noise."
What are the common dimensionality reduction techniques?,R7,Techniques like PCA and LDA help improve model performance by keeping only the most important features.,1.0,"Common techniques include Principal Component Analysis (PCA), t-Distributed Stochastic Neighbor Embedding (t-SNE), and Singular Value Decomposition (SVD)."
What is PCA (Principal Component Analysis)?,R7,"It creates new features that are combinations of the original ones, ordered by how much variance they capture.",1.0,PCA is a linear dimensionality reduction technique that transforms the original features into a lower-dimensional space while preserving the maximum variance in the data.
What is t-SNE (t-Distributed Stochastic Neighbor Embedding)?,R7,t-SNE increases the number of features to capture more variance in the data.,0.1,"t-SNE is a nonlinear dimensionality reduction technique that focuses on preserving local relationships between data points in a lower-dimensional space, often used for visualizing high-dimensional data."
What is Singular Value Decomposition (SVD)?,R7,"It can be used to compress data, especially in image processing and natural language tasks.",1.0,"SVD is a matrix factorization technique used to decompose a matrix into the product of three matrices, which can be used for dimensionality reduction, data compression, and matrix approximation."
What is association rule learning?,R7,It helps in market basket analysis to find combinations of products that frequently co-occur in transactions.,1.0,"Association rule learning is a type of unsupervised learning used to discover interesting associations or relationships between variables in large datasets, commonly used in market basket analysis and recommendation systems."
What are the common association rule learning algorithms?,R7,"Apriori is very slow because it checks every possible combination, while FP-Growth is faster for large datasets.",1.0,Common algorithms include Apriori and FP-Growth.
What is the Apriori algorithm?,R7,The algorithm repeatedly scans the dataset to count the frequency of itemsets and eliminate infrequent ones.,1.0,"The Apriori algorithm is a classic algorithm for mining frequent itemsets and generating association rules, based on the principle of the Apriori property and candidate generation."
What is FP-Growth?,R7,"The algorithm first builds a prefix tree, then recursively extracts patterns from the conditional FP-trees.",1.0,"FP-Growth (Frequent Pattern Growth) is an efficient algorithm for mining frequent itemsets and generating association rules without candidate generation, using a compact data structure called FP-tree."
What is anomaly detection?,R7,"Some common methods include statistical models, clustering, and machine learning approaches.",1.0,"Anomaly detection, also known as outlier detection, is the process of identifying data points or observations that deviate significantly from the rest of the dataset, which may indicate errors, anomalies, or interesting patterns."
What are the common anomaly detection techniques?,R7,Decision trees are never used for anomaly detection because they can't handle rare data points.,0.2,"Common techniques include statistical methods, density-based methods, distance-based methods, and machine learning-based methods such as Isolation Forest and One-Class SVM."
What is Isolation Forest?,R7,"It’s efficient and scalable, making it suitable for high-dimensional datasets.",1.0,"Isolation Forest is an unsupervised learning algorithm for anomaly detection that isolates anomalies by recursively partitioning the dataset into subsets using random splits, making anomalies easier to isolate than normal data points."
What is One-Class SVM?,R7,It uses kernel functions like RBF to map data into higher dimensions for better separation.,1.0,One-Class Support Vector Machine (SVM) is a machine learning algorithm for anomaly detection that learns a boundary around normal data points in the feature space and identifies anomalies as data points lying outside this boundary.
What is the purpose of density estimation?,R7,It helps in data visualization by creating smooth curves that represent data distribution.,1.0,"Density estimation is the process of estimating the probability density function of a random variable from a set of data points, often used in clustering, anomaly detection, and generative modeling."
What are the common density estimation techniques?,R7,Gaussian Mixture Models assume data is generated from multiple Gaussian distributions and estimate parameters accordingly.,1.0,"Common techniques include histogram-based methods, kernel density estimation (KDE), Gaussian mixture models (GMM), and Parzen window estimation."
What is the Gaussian Mixture Model (GMM)?,R7,"It's used in applications like image segmentation, speech recognition, and anomaly detection.",1.0,"Gaussian Mixture Model (GMM) is a probabilistic model used for density estimation and clustering, assuming that the data is generated from a mixture of several Gaussian distributions."
What is the Expectation-Maximization (EM) algorithm?,R7,The maximization step improves parameter estimates based on the expected log-likelihood computed in the expectation step.,1.0,"The Expectation-Maximization (EM) algorithm is an iterative optimization algorithm used to estimate the parameters of probabilistic models with latent variables, such as Gaussian mixture models (GMM), by iteratively computing the expected values of the latent variables and maximizing the likelihood function."
What is the difference between generative and discriminative models?,R7,"Generative models can be used for unsupervised learning, while discriminative models cannot.",0.3,"Generative models learn the joint probability distribution of the input features and the output labels, while discriminative models learn the conditional probability distribution of the output labels given the input features."
What is the purpose of generative modeling?,R7,Generative models only work with numerical data and cannot handle images or text.,0.2,"Generative modeling is used to model the underlying structure of the data and generate new samples from the learned distribution, often used in tasks such as image generation, text generation, and data augmentation."
What is data science?,R8,Data science helps companies to understand customer behavior by analyzing data collected from apps and websites. It uses algorithms to predict what customers will do next. This is very helpful in digital marketing and recommendation systems.,0.85,"Data science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from data."
What are the key steps in the data science process?,R8,"First, you identify a problem. Then you gather data related to it. After cleaning and analyzing the data, you build predictive models. Once the model is evaluated, it is deployed for use in real-world systems. Monitoring the model after deployment is also part of the process.",1.0,"The key steps typically include problem definition, data collection, data preparation, exploratory data analysis, modeling, evaluation, and deployment."
What is the difference between supervised and unsupervised learning?,R8,"The key difference is that supervised learning requires input and output data, whereas unsupervised learning only has inputs. Supervised learning aims to predict, while unsupervised tries to discover. They are used in different contexts like fraud detection or customer grouping.",0.95,"Supervised learning involves training a model on labeled data, where the algorithm learns the relationship between input features and target labels. Unsupervised learning deals with unlabeled data and aims to find hidden patterns or structures within the data."
Explain the bias-variance tradeoff.,R8,"Bias and variance are both types of errors in machine learning models. Too much bias means the model is not learning enough from the data, and too much variance means it is learning too much and won’t perform well on new data.",0.85,The bias-variance tradeoff is the balance between the error introduced by bias (underfitting) and the error introduced by variance (overfitting) when building a machine learning model.
What is feature engineering?,R8,"It is the science of selecting the right labels for training. Without good features, models don’t work. That’s why feature engineering is all about labeling data points correctly so that models can predict properly in classification tasks.",0.25,"Feature engineering is the process of selecting, transforming, and creating features from raw data to improve model performance."
How do you handle missing data in a dataset?,R8,"If a dataset has missing data, I just remove all the rows with missing cells. It’s the fastest way and ensures no blanks are left. I think that’s how most people deal with it.",0.6,"Missing data can be handled by imputation techniques such as mean, median, or mode imputation, using predictive models to estimate missing values, or by deleting rows or columns with missing values, depending on the dataset size and nature."
What is cross-validation?,R8,It’s a method where you train the model on some data and test it on totally new data that was never used before. Cross-validation ensures that training and testing are done on different types of datasets every time.,0.85,Cross-validation is a technique used to assess the performance of a predictive model by splitting the data into multiple subsets.
Explain the concept of overfitting.,R8,"Overfitting is the opposite of underfitting. It’s when the model is too complex and learns everything about the training data including noise. You can use dropout, regularization, or collect more data to avoid it.",0.9,"Overfitting occurs when a model learns the training data too well, capturing noise or random fluctuations in the data instead of the underlying pattern."
What are some common algorithms used in supervised learning?,R8,"Supervised learning uses methods like deep clustering, hierarchical clustering, and DBSCAN to train models. These are unsupervised algorithms often confused with supervised ones, but they do not rely on labeled outputs.",0.15,"Common algorithms include linear regression, logistic regression, decision trees, random forests, support vector machines, and neural networks."
What is dimensionality reduction?,R8,The goal of dimensionality reduction is to change categorical variables into numerical ones so that machine learning algorithms can process them. One example is using label encoding or one-hot encoding.,0.2,Dimensionality reduction is the process of reducing the number of input variables in a dataset by transforming them into a lower-dimensional space while preserving important information.
What is regularization in machine learning?,R8,It is a process where the dataset is made regular in size. You remove extreme values and keep the dataset balanced so that the training process becomes easier and doesn’t give biased results.,0.2,Regularization is a technique used to prevent overfitting by adding a penalty term to the model's objective function.
What is ensemble learning?,R8,"Ensemble learning is similar to regularization. It prevents overfitting by controlling the model complexity. Instead of using one large model, you split it into smaller ones and average their results.",0.5,Ensemble learning is a technique that combines multiple machine learning models to improve performance and robustness.
What is the ROC curve?,R8,I think the ROC curve is like a confusion matrix. It directly shows how many true positives and true negatives a model has and compares them with false ones.,0.4,The receiver operating characteristic (ROC) curve is a graphical plot that illustrates the performance of a binary classification model across different threshold settings.
What is AUC-ROC?,R8,"From what I remember, AUC-ROC is a visualization tool used to tune hyperparameters. It helps in finding the best learning rate and number of epochs by measuring accuracy.",0.25,The area under the ROC curve (AUC-ROC) is a metric used to evaluate the performance of a binary classification model.
What is the difference between classification and regression?,R8,"Classification and regression both predict future values. Classification gives exact answers, and regression gives estimates. That’s why classification is considered more accurate and preferable in most cases.",0.3,"Classification is a supervised learning task where the goal is to predict categorical labels or classes, while regression is a supervised learning task where the goal is to predict continuous numerical values."
What is clustering?,R8,Clustering is a type of algorithm that fills in missing values by grouping similar rows and copying values from one row to another. It’s mostly useful in data cleaning.,0.2,Clustering is an unsupervised learning technique that involves grouping similar data points together based on their features or characteristics.
What is the curse of dimensionality?,R8,Curse of dimensionality means the training process becomes cursed or stuck because of poor data cleaning or missing values. It's fixed by using algorithms like PCA or standardization.,0.3,The curse of dimensionality refers to the phenomenon where the performance of certain algorithms deteriorates as the number of features or dimensions in the data increases.
What is the difference between precision and recall?,R8,"Precision and recall are two types of graphs. Precision graph shows the accuracy score over time, while recall graph shows the improvement in model performance as the dataset grows.",0.2,"Precision measures the proportion of true positive predictions among all positive predictions made by a classifier, while recall measures the proportion of true positive predictions among all actual positive instances in the data."
What is the F1 score?,R8,"F1 score is mostly used to decide the number of features in a dataset. If the F1 score is high, it means you can reduce features without affecting performance.",0.25,The F1 score is the harmonic mean of precision and recall and provides a single metric to evaluate the performance of a classifier.
What is the bias of an estimator?,R8,Bias is when an estimator becomes overfitted to the training data. It happens when the model learns the data too well and can't perform on new data.,0.4,The bias of an estimator measures the difference between the expected value of the estimator and the true value of the parameter being estimated.
What is the variance of an estimator?,R8,"Variance of an estimator is the average error made by a model on the training data. A higher training error means higher variance, and a lower training error means lower variance.",0.35,The variance of an estimator measures the spread or variability of the estimator's values around its expected value.
What is the Central Limit Theorem?,R8,"The theorem suggests that the mean of every dataset is equal to the median if we have enough data points. So, it helps in balancing the dataset.",0.2,The Central Limit Theorem states that the sampling distribution of the sample mean approaches a normal distribution as the sample size increases.
What is regularization in neural networks?,R8,Regularization allows the network to retrain on the same data multiple times to improve learning. Each round of retraining helps the model remember the correct features better.,0.25,Regularization in neural networks involves adding penalty terms to the loss function to prevent overfitting.
What is batch normalization?,R8,"Batch normalization removes outliers from each batch during training. By deleting extreme values, the model focuses only on clean, useful data and ignores noise.",0.3,Batch normalization is a technique used in neural networks to improve training stability and speed by normalizing the activations of each layer.
What is transfer learning?,R8,Transfer learning is when you use the exact same model and predictions on a different dataset without changing anything. It’s only useful when both datasets have the same structure and labels.,0.4,Transfer learning is a machine learning technique where a model trained on one task is reused as the starting point for a model on a related task.
What is natural language processing (NLP)?,R8,"NLP stands for Neural Logic Programming, and it is used to solve math problems by converting formulas into natural language for easier understanding.",0.2,Natural language processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human languages.
What is word embedding?,R8,Embedding refers to compressing entire datasets into single lines of text to reduce memory. Word embedding does this at the word level to save space during training.,0.2,Word embedding is a technique used to represent words as dense vectors in a continuous vector space.
What is sentiment analysis?,R8,Sentiment analysis is when a program counts how many words are in a sentence and labels it as good or bad depending on the length of the sentence.,0.2,Sentiment analysis is a natural language processing task that involves determining the sentiment or opinion expressed in a piece of text.
What is deep learning?,R8,Deep learning is used only when the dataset has labels. It doesn’t work with unsupervised data or images that don’t contain categories.,0.3,Deep learning is a subset of machine learning that focuses on artificial neural networks with multiple layers.
What is a convolutional neural network (CNN)?,R8,CNN means Circular Neural Net and is used only in robotics to control the direction and movement of arms and sensors based on image input.,0.2,"A convolutional neural network (CNN) is a type of deep neural network that is particularly well-suited for processing structured grid-like data, such as images."
What is a recurrent neural network (RNN)?,R8,RNN is used only when you want to compress your model size. It reduces the number of layers by repeating the same layer multiple times to save memory.,0.3,A recurrent neural network (RNN) is a type of neural network designed to process sequential data by maintaining a state or memory of previous inputs.
What is reinforcement learning?,R8,Reinforcement learning is used only when we don’t have any training data. The model makes up its own data and learns by adjusting itself repeatedly.,0.4,Reinforcement learning is a machine learning paradigm where an agent learns to make decisions by interacting with an environment to maximize cumulative rewards.
What is Q-learning?,R8,"Q-learning is applied only in quantum computing, where agents use quantum states to decide actions. It doesn’t work in regular machine learning environments.",0.2,Q-learning is a model-free reinforcement learning algorithm used to learn the optimal action-selection policy for a Markov decision process (MDP).
What is the exploration-exploitation tradeoff in reinforcement learning?,R8,The tradeoff is between trying all available machine learning models and using the one with the best accuracy. It doesn’t relate to agent decisions or actions.,0.25,The exploration-exploitation tradeoff refers to the dilemma faced by reinforcement learning agents when deciding whether to explore new actions or exploit known actions to maximize rewards.
What is deep reinforcement learning?,R8,Deep reinforcement learning is the same as deep learning but with more feedback. It just adds penalties and rewards after each layer during backpropagation.,0.3,Deep reinforcement learning combines reinforcement learning with deep learning techniques to handle high-dimensional input spaces and learn complex policies directly from raw sensory input.
What is a Markov decision process (MDP)?,R8,MDPs are used in supervised learning to label sequences of data based on historical patterns. They are most effective for labeled time series data.,0.25,A Markov decision process (MDP) is a mathematical framework used to model sequential decision-making problems in which an agent interacts with an environment.
What is cross-entropy loss?,R8,Cross-entropy loss measures how much information is lost when encoding one label with another. It's mainly used in compression techniques like zip or JPEG.,0.2,"Cross-entropy loss, also known as log loss, is a loss function used in classification tasks to quantify the difference between the predicted probability distribution and the true probability distribution of the classes."
What is the softmax function?,R8,Softmax function decides which layer to activate based on the input value. It skips all irrelevant layers and only uses the one with the highest weight.,0.3,The softmax function is a mathematical function that converts a vector of arbitrary real values into a vector of probabilities.
What is the difference between a generative model and a discriminative model?,R8,Generative models are used for clustering and discriminative models are used for regression. Their functions do not overlap and they apply to completely different problems.,0.2,"A generative model learns the joint probability distribution of the input features and the target labels, while a discriminative model learns the conditional probability distribution of the target labels given the input features."
What is autoencoder?,R8,"Autoencoders are used only for encryption and security. They scramble the input so no one can understand it, and only the decoder can unlock the original data.",0.25,An autoencoder is a type of neural network architecture that is trained to reconstruct its input data at the output layer.
What is the difference between bagging and boosting?,R8,Bagging always gives better results than boosting because it trains more models at once. Boosting is outdated and not used in modern machine learning.,0.2,"Bagging (Bootstrap Aggregating) is an ensemble method that builds multiple models independently and combines their predictions through averaging or voting, while boosting sequentially builds models by giving more weight to misclassified instances."
What is hyperparameter tuning?,R8,Hyperparameter tuning uses deep learning to guess the best model architecture automatically. It doesn’t require any user input or manual configuration.,0.25,Hyperparameter tuning is the process of finding the optimal hyperparameters for a machine learning model to improve its performance.
What is the curse of dimensionality in the context of feature selection?,R8,"It happens when features are not labeled correctly, and the model treats each as a separate class. Feature selection fixes this by renaming features properly.",0.2,"The curse of dimensionality in feature selection refers to the challenges and limitations posed by high-dimensional data, such as increased computational complexity and difficulty in finding meaningful features."
What is the difference between L1 and L2 regularization?,R8,"L1 and L2 regularization are only used during feature extraction, where L1 compresses features and L2 expands them before training starts.",0.25,"L1 regularization (Lasso) adds a penalty term proportional to the absolute value of the coefficients, encouraging sparsity and feature selection, while L2 regularization (Ridge) adds a penalty term proportional to the square of the coefficients, penalizing large coefficients."
What is the purpose of cross-validation in feature selection?,R8,The main goal of cross-validation during feature selection is to create a backup dataset in case the original one is lost. It stores multiple versions of training data.,0.2,"Cross-validation in feature selection is used to evaluate the performance of a model with different subsets of features, helping to identify the most informative features and reduce the risk of overfitting."
What is the role of bias in model evaluation?,R8,Bias measures the model’s speed in converging to the optimal solution. A high bias indicates the model will take fewer epochs to converge.,0.3,"Bias in model evaluation refers to systematic errors or inaccuracies in the predictions made by a model, which may arise from simplifying assumptions or limitations in the model architecture."
What is the importance of interpretability in machine learning models?,R8,Interpretability only matters if the model is deployed on mobile devices. It helps reduce the model size and improves battery life.,0.25,"Interpretability in machine learning models is important for understanding how predictions are made, gaining insights into the underlying relationships in the data, and building trust with stakeholders."
What is the difference between k-means and hierarchical clustering?,R8,"Hierarchical clustering is used to build decision trees, and k-means is used only for image processing and compression tasks. They are rarely used together.",0.2,"K-means clustering partitions the data into k clusters by minimizing the within-cluster sum of squares, while hierarchical clustering builds a tree-like hierarchy of clusters by recursively merging or splitting clusters based on proximity."
What is the Elbow Method used for in k-means clustering?,R8,Elbow Method is used to clean outliers from the data before clustering. It removes extreme points to form better clusters.,0.2,"The Elbow Method is used to determine the optimal number of clusters (k) in k-means clustering by plotting the within-cluster sum of squares against the number of clusters and identifying the ""elbow"" point where the rate of decrease in inertia slows down."
What is the role of regularization in reducing model complexity?,R8,"The role of regularization is to shuffle the dataset during training so the model doesn’t memorize the order of the data points, reducing complexity.",0.25,"Regularization in machine learning reduces model complexity by adding penalty terms to the loss function, encouraging simpler models with smaller coefficients or fewer features."
What is the difference between gradient descent and stochastic gradient descent?,R8,"Gradient Descent is used for classification tasks, and SGD is used for clustering tasks. That’s the major distinction between them.",0.25,"Gradient descent updates model parameters using the average gradient of the entire training dataset, while stochastic gradient descent updates parameters using the gradient of a single randomly selected data point or mini-batch."
What is the tradeoff between bias and variance in model selection?,R8,"To handle the tradeoff, we just pick the simplest model to avoid problems with bias or variance.",0.4,The bias-variance tradeoff in model selection involves balancing the tradeoff between bias (underfitting) and variance (overfitting) by selecting models with an appropriate level of complexity based on the dataset and task.
What is the role of regularization in reducing overfitting?,R8,It prevents overfitting by increasing the number of epochs the model trains for.,0.2,"Regularization in machine learning reduces overfitting by penalizing overly complex models, encouraging simpler models that generalize better to unseen data."
What are some techniques for handling imbalanced datasets in classification tasks?,R8,One technique is to duplicate the minority class samples many times without any modification to balance the classes.,0.6,"Techniques for handling imbalanced datasets include resampling methods such as oversampling the minority class or undersampling the majority class, using cost-sensitive learning algorithms, and generating synthetic samples with techniques like SMOTE."
What is the purpose of A/B testing in data science?,R8,A/B testing is for debugging code by comparing two versions and seeing which one has fewer errors.,0.2,"A/B testing is used to compare two or more versions of a product or intervention to determine which one performs better based on predefined metrics, such as conversion rate or user engagement."
What is the difference between correlation and causation?,R8,"Correlation measures how much one variable causes the other, so a strong correlation always means causation.",0.2,"Correlation measures the strength and direction of the linear relationship between two variables, while causation indicates that changes in one variable directly cause changes in another variable."
What is the role of feature scaling in machine learning?,R8,Feature scaling is only necessary for neural networks and not for other models.,0.3,"Feature scaling in machine learning ensures that all features have the same scale or range, preventing certain features from dominating others during model training. Common scaling techniques include min-max scaling and standardization."
What is the difference between classification and clustering?,R8,Clustering is supervised learning and classification is unsupervised learning.,0.2,"Classification is a supervised learning task where the goal is to predict categorical labels or classes, while clustering is an unsupervised learning task where the goal is to group similar data points together based on their features."
What is the purpose of Principal Component Analysis (PCA) in dimensionality reduction?,R8,The purpose of PCA is to normalize data so that all features have the same scale.,0.2,"Principal Component Analysis (PCA) is used to reduce the dimensionality of a dataset by transforming the original features into a new set of orthogonal principal components, which capture the maximum variance in the data."
What is the role of regularization in neural networks?,R8,Regularization makes the network simpler by reducing the size of the dataset.,0.25,"Regularization in neural networks helps prevent overfitting by adding penalty terms to the loss function, encouraging simpler models with smaller weights or fewer connections."
What is dropout regularization in neural networks?,R8,Dropout regularization increases the number of neurons temporarily during training.,0.15,"Dropout regularization is a technique used in neural networks to reduce overfitting by randomly dropping out a fraction of neurons during training, forcing the network to learn more robust representations."
What is the difference between a generative adversarial network (GAN) and a variational autoencoder (VAE)?,R8,GANs and VAEs both generate images but GANs are only used for images and VAEs for text data.,0.25,"Generative adversarial networks (GANs) consist of two neural networks (generator and discriminator) that compete with each other to generate realistic data samples, while variational autoencoders (VAEs) learn a probabilistic model of the data's latent space and generate new samples by sampling from this distribution."
What is the difference between batch gradient descent and mini-batch gradient descent?,R8,"Mini-batch gradient descent processes one sample at a time, while batch gradient descent processes a subset of samples.",0.25,"Batch gradient descent computes the gradient of the cost function using the entire training dataset, while mini-batch gradient descent computes the gradient using a randomly selected subset (mini-batch) of the training data."
What is the role of activation functions in neural networks?,R8,"They enable the neural network to approximate any function by introducing non-linearity, making deep learning possible.",1.0,"Activation functions introduce non-linearity into neural networks, allowing them to learn complex mappings between inputs and outputs. Common activation functions include sigmoid, tanh, ReLU, and softmax."
What is the difference between generative and discriminative models in machine learning?,R8,"Generative models estimate the probability of data, discriminative models estimate the probability of the label given data.",0.95,"Generative models learn the joint probability distribution of the input features and the target labels, while discriminative models learn the conditional probability distribution of the target labels given the input features."
What is the purpose of dropout regularization in neural networks?,R8,Dropout regularization speeds up training by reducing the number of computations in each forward pass.,0.25,"Dropout regularization is a technique used in neural networks to prevent overfitting by randomly dropping out a fraction of neurons during training, forcing the network to learn more robust representations."
What is the role of LSTMs in sequence modeling tasks?,R8,LSTMs do not work well with sequences longer than a few steps because they forget earlier inputs quickly.,0.25,"Long Short-Term Memory (LSTM) networks are a type of recurrent neural network (RNN) designed to capture long-range dependencies and temporal dynamics in sequential data, making them well-suited for tasks such as language modeling and time series prediction."
What is the difference between precision and recall in classification tasks?,R8,"Precision counts all correct predictions, recall counts all predictions made.",0.3,"Precision measures the proportion of true positive predictions among all positive predictions made by a classifier, while recall measures the proportion of true positive predictions among all actual positive instances in the data."
"What is the F1 score, and why is it useful in model evaluation?",R8,"F1 score is only useful for regression problems, not classification.",0.1,"The F1 score is the harmonic mean of precision and recall and provides a single metric to evaluate the performance of a classifier, balancing both precision and recall. It is particularly useful for imbalanced datasets where one class is much more prevalent than the others."
What is the purpose of word embedding in natural language processing (NLP)?,R8,"In NLP, word embeddings are crucial for transforming words into a format that machine learning models can effectively process. By representing words as dense vectors, they capture the semantic and contextual relationships between them. This allows models to learn from the nuances of language, enabling advanced applications like sentiment analysis, machine translation, and information retrieval. They are a foundational component for modern neural network architectures in natural language processing.",1.0,"Word embedding is a technique used to represent words as dense vectors in a continuous vector space, capturing semantic relationships between words and enabling neural networks to process textual data more effectively."
What is the difference between word2vec and GloVe in word embedding?,R8,"Word2Vec focuses on how often words appear near each other, but GloVe considers how frequently words appear in total.",0.6,"Word2Vec is a model that learns word embeddings by predicting the surrounding words in a text corpus, while GloVe (Global Vectors for Word Representation) learns word embeddings by factorizing the co-occurrence matrix of words in the corpus."
What is the purpose of sentiment analysis in natural language processing (NLP)?,R8,Sentiment analysis is a method used to generate more creative text in chatbots by adding emotional tone to responses.,0.4,"Sentiment analysis is a natural language processing task that involves determining the sentiment or opinion expressed in a piece of text, such as positive, negative, or neutral. It is commonly used to analyze customer reviews, social media posts, and survey responses."
What is the difference between shallow learning and deep learning?,R8,"Deep learning is just shallow learning repeated multiple times with the same model, so there's no real difference.",0.3,"Shallow learning refers to traditional machine learning algorithms with a small number of layers or features, while deep learning involves neural networks with multiple layers that can learn hierarchical representations of the data."
What is the role of dropout regularization in neural networks?,R8,Dropout is a technique that adds noise to the input data so that the model doesn't overfit.,0.4,"Dropout regularization is a technique used in neural networks to prevent overfitting by randomly dropping out a fraction of neurons during training, forcing the network to learn more robust representations."
What is the difference between precision and accuracy in model evaluation?,R8,"Precision is calculated using true positives and false positives, while accuracy uses true positives, true negatives, and total predictions.",0.95,"Precision measures the proportion of true positive predictions among all positive predictions made by a classifier, while accuracy measures the proportion of correct predictions (both true positives and true negatives) among all predictions made by the classifier."
What is the difference between k-fold cross-validation and leave-one-out cross-validation?,R8,K-fold cross-validation and leave-one-out give the same result if k equals the number of features.,0.2,"K-fold cross-validation divides the data into k subsets and iteratively trains the model k times, each time using a different subset as the validation set, while leave-one-out cross-validation trains the model k times, each time using all but one data point as the training set and the remaining point as the validation set."
What is the Naive Bayes classifier and how does it work?,R8,It builds decision trees using probability formulas to classify data into categories.,0.2,"The Naive Bayes classifier is a probabilistic model based on Bayes' theorem with the ""naive"" assumption of feature independence. It calculates the probability of a class given the input features using the product of conditional probabilities."
What is the difference between correlation and covariance?,R8,Correlation is derived from dividing covariance by the product of standard deviations of the variables.,0.95,"Covariance measures the degree to which two variables change together, while correlation measures both the strength and direction of the linear relationship between two variables."
What is the purpose of a confusion matrix in classification tasks?,R8,It tells us how confident the model is about each prediction.,0.3,"A confusion matrix is a table that visualizes the performance of a classification model by comparing predicted class labels with true class labels. It shows the number of true positives, true negatives, false positives, and false negatives."
What is the difference between stratified sampling and random sampling?,R8,"Stratified sampling chooses only one example from each group, while random sampling picks multiple examples randomly.",0.3,"Stratified sampling divides the population into homogeneous subgroups (strata) and samples from each stratum proportionally, while random sampling selects individuals randomly from the entire population without regard to strata."
"What is the difference between univariate, bivariate, and multivariate analysis?",R8,"Bivariate and multivariate analysis can only be done with numerical data, not categorical.",0.3,"Univariate analysis examines the distribution and summary statistics of a single variable, bivariate analysis explores the relationship between two variables, and multivariate analysis involves analyzing multiple variables simultaneously."
What is the purpose of data preprocessing in machine learning?,R8,"It ensures that the input data format matches the algorithm’s requirement, making training possible.",0.9,"Data preprocessing involves cleaning, transforming, and preparing raw data for machine learning algorithms. It includes tasks such as handling missing values, encoding categorical variables, and scaling features."
What is the bias of a statistical estimator?,R8,"Bias is only used when evaluating classification models, not estimators.",0.2,The bias of a statistical estimator measures the difference between the expected value of the estimator and the true value of the parameter being estimated. A biased estimator systematically overestimates or underestimates the true parameter value.
What is the variance of a statistical estimator?,R8,Estimator variance is how different the estimator is from the true population parameter.,0.3,"The variance of a statistical estimator measures the spread or variability of the estimator's values around its expected value. A high variance indicates that the estimator's values are scattered widely, while a low variance indicates that the values are clustered closely together."
What is the difference between parametric and non-parametric statistical tests?,R8,"Parametric tests can only be used on categorical data, while non-parametric tests are for numerical data.",0.2,"Parametric statistical tests assume that the data follows a specific probability distribution (e.g., normal distribution), while non-parametric tests make no such assumptions and are based on ranks or frequencies."
What is the purpose of feature scaling in machine learning?,R8,"Feature scaling increases the dimensionality of the dataset, which improves accuracy.",0.2,"Feature scaling ensures that all features have the same scale or range, preventing certain features from dominating others during model training. Common scaling techniques include min-max scaling and standardization."
What is the difference between correlation and causation in statistics?,R8,Correlation is a type of causation that occurs in linear regression problems.,0.3,"Correlation measures the strength and direction of the linear relationship between two variables, while causation indicates that changes in one variable directly cause changes in another variable."
What is the Central Limit Theorem and why is it important?,R8,CLT is not useful in real-world data science because most data is too messy to apply it.,0.1,"The Central Limit Theorem states that the sampling distribution of the sample mean approaches a normal distribution as the sample size increases, regardless of the shape of the population distribution. It is important because it allows us to make inferences about population parameters based on sample statistics."
What is the purpose of outlier detection in data analysis?,R8,Outlier detection is not necessary if you're using decision trees or random forests.,0.3,"Outlier detection involves identifying observations that deviate significantly from the rest of the data. Outliers can distort statistical analyses and machine learning models, so detecting and handling them appropriately is important for accurate results."
What is the difference between supervised and unsupervised outlier detection methods?,R8,"Supervised outlier detection uses clustering, while unsupervised methods use regression.",0.2,"Supervised outlier detection methods require labeled data with outlier labels, while unsupervised methods detect outliers based on the characteristics of the data itself without requiring labeled examples."
What is the difference between statistical inference and predictive modeling?,R8,Statistical inference is a type of predictive modeling that uses statistics instead of machine learning.,0.3,"Statistical inference involves drawing conclusions about a population based on a sample of data, while predictive modeling focuses on building models that make predictions about future or unseen data based on past observations."
What is the purpose of regularization in machine learning?,R8,Regularization is only used in unsupervised learning to reduce the number of clusters.,0.2,"Regularization is a technique used to prevent overfitting by adding a penalty term to the model's objective function, discouraging overly complex models. Common regularization techniques include L1 regularization (Lasso) and L2 regularization (Ridge)."
What is the difference between batch gradient descent and stochastic gradient descent?,R8,SGD stands for ‘Simple Gradient Descent’ and is just an easier version of batch gradient descent.,0.2,"Batch gradient descent updates model parameters using the average gradient of the entire training dataset, while stochastic gradient descent updates parameters using the gradient of a single randomly selected data point or mini-batch."
What is the difference between statistical modeling and machine learning?,R8,Statistical modeling and machine learning are opposites—one is mathematical and the other is completely heuristic.,0.2,"Statistical modeling focuses on understanding the relationships between variables in a probabilistic framework, while machine learning emphasizes building algorithms that can learn patterns and make predictions from data without explicitly programming them."
What is the purpose of feature selection in machine learning?,R8,The purpose of feature selection is to merge all features into one single feature to simplify training.,0.1,"Feature selection involves selecting the most relevant features from a dataset to improve model performance, reduce overfitting, and enhance interpretability."
What is the difference between regularization and feature selection in machine learning?,R8,"Feature selection is used only for linear models, while regularization works for all kinds of models.",0.3,"Regularization penalizes overly complex models by adding a penalty term to the objective function, while feature selection explicitly selects a subset of features to include in the model based on their importance or relevance."
What is the difference between grid search and random search for hyperparameter tuning?,R8,Both methods are used for tuning model structure rather than hyperparameters.,0.1,"Grid search exhaustively searches through a specified grid of hyperparameter values, while random search samples hyperparameter values randomly from predefined distributions."
What is the purpose of cross-validation in model evaluation?,R8,Cross-validation is a method to clean the data before training the model.,0.1,Cross-validation is a technique used to assess the performance of a predictive model by splitting the data into multiple subsets. It helps to estimate the model's generalization error and detect overfitting.
What is the difference between precision and recall in classification evaluation?,R8,"Precision is calculated using true positives and false negatives, and recall uses true positives and false positives.",0.2,"Precision measures the proportion of true positive predictions among all positive predictions made by a classifier, while recall measures the proportion of true positive predictions among all actual positive instances in the data."
"What is the F1 score, and why is it useful in imbalanced datasets?",R8,It’s helpful in imbalanced data because it shows a balance between catching real cases (recall) and avoiding false alarms (precision).,0.9,The F1 score is the harmonic mean of precision and recall and provides a single metric to evaluate the performance of a classifier. It is particularly useful for imbalanced datasets where one class is much more prevalent than the others.
What is the difference between bagging and boosting ensemble methods?,R8,"Bagging randomly drops features to make different models, while boosting uses dropout to regularize training.",0.2,"Bagging (Bootstrap Aggregating) builds multiple models independently and combines their predictions through averaging or voting, while boosting sequentially builds models by giving more weight to misclassified instances."
What is the purpose of principal component analysis (PCA) in dimensionality reduction?,R8,PCA clusters data points into similar groups based on distances between features.,0.2,"Principal Component Analysis (PCA) is used to reduce the dimensionality of a dataset by transforming the original features into a new set of orthogonal principal components, which capture the maximum variance in the data."
What is the difference between classification and regression in machine learning?,R8,They are both the same because they use the same algorithms like decision trees and SVMs.,0.2,"Classification is a supervised learning task where the goal is to predict categorical labels or classes, while regression is a supervised learning task where the goal is to predict continuous numerical values."
What is the purpose of natural language processing (NLP) in data science?,R8,NLP is mostly used for voice recognition and has no role in text analysis for data science.,0.2,"Natural language processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human languages. It enables computers to understand, interpret, and generate human language data."
What is the difference between bag-of-words and word embeddings in NLP?,R8,"Word embeddings randomly assign numbers to words, while bag-of-words sorts words by their length.",0.1,"Bag-of-words represents text as a collection of word counts or frequencies, ignoring word order and grammar, while word embeddings represent words as dense vectors in a continuous vector space, capturing semantic relationships between words."
What is the difference between sentiment analysis and topic modeling in NLP?,R8,Topic modeling and sentiment analysis are both only useful for analyzing tweets and cannot be applied to other text formats.,0.1,"Sentiment analysis determines the sentiment or opinion expressed in a piece of text, such as positive, negative, or neutral, while topic modeling identifies the underlying topics or themes present in a collection of documents."
What is the purpose of word2vec in NLP?,R8,Word2Vec converts entire paragraphs into a single vector to help summarize the text.,0.2,Word2Vec is a model that learns word embeddings by predicting the surrounding words in a text corpus. It captures semantic relationships between words and enables algorithms to process textual data more effectively.
What is the difference between a generative model and a discriminative model in machine learning?,R8,"Generative models do not require labels, but discriminative models do.",0.3,"A generative model learns the joint probability distribution of the input features and the target labels, while a discriminative model learns the conditional probability distribution of the target labels given the input features."
What is the difference between deep learning and traditional machine learning algorithms?,R8,"Deep learning models cannot be used for classification tasks, but traditional ML can.",0.1,"Deep learning involves neural networks with multiple layers that can learn hierarchical representations of the data, while traditional machine learning algorithms typically use handcrafted features and simpler models."
What is the purpose of transfer learning in deep learning?,R8,"It is used only for language tasks, not for images or audio.",0.1,"Transfer learning is a technique where a model trained on one task is reused as the starting point for a model on a related task. It enables the transfer of knowledge and features learned from one domain to another, reducing the need for large labeled datasets."
What is the difference between convolutional neural networks (CNNs) and recurrent neural networks (RNNs)?,R8,"CNNs use recurrent layers to store temporal information, while RNNs use dense layers to detect patterns in images.",0.2,"Convolutional neural networks (CNNs) are well-suited for processing grid-like data such as images, while recurrent neural networks (RNNs) are designed to handle sequential data with temporal dependencies such as text or time series."
What is the purpose of batch normalization in neural networks?,R8,Batch normalization randomly resets weights during training to make learning harder and improve robustness.,0.1,Batch normalization is a technique used in neural networks to improve training stability and speed by normalizing the activations of each layer. It reduces internal covariate shift and accelerates convergence during training.
What is the difference between a feedforward neural network and a recurrent neural network?,R8,"Feedforward networks use loops and memory cells, unlike RNNs which are strictly one-directional.",0.1,"A feedforward neural network consists of multiple layers of neurons connected in a forward direction without cycles, while a recurrent neural network has connections that form directed cycles, allowing it to maintain state or memory over time."
What is the difference between LSTMs and GRUs in recurrent neural networks?,R8,"GRUs forget everything after each training epoch, but LSTMs remember all training history.",0.1,"LSTMs (Long Short-Term Memory) and GRUs (Gated Recurrent Units) are both types of recurrent neural network architectures designed to capture long-range dependencies in sequential data, but they differ in their internal gating mechanisms and computational complexity."
What is the purpose of attention mechanisms in neural networks?,R8,Attention is only used in convolutional networks to filter images.,0.1,Attention mechanisms in neural networks enable the model to focus on relevant parts of the input data while ignoring irrelevant information. They have been particularly successful in natural language processing tasks such as machine translation and text summarization.
What is the difference between generative adversarial networks (GANs) and autoencoders in deep learning?,R8,"GANs are only used for image compression, while autoencoders generate completely new samples.",0.1,"Generative adversarial networks (GANs) consist of two neural networks (generator and discriminator) that compete with each other to generate realistic data samples, while autoencoders consist of an encoder and decoder network that reconstruct input data."
What is the purpose of reinforcement learning in machine learning?,R8,Reinforcement learning is a type of supervised learning where data is labeled with the best possible move.,0.2,Reinforcement learning is a machine learning paradigm where an agent learns to make decisions by interacting with an environment to maximize cumulative rewards. It is commonly used in settings where actions influence future states and the agent learns through trial and error.
What is the difference between Q-learning and policy gradient methods in reinforcement learning?,R8,Both methods are identical except that Q-learning uses rewards and policy gradients do not.,0.1,"Q-learning is a model-free reinforcement learning algorithm that learns the optimal action-selection policy by iteratively updating a Q-value function, while policy gradient methods directly parameterize the policy and update the policy parameters based on gradients of expected rewards."
What is the purpose of data preprocessing in exploratory data analysis (EDA)?,R8,"The only step in preprocessing is scaling, which makes all values equal in the dataset.",0.1,"Data preprocessing in EDA involves cleaning, transforming, and preparing raw data for analysis. It includes tasks such as handling missing values, removing outliers, and encoding categorical variables."
What are some common techniques for handling missing data in EDA?,R8,Missing data must be handled by adding random values so the dataset looks complete.,0.1,"Common techniques for handling missing data in EDA include imputation (replacing missing values with estimates), deletion (removing rows or columns with missing values), and prediction (using models to estimate missing values)."
What is the difference between univariate and bivariate analysis in EDA?,R8,"Bivariate analysis focuses on one variable at a time, but it’s visualized in two different ways.",0.2,"Univariate analysis examines the distribution and summary statistics of a single variable, while bivariate analysis explores the relationship between two variables."
What is the purpose of data visualization in EDA?,R8,Visualization is used in EDA to remove duplicate values by highlighting them visually.,0.2,"Data visualization in EDA helps to explore and understand the structure, patterns, and relationships in the data. It provides insights that may not be apparent from raw data or summary statistics alone."
What are some common types of data visualization used in EDA?,R8,3D bar charts are required in EDA because they provide more depth and are always easier to read.,0.1,"Common types of data visualization used in EDA include histograms, scatter plots, box plots, bar charts, line plots, and heatmaps."
What is the difference between continuous and categorical variables in EDA?,R8,"Continuous variables are used for counting things, and categorical variables are used for measuring size.",0.1,"Continuous variables are numeric variables with an infinite number of possible values within a range, while categorical variables are qualitative variables with a finite number of distinct categories or levels."
What is the purpose of statistical summaries in EDA?,R8,Their goal is mainly to hide errors in the dataset by averaging everything out.,0.1,"Statistical summaries in EDA provide descriptive statistics such as mean, median, mode, variance, and percentiles to summarize the central tendency, dispersion, and shape of the data distribution."
What is the difference between central tendency and dispersion measures in EDA?,R8,Dispersion and central tendency are not used together; they are applied to completely different types of data.,0.1,"Central tendency measures (e.g., mean, median, mode) describe the central or typical value of a dataset, while dispersion measures (e.g., variance, standard deviation) quantify the spread or variability of the data around the central value."
What is the purpose of hypothesis testing in EDA?,R8,We use hypothesis testing only when the data has missing values or duplicate entries.,0.1,Hypothesis testing in EDA is used to evaluate whether observed differences or relationships in the data are statistically significant or due to random chance. It involves formulating null and alternative hypotheses and conducting statistical tests to make inferences about population parameters.
What is the difference between parametric and non-parametric tests in hypothesis testing?,R8,There is no real difference; both tests give the same results regardless of the data type.,0.1,"Parametric tests assume that the data follows a specific probability distribution (e.g., normal distribution), while non-parametric tests make no such assumptions and are based on ranks or frequencies."
What is the purpose of correlation analysis in EDA?,R8,Correlation analysis deletes columns with low correlation values automatically.,0.2,Correlation analysis in EDA measures the strength and direction of the linear relationship between two continuous variables. It helps to identify patterns and associations in the data.
What is the difference between Pearson correlation and Spearman correlation?,R8,"Pearson is a non-parametric test, while Spearman is parametric, so they are used differently.",0.2,"Pearson correlation measures the linear relationship between two continuous variables, while Spearman correlation measures the monotonic relationship between variables, allowing for non-linear associations."
What is the purpose of regression analysis in EDA?,R8,Regression analysis in EDA is used to measure how far data points are from the origin.,0.2,Regression analysis in EDA is used to model the relationship between one or more independent variables (predictors) and a dependent variable (outcome). It helps to understand how changes in predictors affect the outcome variable.
What are the assumptions of linear regression in EDA?,R8,One assumption is that you must have more dependent variables than independent ones.,0.1,"The assumptions of linear regression in EDA include linearity (relationship between variables is linear), independence (residuals are independent), homoscedasticity (residuals have constant variance), and normality (residuals are normally distributed)."
What is the difference between simple linear regression and multiple linear regression?,R8,Simple regression uses fewer data points and multiple regression uses a larger dataset with more rows and columns. That’s the main difference.,0.2,"Simple linear regression models the relationship between one independent variable and a dependent variable, while multiple linear regression models the relationship between multiple independent variables and a dependent variable."
What is the purpose of logistic regression in EDA?,R8,The main purpose is to convert text features into numerical values for better visualization during EDA.,0.2,Logistic regression in EDA is used to model the relationship between one or more independent variables (predictors) and a binary outcome variable (0 or 1). It is commonly used for classification tasks.
What are the assumptions of logistic regression in EDA?,R8,"Logistic regression assumes that the features must be scaled between 0 and 1, otherwise it cannot learn from the data correctly.",0.2,"The assumptions of logistic regression in EDA include linearity (relationship between predictors and log-odds is linear), independence (observations are independent), and absence of multicollinearity (predictors are not highly correlated)."
What is the purpose of outlier detection in EDA?,R8,"Outlier detection in EDA creates summary statistics like mean and median. Without it, you can’t calculate averages.",0.2,"Outlier detection in EDA involves identifying observations that deviate significantly from the rest of the data. Outliers can distort statistical analyses and machine learning models, so detecting and handling them appropriately is important for accurate results."
What are some common techniques for detecting outliers in EDA?,R8,The only way to detect outliers is using deep learning. Traditional methods like plots or Z-scores are outdated.,0.1,"Common techniques for detecting outliers in EDA include graphical methods (e.g., scatter plots, box plots), statistical methods (e.g., z-score, modified z-score), and proximity-based methods (e.g., nearest neighbors)."
What is the purpose of box plots in EDA?,R8,"In EDA, box plots are used to remove null values and normalize the dataset before building a model.",0.1,"Box plots in EDA provide a visual summary of the distribution, central tendency, and spread of a dataset. They show the median, quartiles, and potential outliers of the data distribution."
What is the purpose of histograms in EDA?,R8,The histogram is a tool for finding missing values. Any empty bar represents a missing entry in the dataset.,0.1,Histograms in EDA display the frequency distribution of a continuous variable by dividing the data into intervals (bins) and plotting the number of observations in each interval. They provide insights into the shape and spread of the data distribution.
What is the purpose of scatter plots in EDA?,R8,Scatter plots are a kind of histogram that groups data into intervals along both axes to summarize the dataset.,0.2,"Scatter plots in EDA visualize the relationship between two continuous variables by plotting each data point as a point on a graph. They help to identify patterns, trends, and correlations in the data."
What is the purpose of bar charts in EDA?,R8,Bar charts are primarily used to detect outliers in numerical data by highlighting bars that are much taller or shorter than others.,0.2,Bar charts in EDA represent categorical data by displaying the frequency or proportion of observations in each category as bars of varying heights. They are useful for comparing the distribution of categorical variables.
What is the purpose of line plots in EDA?,R8,Line plots are useful for displaying the frequency of different words in text data. Each line represents one word's usage.,0.2,Line plots in EDA visualize the trend or pattern of a variable over time or another continuous dimension by connecting data points with lines. They are commonly used for time series analysis and trend identification.
What is the purpose of heatmaps in EDA?,R8,"Heatmaps display regression lines between all variables, helping identify how each feature contributes to the target variable.",0.2,Heatmaps in EDA visualize the magnitude of a variable or the strength of a relationship between variables using colors. They are useful for identifying patterns and correlations in large datasets.
What is the purpose of correlation matrices in EDA?,R8,They are mainly used to scale data features to the same range before applying machine learning algorithms.,0.2,Correlation matrices in EDA display the pairwise correlations between variables in a tabular format or heatmap. They help to identify patterns and associations among variables.
What is the difference between correlation and causation in EDA?,R8,"Causation is about finding missing values in the data, while correlation deals with null values only.",0.1,"Correlation measures the strength and direction of the linear relationship between two variables, while causation indicates that changes in one variable directly cause changes in another variable. Correlation does not imply causation."
What is the purpose of statistical tests in EDA?,R8,The purpose of statistical tests is to train machine learning models faster by removing features that are too correlated.,0.1,Statistical tests in EDA are used to assess the significance of observed differences or relationships in the data. They help to make inferences about population parameters and draw conclusions from sample data.
What is the difference between parametric and non-parametric tests in EDA?,R8,"Parametric tests are used to create models, while non-parametric tests are used to clean missing values from the dataset.",0.1,"Parametric tests assume that the data follows a specific probability distribution (e.g., normal distribution), while non-parametric tests make no such assumptions and are based on ranks or frequencies."
What is the purpose of ANOVA in EDA?,R8,ANOVA is used to cluster variables into groups based on how similar their distributions are.,0.2,Analysis of Variance (ANOVA) in EDA is used to compare the means of two or more groups to determine whether they are significantly different. It is commonly used for comparing means across multiple categories or treatments.
What is the difference between one-way ANOVA and two-way ANOVA in EDA?,R8,"Two-way ANOVA is mainly used to visualize data with two variables by plotting bar charts, unlike one-way ANOVA.",0.2,"One-way ANOVA compares the means of two or more groups based on a single categorical factor, while two-way ANOVA considers the effects of two categorical factors (main effects and interaction) on the outcome variable."
What is the purpose of chi-squared test in EDA?,R8,Chi-squared is used to fill missing data values automatically in categorical columns during EDA.,0.1,The chi-squared test in EDA is used to assess the independence between two categorical variables by comparing the observed frequencies to the expected frequencies under the null hypothesis of independence.
What is the purpose of t-tests in EDA?,R8,T-tests in EDA are used to cluster similar data points based on their mean values.,0.1,T-tests in EDA are used to compare the means of two independent samples or the mean of a sample to a known value. They help to determine whether observed differences in means are statistically significant.
What is the difference between independent samples t-test and paired samples t-test in EDA?,R8,"Independent t-tests require the data to be normally distributed, but paired t-tests do not require this assumption.",0.3,"Independent samples t-test compares the means of two independent groups, while paired samples t-test compares the means of two related groups (e.g., before and after treatment) based on matched pairs of observations."
What is the purpose of non-parametric tests in EDA?,R8,Non-parametric tests are used to scale numerical data between 0 and 1 before model training.,0.2,"Non-parametric tests in EDA are used when the data does not meet the assumptions of parametric tests (e.g., normal distribution, homogeneity of variance). They provide robust alternatives for analyzing non-normally distributed or skewed data."
What is the difference between Wilcoxon signed-rank test and Mann-Whitney U test in EDA?,R8,"Wilcoxon test is mainly used to identify outliers, while Mann-Whitney U test is used to clean missing data.",0.1,"Wilcoxon signed-rank test is a non-parametric test used to compare the medians of two related groups, while Mann-Whitney U test compares the medians of two independent groups."
What is the purpose of exploratory factor analysis (EFA) in EDA?,R8,EFA can identify irrelevant features by analyzing which variables do not load significantly on any factor.,0.9,Exploratory factor analysis (EFA) in EDA is used to identify underlying factors or latent variables that explain the common variance among observed variables. It helps to uncover the underlying structure of a dataset.
What is the difference between EFA and principal component analysis (PCA) in EDA?,R8,EFA and PCA serve the same purpose and can be used interchangeably in EDA without any difference.,0.1,"EFA and PCA are both dimensionality reduction techniques used in EDA, but EFA aims to identify underlying factors that explain the common variance among observed variables, while PCA aims to capture the maximum variance in the data with orthogonal principal components."
What is the purpose of cluster analysis in EDA?,R8,The main purpose of clustering is to detect outliers by isolating unusual points as separate clusters.,0.6,Cluster analysis in EDA is used to identify groups or clusters of similar observations based on their characteristics or features. It helps to uncover natural groupings in the data and identify patterns or relationships.
What are some common algorithms used for cluster analysis in EDA?,R8,"K-means is fast but sensitive to initial centroids, while hierarchical clustering provides more detailed cluster relationships but is slower.",0.95,"Common algorithms used for cluster analysis in EDA include K-means clustering, hierarchical clustering, DBSCAN (Density-Based Spatial Clustering of Applications with Noise), and Gaussian mixture models."
What is the difference between K-means clustering and hierarchical clustering in EDA?,R8,"K-means clustering is a supervised learning method, while hierarchical clustering is unsupervised.",0.1,"K-means clustering partitions the data into K clusters by minimizing the within-cluster sum of squares, while hierarchical clustering builds a tree-like hierarchy of clusters by recursively merging or splitting clusters based on proximity."
What is the purpose of dendrogram in hierarchical clustering in EDA?,R8,They replace scatter plots in EDA for visualizing numerical data distributions.,0.2,A dendrogram in hierarchical clustering visualizes the hierarchical relationships between clusters by displaying the merging order and distance between clusters. It helps to identify the optimal number of clusters and interpret the clustering results.
What is the purpose of silhouette analysis in cluster validation in EDA?,R8,It calculates the correlation between clusters to improve supervised learning models.,0.2,"Silhouette analysis in cluster validation assesses the quality of clustering results by calculating silhouette coefficients for each data point, which measure the cohesion within clusters and separation between clusters. Higher silhouette coefficients indicate better-defined clusters."
What is the purpose of dimensionality reduction techniques in EDA?,R8,These methods are mainly used to clean data by removing outliers and errors before modeling.,0.2,Dimensionality reduction techniques in EDA are used to reduce the number of features or variables in a dataset while preserving the most important information. They help to overcome the curse of dimensionality and improve computational efficiency.
What are some common dimensionality reduction techniques used in EDA?,R8,t-SNE helps in visualizing clusters in high-dimensional datasets by projecting data into 2D or 3D space.,0.95,"Common dimensionality reduction techniques used in EDA include principal component analysis (PCA), factor analysis, t-distributed stochastic neighbor embedding (t-SNE), and linear discriminant analysis (LDA)."
What is the purpose of PCA in EDA?,R8,It transforms categorical variables into numerical form for better visualization.,0.1,Principal Component Analysis (PCA) in EDA is used to reduce the dimensionality of a dataset by transforming the original features into a new set of orthogonal principal components. It helps to identify the underlying structure and patterns in high-dimensional data.
What is the difference between PCA and factor analysis in EDA?,R8,"Factor analysis is primarily used for classification, whereas PCA is for regression tasks.",0.1,"PCA and factor analysis are both dimensionality reduction techniques used in EDA, but PCA aims to capture the maximum variance in the data with orthogonal principal components, while factor analysis aims to identify underlying factors that explain the common variance among observed variables."
What is the purpose of t-SNE in EDA?,R8,t-SNE is mainly useful for very small datasets because it is computationally expensive for large datasets.,0.7,t-Distributed Stochastic Neighbor Embedding (t-SNE) in EDA is a technique used for visualizing high-dimensional data in a lower-dimensional space while preserving the local structure and relationships between data points. It is commonly used for visualizing clusters and patterns in complex datasets.
What is the difference between linear discriminant analysis (LDA) and PCA in EDA?,R8,LDA is used to visualize clusters after PCA has been applied to the data.,0.3,"Linear Discriminant Analysis (LDA) and PCA are both dimensionality reduction techniques used in EDA, but LDA aims to maximize the separation between classes or groups while reducing dimensionality, whereas PCA focuses on capturing the maximum variance in the data regardless of class labels."
What is the purpose of feature selection in EDA?,R8,Feature selection is used to detect outliers and missing values in datasets during EDA.,0.1,"Feature selection in EDA involves selecting the most relevant features from a dataset to improve model performance, reduce overfitting, and enhance interpretability. It helps to identify the subset of features that contribute the most to predicting the target variable."
What are some common feature selection techniques used in EDA?,R8,Random forests select features by randomly choosing variables and splitting nodes based on information gain.,0.85,"Common feature selection techniques used in EDA include filter methods (e.g., correlation analysis, information gain), wrapper methods (e.g., recursive feature elimination, forward/backward selection), and embedded methods (e.g., LASSO, decision trees)."
What is the purpose of ensemble methods in EDA?,R8,Ensemble methods are mainly used to clean data by removing outliers during EDA.,0.1,"Ensemble methods in EDA combine multiple models or algorithms to improve predictive performance, robustness, and generalization. They help to reduce overfitting and capture diverse patterns in the data."
What are some common ensemble methods used in EDA?,R8,Boosting methods like XGBoost and Gradient Boosting are popular for handling large datasets.,0.9,"Common ensemble methods used in EDA include bagging (Bootstrap Aggregating), boosting (e.g., AdaBoost, Gradient Boosting), and stacking (meta-learning). They combine predictions from multiple base models to make more accurate and reliable predictions."
What is the purpose of model evaluation in EDA?,R8,Evaluation allows tuning hyperparameters to improve the model’s predictive power.,0.85,Model evaluation in EDA involves assessing the performance of predictive models using various metrics and techniques. It helps to determine the effectiveness of the models in capturing patterns and making accurate predictions on unseen data.
What are some common metrics used for model evaluation in EDA?,R8,Precision and recall help evaluate how well a model handles imbalanced datasets.,0.9,"Common metrics used for model evaluation in EDA include accuracy, precision, recall, F1 score, ROC-AUC score, mean squared error (MSE), and R-squared (R2) score. They provide insights into different aspects of model performance."
What is the difference between cross-validation and holdout validation in model evaluation in EDA?,R8,Holdout validation is a form of cross-validation where only two folds are used.,0.6,"Cross-validation in model evaluation splits the data into multiple subsets (folds) for training and testing, while holdout validation splits the data into two disjoint sets (training and testing). Cross-validation provides more reliable estimates of model performance but is computationally more expensive."
What is the purpose of hyperparameter tuning in model evaluation in EDA?,R8,It helps generate new features by modifying existing ones to fit the model better.,0.2,Hyperparameter tuning in model evaluation involves optimizing the hyperparameters of machine learning algorithms to improve model performance and generalization. It helps to find the best combination of hyperparameters for a given dataset and problem.
What are some common techniques for hyperparameter tuning in EDA?,R8,Tuning is usually done by increasing the number of epochs until the model overfits.,0.3,"Common techniques for hyperparameter tuning in EDA include grid search, random search, Bayesian optimization, and genetic algorithms. They systematically search the hyperparameter space to find the optimal values for model performance."
What is the purpose of model interpretation in EDA?,R8,"Model interpretation during EDA also helps detect bias or unfair influence of certain features on predictions, supporting ethical analysis.",0.85,"Model interpretation in EDA involves understanding and explaining how predictive models make decisions and predictions. It helps to gain insights into the underlying patterns, relationships, and features that contribute to the model's predictions."
What are some common techniques for model interpretation in EDA?,R8,Confusion matrices are model interpretation tools that explain why each prediction was made for individual data points.,0.3,"Common techniques for model interpretation in EDA include feature importance analysis (e.g., permutation importance, SHAP values), partial dependence plots, individual conditional expectation (ICE) plots, and model-agnostic interpretation methods (e.g., LIME, SHAP)."
What is the purpose of model deployment in EDA?,R8,Deployment in the context of EDA refers to automating the analysis process so that every new dataset is explored using predefined rules.,0.5,Model deployment in EDA involves deploying predictive models into production environments to make real-time predictions on new data. It allows organizations to leverage the insights and predictions generated from data analysis for decision-making and business applications.
What are some common techniques for model deployment in EDA?,R8,"During EDA, we deploy models using Excel plug-ins that can embed trained models into spreadsheets for ongoing analysis.",0.4,"Common techniques for model deployment in EDA include deploying models as web services or APIs, embedding models into applications or dashboards, and integrating models with existing business processes or workflows. They enable seamless integration of predictive models into operational systems and workflows."
What are the common types of regularization used in machine learning?,R8,"L1 regularization adds the sum of weights to the loss function, while L2 adds the squared sum of weights as a penalty.",0.95,"Common types of regularization used in machine learning include L1 regularization (Lasso), L2 regularization (Ridge), and elastic net regularization (combination of L1 and L2 penalties)."
What is the bias-variance trade-off in machine learning?,R8,"The trade-off refers to the challenge of minimizing bias and variance simultaneously, which is often not possible, so a balance is needed.",1.0,"The bias-variance trade-off refers to the balance between bias (error due to overly simplistic assumptions) and variance (error due to sensitivity to fluctuations in the training data) in machine learning models. Reducing bias typically increases variance, and vice versa."
What are the common types of cross-validation techniques used in machine learning?,R8,"K-fold cross-validation is used for categorical data, while LOOCV is used only for numeric data.",0.25,"Common types of cross-validation techniques used in machine learning include k-fold cross-validation, stratified k-fold cross-validation, leave-one-out cross-validation, and nested cross-validation."
What is the purpose of hyperparameter tuning in machine learning?,R8,We tune hyperparameters mainly for unsupervised models like DBSCAN because supervised ones don’t need them.,0.2,Hyperparameter tuning in machine learning involves optimizing the hyperparameters of models to improve their performance and generalization. It helps to find the best combination of hyperparameters for a given dataset and problem.
What are some common techniques for hyperparameter tuning in machine learning?,R8,Dropout and L1 regularization are hyperparameter tuning techniques used mostly for ensemble learning.,0.2,"Common techniques for hyperparameter tuning in machine learning include grid search, random search, Bayesian optimization, genetic algorithms, and gradient-based optimization methods. They systematically search the hyperparameter space to find optimal values."
What is the difference between supervised and unsupervised learning in machine learning?,R8,"Unsupervised learning is mainly used when the data is clean and labeled, whereas supervised learning is used for messy, unstructured data.",0.25,"Supervised learning involves learning from labeled data with input-output pairs, where the model predicts the output variable based on input features. Unsupervised learning involves learning from unlabeled data to discover hidden patterns or structures in the data."
What are some common dimensionality reduction techniques used in machine learning?,R8,We can apply decision trees to remove less important dimensions and keep only useful features automatically.,0.4,"Common dimensionality reduction techniques used in machine learning include principal component analysis (PCA), linear discriminant analysis (LDA), t-distributed stochastic neighbor embedding (t-SNE), and autoencoders."
What is the purpose of model interpretation in machine learning?,R8,"Interpretation is used only for linear models, because nonlinear ones are too complex to interpret.",0.3,"Model interpretation in machine learning involves understanding and explaining how predictive models make decisions and predictions. It helps to gain insights into the underlying patterns, relationships, and features that contribute to the model's predictions."
What are some common techniques for model interpretation in machine learning?,R8,Model interpretation is done using t-tests and ANOVA to compare predictions across groups.,0.25,"Common techniques for model interpretation in machine learning include feature importance analysis (e.g., permutation importance, SHAP values), partial dependence plots, individual conditional expectation (ICE) plots, and model-agnostic interpretation methods (e.g., LIME, SHAP)."
What is the purpose of model deployment in machine learning?,R8,Deployment involves setting up APIs or interfaces so that other systems can send data and receive predictions from the model.,0.95,Model deployment in machine learning involves deploying predictive models into production environments to make real-time predictions on new data. It allows organizations to leverage the insights and predictions generated from data analysis for decision-making and business applications.
What are some common techniques for model deployment in machine learning?,R8,"To deploy a model, we just save it using pickle and send it via email to users.",0.3,"Common techniques for model deployment in machine learning include deploying models as web services or APIs, embedding models into applications or dashboards, and integrating models with existing business processes or workflows. They enable seamless integration of predictive models into operational systems and workflows."
What is the purpose of transfer learning in machine learning?,R8,Transfer learning trains models from scratch but copies the test results from other similar tasks to improve output.,0.2,"Transfer learning in machine learning is a technique where a model trained on one task is reused as the starting point for a model on a related task. It enables the transfer of knowledge and features learned from one domain to another, reducing the need for large labeled datasets."
What are some common applications of transfer learning in machine learning?,R8,The main application of transfer learning is in video editing tools that automatically color grade scenes using previously learned aesthetics.,0.3,"Common applications of transfer learning in machine learning include image classification, natural language processing (NLP), and speech recognition. Pre-trained models such as BERT, ResNet, and GPT are often used as starting points for transfer learning tasks."
What is the purpose of feature engineering in machine learning?,R8,"It allows us to convert categorical data into numerical formats, handle missing values, and scale features properly.",0.95,"Feature engineering involves selecting, transforming, or creating new features from the raw data to improve the performance of machine learning models by making them more expressive, informative, and suitable for the task at hand."
What are some common techniques for feature engineering?,R8,"A popular method is to delete features with low variance, as they don’t add much predictive power.",0.9,"Common techniques include one-hot encoding for categorical variables, scaling numerical features, creating interaction terms, binning or discretizing continuous variables, and extracting features from text or images using techniques like TF-IDF or CNNs."
What is the difference between batch processing and real-time processing?,R8,"Real-time processing stores data in files and processes them once a day, unlike batch which uses live streams.",0.2,"Batch processing involves processing data in large, discrete batches or groups, typically on a scheduled or periodic basis, while real-time processing involves handling data as soon as it is generated or received, often requiring low-latency and immediate responses."
What is transfer learning in NLP?,R8,"Transfer learning is only used in speech-to-text systems, not for any written NLP tasks.",0.2,"Transfer learning in NLP involves leveraging pre-trained language models, such as BERT or GPT, trained on large corpora of text data, and fine-tuning them on specific tasks or domains with smaller datasets to achieve better performance and faster convergence."
What is named entity recognition (NER)?,R8,The goal of NER is to detect emotions like happiness or sadness from the given sentence.,0.2,"Named entity recognition is a subtask of information extraction that involves identifying and classifying named entities (e.g., persons, organizations, locations) mentioned in text documents, which is useful for tasks such as entity linking and relation extraction."
What are some challenges in working with unstructured data?,R8,"Unstructured data is always numerical, so it can be analyzed easily using basic statistics only.",0.2,"Challenges include extracting meaningful information from unstructured text, handling noisy or inconsistent data, dealing with large volumes of data, and ensuring privacy and security when working with sensitive information."
What is collaborative filtering in recommendation systems?,R8,"Collaborative filtering is used only in social media to recommend new friends, not in shopping apps.",0.2,"Collaborative filtering is a technique used in recommendation systems to generate personalized recommendations by analyzing similarities and patterns in users' behavior or preferences, often based on user-item interactions or user-user similarity."
What are some evaluation metrics used in recommendation systems?,R8,"In recommendation systems, we mostly use BLEU and ROUGE scores like in NLP tasks.",0.2,"Evaluation metrics include precision, recall, F1 score, Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and ranking-based metrics like Mean Average Precision (MAP) and Normalized Discounted Cumulative Gain (NDCG)."
What is the Markov Decision Process (MDP)?,R8,Markov Decision Process is a type of neural network used for classification problems.,0.2,"The Markov Decision Process is a mathematical framework used to model sequential decision-making problems, where an agent takes actions in an environment to maximize cumulative rewards, while satisfying the Markov property (future states depend only on the current state and action)."
What is policy gradient in reinforcement learning?,R8,Policy gradients are only used when Q-learning fails and not in general reinforcement learning tasks.,0.3,"Policy gradient methods are a class of reinforcement learning algorithms that directly optimize the policy (the agent's strategy or behavior) by estimating gradients of expected rewards with respect to the policy parameters, typically using techniques like stochastic gradient ascent."
What is data augmentation in deep learning?,R8,It involves duplicating training data without any changes to balance the dataset.,0.3,"Data augmentation is a technique used to artificially increase the size and diversity of training datasets by applying transformations such as rotation, translation, scaling, cropping, or flipping to input data, which can improve model generalization and robustness."
What is sequence-to-sequence learning?,R8,Sequence-to-sequence learning is only used for image recognition problems and cannot handle text data.,0.2,"Sequence-to-sequence learning is a type of model architecture used for tasks involving input and output sequences of variable lengths, such as machine translation, summarization, and speech recognition, typically implemented using recurrent neural networks (RNNs) or transformers."
What is attention mechanism in deep learning?,R8,Attention is only used in CNNs to sharpen image edges using filters.,0.2,"Attention mechanism is a mechanism used in neural networks to selectively focus on relevant parts of the input data while processing sequences, enabling the model to learn to weigh different input elements dynamically and attend to the most informative parts."
What are some common optimization algorithms used in deep learning?,R8,"Backpropagation is the only optimization algorithm, and others like Adam or RMSprop are used just for debugging.",0.2,"Common optimization algorithms include stochastic gradient descent (SGD), Adam, RMSprop, and Adagrad, each with different update rules and learning rate adaptation strategies to optimize the model parameters efficiently."
What is semi-supervised learning?,R8,This technique randomly deletes labels from the dataset to simulate noise and confusion for the model.,0.2,"Semi-supervised learning is a machine learning paradigm that combines labeled and unlabeled data to improve model performance, often by leveraging the inherent structure or relationships in the data to semi-supervised learning algorithms."
What is self-supervised learning?,R8,Self-supervised learning is just another name for supervised learning with human annotations.,0.2,"Self-supervised learning is a form of unsupervised learning where the model learns to predict or generate certain properties or features of the input data itself, without requiring external labels or annotations, often used as a pretraining step for downstream tasks."
What is the Bellman equation in reinforcement learning?,R8,The Bellman equation is mainly used in supervised learning to adjust weights using labeled data.,0.2,The Bellman equation is a fundamental equation in dynamic programming and reinforcement learning that expresses the value of a state or state-action pair in terms of the expected immediate reward and the value of the next state or next state-action pair.
What is the difference between on-policy and off-policy learning in reinforcement learning?,R8,"In on-policy learning, the agent ignores rewards and only focuses on the states.",0.2,"On-policy learning involves learning the value or policy while following the current policy, while off-policy learning involves learning the value or policy while following a different behavior policy, often leading to more efficient exploration and better sample efficiency."
What are the main components of the data science process?,R8,The main component is to run deep learning models on any dataset without cleaning it.,0.2,"The main components include data collection, data cleaning and preprocessing, exploratory data analysis, modeling, evaluation, and deployment."
What is overfitting in machine learning?,R8,"Overfitting means the model is trained with too little data, so it cannot learn any patterns.",0.3,Overfitting occurs when a model learns the details and noise in the training data to the extent that it negatively impacts its performance on unseen data.
Explain the curse of dimensionality.,R8,The curse of dimensionality is a myth and has no effect on real-world machine learning problems.,0.1,"The curse of dimensionality refers to the increased difficulty of analyzing and processing data as the number of features or dimensions increases, leading to sparsity and computational challenges."
What is the purpose of cross-validation?,R8,It’s a way to validate the model's performance without needing a separate test set.,0.95,"Cross-validation is used to assess the generalization performance of a model by partitioning the data into multiple subsets, training the model on some subsets, and evaluating it on the remaining subsets."
How does regularization prevent overfitting in neural networks?,R8,Dropout is a regularization method that randomly disables neurons during training to prevent overfitting.,0.95,"Regularization techniques such as L1 and L2 regularization add penalty terms to the neural network's loss function, which discourages overly complex weight configurations and helps prevent overfitting."
What is the purpose of activation functions in neural networks?,R8,They convert outputs into probabilities for classification tasks.,0.9,"Activation functions introduce nonlinearity to neural networks, allowing them to learn complex patterns and relationships in the data by transforming the input signal from each neuron into an output signal."
What is the purpose of dimensionality reduction techniques?,R8,They help improve computational efficiency and make data easier to interpret.,0.95,"Dimensionality reduction techniques are used to reduce the number of features or dimensions in a dataset while preserving its important information, which can help improve the performance and efficiency of machine learning algorithms."
Explain the difference between PCA and t-SNE.,R8,"PCA works by finding the directions of maximum variance, while t-SNE models the probability distributions of points to maintain similarities.",1.0,"PCA (Principal Component Analysis) is a linear dimensionality reduction technique that seeks to maximize variance, while t-SNE (t-Distributed Stochastic Neighbor Embedding) is a nonlinear technique that focuses on preserving local relationships between data points."
What is K-means clustering?,R8,The algorithm clusters data by maximizing the distance between data points within the same cluster.,0.3,"K-means clustering is an unsupervised learning algorithm used to partition a dataset into K clusters based on similarities in the data points' features, with the goal of minimizing the within-cluster sum of squares."
What is the elbow method used for in K-means clustering?,R8,The elbow method shows which data points are outliers in clustering.,0.2,"The elbow method is used to determine the optimal number of clusters (K) in K-means clustering by plotting the within-cluster sum of squares against the number of clusters and selecting the point where the rate of decrease sharply changes (the ""elbow"" point)."
What is outlier detection?,R8,Outlier detection predicts the future values based on extreme data points.,0.2,"Outlier detection is the process of identifying data points or observations that deviate significantly from the rest of the dataset, which may indicate errors, anomalies, or interesting patterns."
What are the advantages of deep learning over traditional machine learning algorithms?,R8,"Traditional machine learning models work better when the dataset is massive, and deep learning is only good for small structured data.",0.2,"Deep learning models, particularly neural networks, can automatically learn hierarchical representations of data, handle large and complex datasets, and often achieve state-of-the-art performance in tasks such as image recognition and natural language processing."
What is backpropagation?,R8,"It’s a process of adjusting weights in reverse order using the derivative of the loss with respect to each weight, helping minimize error.",1.0,Backpropagation is a supervised learning algorithm used to train neural networks by iteratively adjusting the weights of connections between neurons in the network based on the error between predicted and actual outputs.
How do you evaluate the performance of a regression model?,R8,The F1 score is the best metric for evaluating regression because it combines precision and recall in one number.,0.1,"Performance metrics for regression models include Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), R-squared (coefficient of determination), and others."
What is the purpose of the A/B test in data science?,R8,"It’s commonly used in web analytics to test different UI designs, button colors, or headlines to find the one users respond to best.",1.0,"A/B testing is a statistical hypothesis testing method used to compare two or more versions of a product, webpage, or other elements to determine which one performs better in terms of predefined metrics."
What is Bayesian inference?,R8,The key idea is to use a prior distribution and update it using the likelihood of new data to get the posterior distribution.,1.0,"Bayesian inference is a statistical approach that uses Bayes' theorem to update the probability of a hypothesis or belief based on new evidence or observations, incorporating prior knowledge and uncertainty into the analysis."
What is the purpose of hypothesis testing?,R8,The purpose of hypothesis testing is mostly to clean and preprocess data before training machine learning models.,0.1,"Hypothesis testing is used to make inferences or decisions about a population parameter based on sample data, by testing a null hypothesis against an alternative hypothesis using statistical methods."
What is regularization in linear regression?,R8,It increases model complexity to better fit training data by adding more features and parameters.,0.2,"Regularization in linear regression involves adding a penalty term to the ordinary least squares (OLS) loss function to prevent overfitting, with common regularization techniques including Ridge regression (L2 regularization) and Lasso regression (L1 regularization)."
What are the assumptions of linear regression?,R8,"There should be no autocorrelation in the residuals, especially important in time series data.",1.0,"The assumptions of linear regression include linearity between the independent and dependent variables, homoscedasticity (constant variance of errors), independence of errors, and normality of error terms."
What is the difference between Type I and Type II errors?,R8,Type II error means the test is too sensitive and finds effects that don’t exist.,0.2,"Type I error occurs when a true null hypothesis is rejected (false positive), while Type II error occurs when a false null hypothesis is not rejected (false negative)."
What is the p-value in hypothesis testing?,R8,"The p-value is used to calculate the test statistic, which is then compared with a critical value to decide the result.",0.4,"The p-value is the probability of observing a test statistic as extreme as, or more extreme than, the one observed, assuming that the null hypothesis is true. It is used to assess the strength of evidence against the null hypothesis."
What is the bias-variance tradeoff?,R8,The bias-variance tradeoff only applies to unsupervised learning models like clustering and dimensionality reduction.,0.1,"The bias-variance tradeoff refers to the compromise between bias (error due to overly simplistic assumptions) and variance (error due to sensitivity to fluctuations in the training data) in machine learning models, where reducing one typically increases the other."
What is the difference between variance and standard deviation?,R8,"Variance gives a rough idea of spread, while standard deviation gives an exact measurement.",0.5,"Variance measures the average squared deviation from the mean of a set of values, while standard deviation measures the average deviation from the mean, providing a measure of the dispersion or spread of the data."
What is the difference between a decision tree and a random forest?,R8,"Decision trees are faster and simpler but prone to overfitting, while random forests are slower but more accurate.",1.0,"A decision tree is a simple, interpretable tree-like structure that recursively splits the data based on features, while a random forest is an ensemble of decision trees that aggregates their predictions to improve performance and reduce overfitting."
What is batch normalization in neural networks?,R8,Batch normalization is used only in the output layer to ensure the final prediction is scaled correctly.,0.3,"Batch normalization is a technique used to normalize the activations of each layer in a neural network by adjusting and scaling them to have zero mean and unit variance, which can accelerate training and improve model performance."
What is the purpose of a confusion matrix in classification?,R8,The matrix provides a graphical representation of the loss function over each epoch.,0.2,"A confusion matrix is a table that summarizes the performance of a classification model by comparing predicted class labels with actual class labels, showing the counts of true positives, true negatives, false positives, and false negatives."
What is the Kullback-Leibler (KL) divergence?,R8,It is used to evaluate model accuracy by counting how many predictions are correct.,0.2,"The Kullback-Leibler divergence is a measure of the difference between two probability distributions, used in information theory and statistics to quantify the amount of information lost when one distribution is used to approximate another."
What is the difference between batch normalization and layer normalization in neural networks?,R8,"Layer normalization is useful when the batch size is small or changes frequently, unlike batch normalization.",1.0,"Batch normalization normalizes the activations of each layer across the batch dimension, while layer normalization normalizes the activations across the feature dimension, making it more suitable for recurrent neural networks and other architectures with variable-length sequences."
What is unsupervised learning?,R8,Unsupervised learning models always perform better than supervised ones because they don’t need labels.,0.2,"Unsupervised learning is a type of machine learning where the model is trained on unlabeled data, and it seeks to find patterns or structures in the data without explicit supervision."
What are the main types of unsupervised learning techniques?,R8,"Unsupervised learning includes methods like k-means, PCA, and Apriori for different goals like grouping or association.",1.0,"The main types include clustering, dimensionality reduction, and association rule learning."
What are the common clustering algorithms?,R8,K-Medoids is another clustering algorithm similar to K-Means but more robust to outliers.,1.0,"Common clustering algorithms include K-means clustering, hierarchical clustering, DBSCAN, and Gaussian mixture models (GMM)."
What is hierarchical clustering?,R8,It is used mostly for regression problems where we predict numeric output values.,0.1,Hierarchical clustering is an algorithm that creates a hierarchy of clusters by recursively merging or splitting clusters based on their similarity or dissimilarity.
What is DBSCAN clustering?,R8,DBSCAN assigns cluster labels by calculating distances between every point and then using k-nearest neighbors for classification.,0.3,"DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a density-based clustering algorithm that groups together data points that are closely packed, while marking outliers as noise."
What are the common dimensionality reduction techniques?,R8,Naive Bayes and Logistic Regression are also dimensionality reduction techniques.,0.1,"Common techniques include Principal Component Analysis (PCA), t-Distributed Stochastic Neighbor Embedding (t-SNE), and Singular Value Decomposition (SVD)."
What is PCA (Principal Component Analysis)?,R8,PCA can be used to denoise data by ignoring low-variance components.,1.0,PCA is a linear dimensionality reduction technique that transforms the original features into a lower-dimensional space while preserving the maximum variance in the data.
What is t-SNE (t-Distributed Stochastic Neighbor Embedding)?,R8,It often produces visually meaningful results but doesn’t preserve global structure well.,1.0,"t-SNE is a nonlinear dimensionality reduction technique that focuses on preserving local relationships between data points in a lower-dimensional space, often used for visualizing high-dimensional data."
What is Singular Value Decomposition (SVD)?,R8,SVD replaces missing values in a dataset by estimating them from existing data.,0.3,"SVD is a matrix factorization technique used to decompose a matrix into the product of three matrices, which can be used for dimensionality reduction, data compression, and matrix approximation."
What is association rule learning?,R8,"Support, confidence, and lift are key metrics used to evaluate the strength of association rules.",1.0,"Association rule learning is a type of unsupervised learning used to discover interesting associations or relationships between variables in large datasets, commonly used in market basket analysis and recommendation systems."
What are the common association rule learning algorithms?,R8,Random Forest and Logistic Regression are typically used for generating association rules.,0.1,Common algorithms include Apriori and FP-Growth.
What is the Apriori algorithm?,R8,Apriori generates rules using support and confidence thresholds to ensure the rules are meaningful.,1.0,"The Apriori algorithm is a classic algorithm for mining frequent itemsets and generating association rules, based on the principle of the Apriori property and candidate generation."
What is FP-Growth?,R8,It is widely used in market basket analysis to identify common product combinations.,1.0,"FP-Growth (Frequent Pattern Growth) is an efficient algorithm for mining frequent itemsets and generating association rules without candidate generation, using a compact data structure called FP-tree."
What is anomaly detection?,R8,Anomaly detection always assumes the data is normally distributed to find outliers.,0.3,"Anomaly detection, also known as outlier detection, is the process of identifying data points or observations that deviate significantly from the rest of the dataset, which may indicate errors, anomalies, or interesting patterns."
What are the common anomaly detection techniques?,R8,Autoencoders are deep learning models that detect anomalies by checking for high reconstruction errors.,1.0,"Common techniques include statistical methods, density-based methods, distance-based methods, and machine learning-based methods such as Isolation Forest and One-Class SVM."
What is Isolation Forest?,R8,Isolation Forest uses ensemble learning to combine multiple isolation trees for better accuracy.,1.0,"Isolation Forest is an unsupervised learning algorithm for anomaly detection that isolates anomalies by recursively partitioning the dataset into subsets using random splits, making anomalies easier to isolate than normal data points."
What is One-Class SVM?,R8,The algorithm works only with numerical data and cannot handle categorical features.,0.4,One-Class Support Vector Machine (SVM) is a machine learning algorithm for anomaly detection that learns a boundary around normal data points in the feature space and identifies anomalies as data points lying outside this boundary.
What is the purpose of density estimation?,R8,Density estimation can be used in clustering to identify dense groups of points.,1.0,"Density estimation is the process of estimating the probability density function of a random variable from a set of data points, often used in clustering, anomaly detection, and generative modeling."
What are the common density estimation techniques?,R8,Principal Component Analysis is a common density estimation technique.,0.2,"Common techniques include histogram-based methods, kernel density estimation (KDE), Gaussian mixture models (GMM), and Parzen window estimation."
What is the Gaussian Mixture Model (GMM)?,R8,GMM works by fitting a linear regression model to the data using Gaussian noise assumptions.,0.2,"Gaussian Mixture Model (GMM) is a probabilistic model used for density estimation and clustering, assuming that the data is generated from a mixture of several Gaussian distributions."
What is the Expectation-Maximization (EM) algorithm?,R8,EM is used in deep learning for training neural networks.,0.2,"The Expectation-Maximization (EM) algorithm is an iterative optimization algorithm used to estimate the parameters of probabilistic models with latent variables, such as Gaussian mixture models (GMM), by iteratively computing the expected values of the latent variables and maximizing the likelihood function."
What is the difference between generative and discriminative models?,R8,"Discriminative models directly estimate the decision boundary, ignoring how data is generated.",1.0,"Generative models learn the joint probability distribution of the input features and the output labels, while discriminative models learn the conditional probability distribution of the output labels given the input features."
What is the purpose of generative modeling?,R8,They can be used to detect anomalies by identifying data points that don’t fit the learned distribution.,1.0,"Generative modeling is used to model the underlying structure of the data and generate new samples from the learned distribution, often used in tasks such as image generation, text generation, and data augmentation."
What is data science?,R9,It is a branch of computer science that teaches us how to store and retrieve data using databases. Data science also teaches us how to write queries in SQL and structure the information in tables.,0.3,"Data science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from data."
What are the key steps in the data science process?,R9,"In data science, you start with a hypothesis, then you test it by generating some data from simulations or experiments. After that, you summarize the findings and make decisions. There’s no need for machine learning unless it’s a really big project.",0.25,"The key steps typically include problem definition, data collection, data preparation, exploratory data analysis, modeling, evaluation, and deployment."
What is the difference between supervised and unsupervised learning?,R9,"Supervised learning includes algorithms like linear regression and decision trees, while unsupervised learning includes clustering and PCA. The main difference is whether or not the output labels are available during training. Each has its specific use cases.",0.9,"Supervised learning involves training a model on labeled data, where the algorithm learns the relationship between input features and target labels. Unsupervised learning deals with unlabeled data and aims to find hidden patterns or structures within the data."
Explain the bias-variance tradeoff.,R9,"This tradeoff refers to the challenge of balancing errors due to bias and errors due to variance. Models with high bias are too rigid, and those with high variance are too flexible. Finding the sweet spot ensures good predictions on unseen data.",0.95,The bias-variance tradeoff is the balance between the error introduced by bias (underfitting) and the error introduced by variance (overfitting) when building a machine learning model.
What is feature engineering?,R9,"Feature engineering involves transforming, creating, or selecting features to make machine learning models more effective. It’s especially important when working with structured data and can drastically improve model performance when done correctly.",1.0,"Feature engineering is the process of selecting, transforming, and creating features from raw data to improve model performance."
How do you handle missing data in a dataset?,R9,"There are several methods like mean imputation, regression imputation, and even using machine learning models to predict missing values. The choice depends on how much data is missing and the type of data.",0.95,"Missing data can be handled by imputation techniques such as mean, median, or mode imputation, using predictive models to estimate missing values, or by deleting rows or columns with missing values, depending on the dataset size and nature."
What is cross-validation?,R9,Cross-validation is an important part of model tuning where we test different models on the same data until we find the one that gives the best performance. It’s more like trial and error method for model selection.,0.4,Cross-validation is a technique used to assess the performance of a predictive model by splitting the data into multiple subsets.
Explain the concept of overfitting.,R9,Overfitting means your model doesn't generalize well to new data. It's a problem in machine learning where the model performs well during training but poorly during real-world use. It can be avoided with proper validation techniques.,1.0,"Overfitting occurs when a model learns the training data too well, capturing noise or random fluctuations in the data instead of the underlying pattern."
What are some common algorithms used in supervised learning?,R9,"Linear regression and logistic regression are two simple yet powerful supervised learning algorithms. Linear regression predicts continuous outcomes, while logistic regression is used for binary classification problems like yes/no decisions.",1.0,"Common algorithms include linear regression, logistic regression, decision trees, random forests, support vector machines, and neural networks."
What is dimensionality reduction?,R9,Dimensionality reduction is like shrinking a big Excel sheet into a smaller one by removing unrelated features. This helps in visualization and understanding of complex datasets more easily.,0.85,Dimensionality reduction is the process of reducing the number of input variables in a dataset by transforming them into a lower-dimensional space while preserving important information.
What is regularization in machine learning?,R9,"Regularization ensures the model performs well by simplifying the hypothesis function. This is done by penalizing high coefficient values, especially in regression algorithms. It improves generalization and reduces the risk of overfitting.",1.0,Regularization is a technique used to prevent overfitting by adding a penalty term to the model's objective function.
What is ensemble learning?,R9,It involves training different models on the same data and combining their predictions to get better results. The idea is that a group of weak models together can outperform a single strong one in many tasks.,0.95,Ensemble learning is a technique that combines multiple machine learning models to improve performance and robustness.
What is the ROC curve?,R9,"ROC curve shows a trade-off between sensitivity and specificity. The curve is used to evaluate classifiers, and a curve closer to the top-left means the model has better performance.",0.9,The receiver operating characteristic (ROC) curve is a graphical plot that illustrates the performance of a binary classification model across different threshold settings.
What is AUC-ROC?,R9,"AUC stands for Area Under the Curve, and ROC is the curve itself. Together they give a complete picture of a classifier’s performance at all threshold levels.",0.95,The area under the ROC curve (AUC-ROC) is a metric used to evaluate the performance of a binary classification model.
What is the difference between classification and regression?,R9,"Classification means grouping things into different buckets or classes, whereas regression estimates a value along a continuous range. For example, detecting disease type is classification, and predicting blood pressure is regression.",0.95,"Classification is a supervised learning task where the goal is to predict categorical labels or classes, while regression is a supervised learning task where the goal is to predict continuous numerical values."
What is clustering?,R9,It’s a technique where we divide the data into different clusters so that data points in the same group are more similar to each other than to those in other groups.,0.9,Clustering is an unsupervised learning technique that involves grouping similar data points together based on their features or characteristics.
What is the curse of dimensionality?,R9,"As dimensionality increases, data becomes sparse, and it becomes difficult for algorithms to detect patterns. This affects clustering and classification accuracy and can lead to increased computation time.",0.95,The curse of dimensionality refers to the phenomenon where the performance of certain algorithms deteriorates as the number of features or dimensions in the data increases.
What is the difference between precision and recall?,R9,"Recall is the ability to find all relevant cases, especially when you don’t want to miss any true positives. Precision is needed when you want to minimize false alarms.",0.9,"Precision measures the proportion of true positive predictions among all positive predictions made by a classifier, while recall measures the proportion of true positive predictions among all actual positive instances in the data."
What is the F1 score?,R9,"In binary classification, the F1 score provides a balanced metric, especially when the cost of false positives and false negatives is similar. It is widely used in medical diagnoses and spam detection.",0.95,The F1 score is the harmonic mean of precision and recall and provides a single metric to evaluate the performance of a classifier.
What is the bias of an estimator?,R9,"Bias is the built-in error that comes from the assumptions made by the learning algorithm. Simpler models often have high bias, which can cause them to miss relevant patterns in the data.",0.95,The bias of an estimator measures the difference between the expected value of the estimator and the true value of the parameter being estimated.
What is the variance of an estimator?,R9,Variance measures the variability in predictions if the same estimator is applied to different training sets. High variance models often overfit because they memorize the training data.,0.95,The variance of an estimator measures the spread or variability of the estimator's values around its expected value.
What is the Central Limit Theorem?,R9,"CLT is used to prove that if a machine learning model is trained with enough epochs, it will always produce normally distributed results, regardless of the input data.",0.2,The Central Limit Theorem states that the sampling distribution of the sample mean approaches a normal distribution as the sample size increases.
What is regularization in neural networks?,R9,"In neural networks, regularization techniques like L2 and dropout help make the model more robust and less likely to memorize. It ensures better generalization on validation or test data.",1.0,Regularization in neural networks involves adding penalty terms to the loss function to prevent overfitting.
What is batch normalization?,R9,This technique helps by maintaining consistent data distribution through layers. It normalizes data within mini-batches and introduces two learnable parameters (gamma and beta) to scale and shift the normalized values.,1.0,Batch normalization is a technique used in neural networks to improve training stability and speed by normalizing the activations of each layer.
What is transfer learning?,R9,"It is widely used in deep learning, especially in tasks like image classification and NLP, where training from scratch would require large amounts of data and computation.",1.0,Transfer learning is a machine learning technique where a model trained on one task is reused as the starting point for a model on a related task.
What is natural language processing (NLP)?,R9,"NLP involves cleaning and preparing text data through processes like stemming, lemmatization, and removing stop words to make it usable for machine learning tasks.",0.9,Natural language processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human languages.
What is word embedding?,R9,"In word embedding, high-dimensional one-hot vectors are transformed into low-dimensional dense vectors that capture context and meaning of the words more effectively.",1.0,Word embedding is a technique used to represent words as dense vectors in a continuous vector space.
What is sentiment analysis?,R9,"The technique uses algorithms to identify emotional cues in text, helping companies and researchers assess public reactions to products, services, and events.",0.95,Sentiment analysis is a natural language processing task that involves determining the sentiment or opinion expressed in a piece of text.
What is deep learning?,R9,"By stacking layers of neurons, deep learning allows models to learn hierarchical representations of data, which makes it highly effective for complex tasks.",0.95,Deep learning is a subset of machine learning that focuses on artificial neural networks with multiple layers.
What is a convolutional neural network (CNN)?,R9,It’s a type of neural network where each neuron connects to every pixel in an image to directly predict output values without any hidden processing layers.,0.25,"A convolutional neural network (CNN) is a type of deep neural network that is particularly well-suited for processing structured grid-like data, such as images."
What is a recurrent neural network (RNN)?,R9,"RNNs have trouble with long sequences due to issues like vanishing gradients, but they are powerful for tasks where context over time is important, such as machine translation.",0.95,A recurrent neural network (RNN) is a type of neural network designed to process sequential data by maintaining a state or memory of previous inputs.
What is reinforcement learning?,R9,"It is commonly used in scenarios where decisions must be made in sequence, such as in self-driving cars or dynamic pricing systems. The model continuously improves its decision-making process.",0.95,Reinforcement learning is a machine learning paradigm where an agent learns to make decisions by interacting with an environment to maximize cumulative rewards.
What is Q-learning?,R9,"Through repeated updates using the Bellman equation, Q-learning refines its estimates of action values, allowing it to converge to the best policy in a Markov Decision Process.",1.0,Q-learning is a model-free reinforcement learning algorithm used to learn the optimal action-selection policy for a Markov decision process (MDP).
What is the exploration-exploitation tradeoff in reinforcement learning?,R9,"Exploration helps in gathering knowledge about the environment, while exploitation applies that knowledge to maximize reward. The tradeoff affects how quickly and effectively the agent learns.",0.9,The exploration-exploitation tradeoff refers to the dilemma faced by reinforcement learning agents when deciding whether to explore new actions or exploit known actions to maximize rewards.
What is deep reinforcement learning?,R9,"It’s a powerful AI approach where agents learn through interaction with their environment, combining the perception abilities of deep learning with decision-making from reinforcement learning.",1.0,Deep reinforcement learning combines reinforcement learning with deep learning techniques to handle high-dimensional input spaces and learn complex policies directly from raw sensory input.
What is a Markov decision process (MDP)?,R9,An MDP assumes that the environment has no randomness and only one fixed outcome for each action. This makes the learning process straightforward and predictable.,0.2,A Markov decision process (MDP) is a mathematical framework used to model sequential decision-making problems in which an agent interacts with an environment.
What is cross-entropy loss?,R9,It is calculated using a log loss formula and is suitable for binary and multi-class classification problems. It helps guide model updates during backpropagation.,0.95,"Cross-entropy loss, also known as log loss, is a loss function used in classification tasks to quantify the difference between the predicted probability distribution and the true probability distribution of the classes."
What is the softmax function?,R9,"It ensures that all output values are positive and normalized, making it useful when the model needs to make a final decision among several possible classes.",0.95,The softmax function is a mathematical function that converts a vector of arbitrary real values into a vector of probabilities.
What is the difference between a generative model and a discriminative model?,R9,"Discriminative models aim to separate the classes effectively, while generative models try to understand how each class is formed, often allowing generation of synthetic data.",0.95,"A generative model learns the joint probability distribution of the input features and the target labels, while a discriminative model learns the conditional probability distribution of the target labels given the input features."
What is autoencoder?,R9,"They can be used for tasks like denoising, anomaly detection, and feature extraction. However, they aren’t ideal for tasks like direct classification or regression.",0.9,An autoencoder is a type of neural network architecture that is trained to reconstruct its input data at the output layer.
What is the difference between bagging and boosting?,R9,"Bagging is used to handle noisy data and reduce overfitting, while boosting is more prone to overfitting but can achieve better accuracy with well-tuned models.",0.9,"Bagging (Bootstrap Aggregating) is an ensemble method that builds multiple models independently and combines their predictions through averaging or voting, while boosting sequentially builds models by giving more weight to misclassified instances."
What is hyperparameter tuning?,R9,"It is a trial-and-error process that searches for optimal values of parameters that control the learning process, not the model weights themselves.",0.95,Hyperparameter tuning is the process of finding the optimal hyperparameters for a machine learning model to improve its performance.
What is the curse of dimensionality in the context of feature selection?,R9,"With more dimensions, data points become increasingly distant from each other. This makes clustering, classification, and regression more difficult unless dimensionality is reduced.",0.95,"The curse of dimensionality in feature selection refers to the challenges and limitations posed by high-dimensional data, such as increased computational complexity and difficulty in finding meaningful features."
What is the difference between L1 and L2 regularization?,R9,"L1 is more interpretable for sparse models, while L2 provides stability and prevents sharp changes in weights. They are often used together in Elastic Net.",0.95,"L1 regularization (Lasso) adds a penalty term proportional to the absolute value of the coefficients, encouraging sparsity and feature selection, while L2 regularization (Ridge) adds a penalty term proportional to the square of the coefficients, penalizing large coefficients."
What is the purpose of cross-validation in feature selection?,R9,Using cross-validation helps avoid selecting features that perform well on a single dataset by chance. It improves the reliability of the feature selection process.,0.95,"Cross-validation in feature selection is used to evaluate the performance of a model with different subsets of features, helping to identify the most informative features and reduce the risk of overfitting."
What is the role of bias in model evaluation?,R9,"In model evaluation, bias is important for understanding model limitations. It shows whether errors are due to the model being too rigid or too simple.",0.95,"Bias in model evaluation refers to systematic errors or inaccuracies in the predictions made by a model, which may arise from simplifying assumptions or limitations in the model architecture."
What is the importance of interpretability in machine learning models?,R9,"In regulated industries, interpretable models are often required by law so that decisions can be traced and justified. This can also improve adoption and accountability.",0.95,"Interpretability in machine learning models is important for understanding how predictions are made, gaining insights into the underlying relationships in the data, and building trust with stakeholders."
What is the difference between k-means and hierarchical clustering?,R9,"K-means updates cluster centroids iteratively, while hierarchical clustering builds a nested structure without any centroid updates. The two methods have different strategies for grouping data.",0.95,"K-means clustering partitions the data into k clusters by minimizing the within-cluster sum of squares, while hierarchical clustering builds a tree-like hierarchy of clusters by recursively merging or splitting clusters based on proximity."
What is the Elbow Method used for in k-means clustering?,R9,It involves plotting the number of clusters against the accuracy of the model. The point where accuracy drops is called the elbow.,0.3,"The Elbow Method is used to determine the optimal number of clusters (k) in k-means clustering by plotting the within-cluster sum of squares against the number of clusters and identifying the ""elbow"" point where the rate of decrease in inertia slows down."
What is the role of regularization in reducing model complexity?,R9,It helps prevent the model from becoming too sensitive to small fluctuations in the training data. This leads to more robust and interpretable models.,0.9,"Regularization in machine learning reduces model complexity by adding penalty terms to the loss function, encouraging simpler models with smaller coefficients or fewer features."
What is the difference between gradient descent and stochastic gradient descent?,R9,"The key difference is in the batch size used for updates: full batch for gradient descent and one (or mini-batch) for stochastic, leading to different speed and stability tradeoffs.",0.95,"Gradient descent updates model parameters using the average gradient of the entire training dataset, while stochastic gradient descent updates parameters using the gradient of a single randomly selected data point or mini-batch."
What is the tradeoff between bias and variance in model selection?,R9,The bias-variance tradeoff is important because a model with low bias and low variance will perform well on both training and unseen data.,0.95,The bias-variance tradeoff in model selection involves balancing the tradeoff between bias (underfitting) and variance (overfitting) by selecting models with an appropriate level of complexity based on the dataset and task.
What is the role of regularization in reducing overfitting?,R9,"Regularization techniques like L1 and L2 add penalties that shrink coefficients, helping to keep the model from being too complex and reducing overfitting.",0.95,"Regularization in machine learning reduces overfitting by penalizing overly complex models, encouraging simpler models that generalize better to unseen data."
What are some techniques for handling imbalanced datasets in classification tasks?,R9,"Class weighting adjusts the loss function to penalize misclassifications of the minority class more heavily, helping the model focus on it during training.",0.95,"Techniques for handling imbalanced datasets include resampling methods such as oversampling the minority class or undersampling the majority class, using cost-sensitive learning algorithms, and generating synthetic samples with techniques like SMOTE."
What is the purpose of A/B testing in data science?,R9,The purpose is to split data into two parts and analyze them separately for different results.,0.4,"A/B testing is used to compare two or more versions of a product or intervention to determine which one performs better based on predefined metrics, such as conversion rate or user engagement."
What is the difference between correlation and causation?,R9,"Causation is when changes in one variable produce changes in another, correlation just means they happen at the same time without proof of cause.",0.9,"Correlation measures the strength and direction of the linear relationship between two variables, while causation indicates that changes in one variable directly cause changes in another variable."
What is the role of feature scaling in machine learning?,R9,It helps by converting categorical data into numerical values so the model can understand them.,0.1,"Feature scaling in machine learning ensures that all features have the same scale or range, preventing certain features from dominating others during model training. Common scaling techniques include min-max scaling and standardization."
What is the difference between classification and clustering?,R9,"Classification uses algorithms like decision trees and SVMs, while clustering uses k-means and hierarchical methods to find patterns in data.",0.95,"Classification is a supervised learning task where the goal is to predict categorical labels or classes, while clustering is an unsupervised learning task where the goal is to group similar data points together based on their features."
What is the purpose of Principal Component Analysis (PCA) in dimensionality reduction?,R9,PCA clusters similar data points together to reduce the dataset size.,0.3,"Principal Component Analysis (PCA) is used to reduce the dimensionality of a dataset by transforming the original features into a new set of orthogonal principal components, which capture the maximum variance in the data."
What is the role of regularization in neural networks?,R9,Regularization helps by penalizing large weights using techniques like L1 and L2 regularization.,0.9,"Regularization in neural networks helps prevent overfitting by adding penalty terms to the loss function, encouraging simpler models with smaller weights or fewer connections."
What is dropout regularization in neural networks?,R9,"It works by randomly ignoring some neurons each iteration, preventing complex co-adaptations and encouraging the network to learn more robust features.",0.95,"Dropout regularization is a technique used in neural networks to reduce overfitting by randomly dropping out a fraction of neurons during training, forcing the network to learn more robust representations."
What is the difference between a generative adversarial network (GAN) and a variational autoencoder (VAE)?,R9,"VAEs rely on a probabilistic framework, while GANs rely on game theory concepts involving generator and discriminator networks.",0.9,"Generative adversarial networks (GANs) consist of two neural networks (generator and discriminator) that compete with each other to generate realistic data samples, while variational autoencoders (VAEs) learn a probabilistic model of the data's latent space and generate new samples by sampling from this distribution."
What is the difference between batch gradient descent and mini-batch gradient descent?,R9,Batch gradient descent is more noisy compared to mini-batch gradient descent because it updates more frequently.,0.2,"Batch gradient descent computes the gradient of the cost function using the entire training dataset, while mini-batch gradient descent computes the gradient using a randomly selected subset (mini-batch) of the training data."
What is the role of activation functions in neural networks?,R9,Activation functions are used to increase the number of neurons dynamically during training.,0.2,"Activation functions introduce non-linearity into neural networks, allowing them to learn complex mappings between inputs and outputs. Common activation functions include sigmoid, tanh, ReLU, and softmax."
What is the difference between generative and discriminative models in machine learning?,R9,"Discriminative models are more complex because they model the entire data distribution, generative models are simpler as they only model the decision boundary.",0.2,"Generative models learn the joint probability distribution of the input features and the target labels, while discriminative models learn the conditional probability distribution of the target labels given the input features."
What is the purpose of dropout regularization in neural networks?,R9,"Dropout randomly ignores some neurons in each training iteration, preventing co-adaptation of neurons and improving model robustness.",0.95,"Dropout regularization is a technique used in neural networks to prevent overfitting by randomly dropping out a fraction of neurons during training, forcing the network to learn more robust representations."
What is the role of LSTMs in sequence modeling tasks?,R9,"LSTMs help model sequential data by maintaining hidden states that store context, enabling better predictions for tasks like time series forecasting.",0.95,"Long Short-Term Memory (LSTM) networks are a type of recurrent neural network (RNN) designed to capture long-range dependencies and temporal dynamics in sequential data, making them well-suited for tasks such as language modeling and time series prediction."
What is the difference between precision and recall in classification tasks?,R9,"Recall is the ratio of true positives to false positives, and precision is the ratio of false negatives to true negatives.",0.1,"Precision measures the proportion of true positive predictions among all positive predictions made by a classifier, while recall measures the proportion of true positive predictions among all actual positive instances in the data."
"What is the F1 score, and why is it useful in model evaluation?",R9,The F1 score is calculated by adding precision and recall and dividing by two.,0.3,"The F1 score is the harmonic mean of precision and recall and provides a single metric to evaluate the performance of a classifier, balancing both precision and recall. It is particularly useful for imbalanced datasets where one class is much more prevalent than the others."
What is the purpose of word embedding in natural language processing (NLP)?,R9,"The purpose of word embedding is to convert written text into spoken audio. They are used to generate natural-sounding speech from text by analyzing the words and applying appropriate pronunciation rules and intonation patterns. This technology is primarily focused on speech synthesis, aiming to create a realistic and understandable auditory representation of written language for various applications, such as voice assistants and audiobooks.",0.1,"Word embedding is a technique used to represent words as dense vectors in a continuous vector space, capturing semantic relationships between words and enabling neural networks to process textual data more effectively."
What is the difference between word2vec and GloVe in word embedding?,R9,"Both models use word co-occurrence, but Word2Vec does it through context prediction and GloVe uses it directly through matrix statistics.",0.9,"Word2Vec is a model that learns word embeddings by predicting the surrounding words in a text corpus, while GloVe (Global Vectors for Word Representation) learns word embeddings by factorizing the co-occurrence matrix of words in the corpus."
What is the purpose of sentiment analysis in natural language processing (NLP)?,R9,It allows businesses to monitor customer satisfaction and brand perception by analyzing textual data.,0.9,"Sentiment analysis is a natural language processing task that involves determining the sentiment or opinion expressed in a piece of text, such as positive, negative, or neutral. It is commonly used to analyze customer reviews, social media posts, and survey responses."
What is the difference between shallow learning and deep learning?,R9,"Deep learning models can discover hierarchical features from raw data, which shallow models usually cannot do.",0.95,"Shallow learning refers to traditional machine learning algorithms with a small number of layers or features, while deep learning involves neural networks with multiple layers that can learn hierarchical representations of the data."
What is the role of dropout regularization in neural networks?,R9,"During training, dropout disables different neurons each time, so the model is less likely to memorize the training data.",0.95,"Dropout regularization is a technique used in neural networks to prevent overfitting by randomly dropping out a fraction of neurons during training, forcing the network to learn more robust representations."
What is the difference between precision and accuracy in model evaluation?,R9,Precision and accuracy both increase when the model avoids false negatives.,0.4,"Precision measures the proportion of true positive predictions among all positive predictions made by a classifier, while accuracy measures the proportion of correct predictions (both true positives and true negatives) among all predictions made by the classifier."
What is the difference between k-fold cross-validation and leave-one-out cross-validation?,R9,"The difference is that leave-one-out never repeats training, but k-fold does multiple times.",0.3,"K-fold cross-validation divides the data into k subsets and iteratively trains the model k times, each time using a different subset as the validation set, while leave-one-out cross-validation trains the model k times, each time using all but one data point as the training set and the remaining point as the validation set."
What is the Naive Bayes classifier and how does it work?,R9,Naive Bayes predicts classes based on conditional probability and is surprisingly effective even when its independence assumption is violated.,0.95,"The Naive Bayes classifier is a probabilistic model based on Bayes' theorem with the ""naive"" assumption of feature independence. It calculates the probability of a class given the input features using the product of conditional probabilities."
What is the difference between correlation and covariance?,R9,"Covariance is always positive, but correlation can be negative if variables move in opposite directions.",0.2,"Covariance measures the degree to which two variables change together, while correlation measures both the strength and direction of the linear relationship between two variables."
What is the purpose of a confusion matrix in classification tasks?,R9,"Using a confusion matrix, we can understand both the types and counts of errors made by the model.",0.95,"A confusion matrix is a table that visualizes the performance of a classification model by comparing predicted class labels with true class labels. It shows the number of true positives, true negatives, false positives, and false negatives."
What is the difference between stratified sampling and random sampling?,R9,"Random sampling can lead to biased samples if the population is imbalanced, but stratified sampling avoids that by preserving proportions.",0.95,"Stratified sampling divides the population into homogeneous subgroups (strata) and samples from each stratum proportionally, while random sampling selects individuals randomly from the entire population without regard to strata."
"What is the difference between univariate, bivariate, and multivariate analysis?",R9,"In univariate analysis, you look at the distribution of a single feature. In multivariate, you analyze multiple features and their combined effect on outcomes.",0.9,"Univariate analysis examines the distribution and summary statistics of a single variable, bivariate analysis explores the relationship between two variables, and multivariate analysis involves analyzing multiple variables simultaneously."
What is the purpose of data preprocessing in machine learning?,R9,Preprocessing increases overfitting by simplifying the data too much before training.,0.2,"Data preprocessing involves cleaning, transforming, and preparing raw data for machine learning algorithms. It includes tasks such as handling missing values, encoding categorical variables, and scaling features."
What is the bias of a statistical estimator?,R9,"The lower the bias, the more accurate the estimator tends to be across different samples.",0.95,The bias of a statistical estimator measures the difference between the expected value of the estimator and the true value of the parameter being estimated. A biased estimator systematically overestimates or underestimates the true parameter value.
What is the variance of a statistical estimator?,R9,"High variance can lead to overfitting, because the model captures noise in the data.",0.9,"The variance of a statistical estimator measures the spread or variability of the estimator's values around its expected value. A high variance indicates that the estimator's values are scattered widely, while a low variance indicates that the values are clustered closely together."
What is the difference between parametric and non-parametric statistical tests?,R9,Non-parametric tests are best when the data is skewed or has outliers that violate parametric assumptions.,0.95,"Parametric statistical tests assume that the data follows a specific probability distribution (e.g., normal distribution), while non-parametric tests make no such assumptions and are based on ranks or frequencies."
What is the purpose of feature scaling in machine learning?,R9,It’s mainly used to normalize or standardize features before training certain machine learning models.,0.9,"Feature scaling ensures that all features have the same scale or range, preventing certain features from dominating others during model training. Common scaling techniques include min-max scaling and standardization."
What is the difference between correlation and causation in statistics?,R9,"When two things are correlated, we can’t assume causation unless we conduct controlled experiments.",0.9,"Correlation measures the strength and direction of the linear relationship between two variables, while causation indicates that changes in one variable directly cause changes in another variable."
What is the Central Limit Theorem and why is it important?,R9,"The CLT applies to any kind of statistic, like median and mode, not just the mean.",0.3,"The Central Limit Theorem states that the sampling distribution of the sample mean approaches a normal distribution as the sample size increases, regardless of the shape of the population distribution. It is important because it allows us to make inferences about population parameters based on sample statistics."
What is the purpose of outlier detection in data analysis?,R9,"Sometimes outliers are not errors, but rare meaningful cases like fraud or medical anomalies, so we need to identify them.",0.95,"Outlier detection involves identifying observations that deviate significantly from the rest of the data. Outliers can distort statistical analyses and machine learning models, so detecting and handling them appropriately is important for accurate results."
What is the difference between supervised and unsupervised outlier detection methods?,R9,"In unsupervised methods, no prior knowledge of what is an outlier is required, unlike supervised methods where such labels are essential.",0.95,"Supervised outlier detection methods require labeled data with outlier labels, while unsupervised methods detect outliers based on the characteristics of the data itself without requiring labeled examples."
What is the difference between statistical inference and predictive modeling?,R9,"Inference focuses more on explaining ‘why,’ while predictive modeling is more about answering ‘what will happen.’",0.9,"Statistical inference involves drawing conclusions about a population based on a sample of data, while predictive modeling focuses on building models that make predictions about future or unseen data based on past observations."
What is the purpose of regularization in machine learning?,R9,"By adding a penalty term to the loss function, regularization limits how much the model can adjust to noise in the training data.",0.9,"Regularization is a technique used to prevent overfitting by adding a penalty term to the model's objective function, discouraging overly complex models. Common regularization techniques include L1 regularization (Lasso) and L2 regularization (Ridge)."
What is the difference between batch gradient descent and stochastic gradient descent?,R9,"The main difference is that batch GD requires more memory since it processes all data at once, whereas SGD needs less memory.",0.9,"Batch gradient descent updates model parameters using the average gradient of the entire training dataset, while stochastic gradient descent updates parameters using the gradient of a single randomly selected data point or mini-batch."
What is the difference between statistical modeling and machine learning?,R9,"Machine learning adapts better to non-linear patterns in the data, while statistical models usually require explicit assumptions.",0.95,"Statistical modeling focuses on understanding the relationships between variables in a probabilistic framework, while machine learning emphasizes building algorithms that can learn patterns and make predictions from data without explicitly programming them."
What is the purpose of feature selection in machine learning?,R9,"It is used to eliminate features that do not contribute significantly to the target variable, making the model simpler.",0.95,"Feature selection involves selecting the most relevant features from a dataset to improve model performance, reduce overfitting, and enhance interpretability."
What is the difference between regularization and feature selection in machine learning?,R9,"Regularization and feature selection both aim to improve model generalization, but they work in different ways—one during training, the other before.",0.95,"Regularization penalizes overly complex models by adding a penalty term to the objective function, while feature selection explicitly selects a subset of features to include in the model based on their importance or relevance."
What is the difference between grid search and random search for hyperparameter tuning?,R9,"Grid search creates a grid of parameters and tests all, while random search samples parameters randomly for a fixed number of iterations.",0.95,"Grid search exhaustively searches through a specified grid of hyperparameter values, while random search samples hyperparameter values randomly from predefined distributions."
What is the purpose of cross-validation in model evaluation?,R9,Cross-validation helps us understand how sensitive the model is to the specific choice of training data.,0.9,Cross-validation is a technique used to assess the performance of a predictive model by splitting the data into multiple subsets. It helps to estimate the model's generalization error and detect overfitting.
What is the difference between precision and recall in classification evaluation?,R9,"Recall is about catching all the relevant cases, even if it means making more mistakes, while precision tries to be more selective.",0.95,"Precision measures the proportion of true positive predictions among all positive predictions made by a classifier, while recall measures the proportion of true positive predictions among all actual positive instances in the data."
"What is the F1 score, and why is it useful in imbalanced datasets?",R9,"F1 score works well when you care equally about precision and recall, especially when classes are not evenly distributed.",0.95,The F1 score is the harmonic mean of precision and recall and provides a single metric to evaluate the performance of a classifier. It is particularly useful for imbalanced datasets where one class is much more prevalent than the others.
What is the difference between bagging and boosting ensemble methods?,R9,"Boosting is slower but can lead to higher accuracy by focusing more on misclassified examples, unlike bagging.",0.9,"Bagging (Bootstrap Aggregating) builds multiple models independently and combines their predictions through averaging or voting, while boosting sequentially builds models by giving more weight to misclassified instances."
What is the purpose of principal component analysis (PCA) in dimensionality reduction?,R9,PCA creates new features (principal components) that capture the directions of maximum variance in the data.,0.95,"Principal Component Analysis (PCA) is used to reduce the dimensionality of a dataset by transforming the original features into a new set of orthogonal principal components, which capture the maximum variance in the data."
What is the difference between classification and regression in machine learning?,R9,"In regression, errors are measured using metrics like MSE, while in classification, metrics like accuracy or F1 score are used.",0.95,"Classification is a supervised learning task where the goal is to predict categorical labels or classes, while regression is a supervised learning task where the goal is to predict continuous numerical values."
What is the purpose of natural language processing (NLP) in data science?,R9,"With NLP, we can perform named entity recognition, keyword extraction, and other tasks important for analyzing textual datasets.",0.95,"Natural language processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human languages. It enables computers to understand, interpret, and generate human language data."
What is the difference between bag-of-words and word embeddings in NLP?,R9,"Bag-of-words is simpler and faster, but embeddings are more powerful in capturing deeper linguistic patterns.",0.9,"Bag-of-words represents text as a collection of word counts or frequencies, ignoring word order and grammar, while word embeddings represent words as dense vectors in a continuous vector space, capturing semantic relationships between words."
What is the difference between sentiment analysis and topic modeling in NLP?,R9,"Sentiment analysis focuses on feelings, emotions, or opinions, whereas topic modeling looks for repeated themes across documents.",0.95,"Sentiment analysis determines the sentiment or opinion expressed in a piece of text, such as positive, negative, or neutral, while topic modeling identifies the underlying topics or themes present in a collection of documents."
What is the purpose of word2vec in NLP?,R9,"Word2Vec captures syntactic and semantic relationships, like ‘king’ – ‘man’ + ‘woman’ = ‘queen’.",0.95,Word2Vec is a model that learns word embeddings by predicting the surrounding words in a text corpus. It captures semantic relationships between words and enables algorithms to process textual data more effectively.
What is the difference between a generative model and a discriminative model in machine learning?,R9,"Discriminative models are faster at making predictions, but generative models offer more flexibility in applications like image generation.",0.9,"A generative model learns the joint probability distribution of the input features and the target labels, while a discriminative model learns the conditional probability distribution of the target labels given the input features."
What is the difference between deep learning and traditional machine learning algorithms?,R9,Deep learning is a subset of machine learning focused on learning from data with many layers of transformations.,0.95,"Deep learning involves neural networks with multiple layers that can learn hierarchical representations of the data, while traditional machine learning algorithms typically use handcrafted features and simpler models."
What is the purpose of transfer learning in deep learning?,R9,"Transfer learning is useful when we want to apply a model’s previous experience to a new, related challenge.",0.95,"Transfer learning is a technique where a model trained on one task is reused as the starting point for a model on a related task. It enables the transfer of knowledge and features learned from one domain to another, reducing the need for large labeled datasets."
What is the difference between convolutional neural networks (CNNs) and recurrent neural networks (RNNs)?,R9,"CNNs are feedforward networks with local connections, and RNNs are networks with feedback loops to handle temporal dependencies.",0.95,"Convolutional neural networks (CNNs) are well-suited for processing grid-like data such as images, while recurrent neural networks (RNNs) are designed to handle sequential data with temporal dependencies such as text or time series."
What is the purpose of batch normalization in neural networks?,R9,"It standardizes activations across the mini-batch, which helps in faster and more stable training.",0.95,Batch normalization is a technique used in neural networks to improve training stability and speed by normalizing the activations of each layer. It reduces internal covariate shift and accelerates convergence during training.
What is the difference between a feedforward neural network and a recurrent neural network?,R9,"The key difference is that RNNs can process data with temporal dependencies, but feedforward networks process each input independently.",0.95,"A feedforward neural network consists of multiple layers of neurons connected in a forward direction without cycles, while a recurrent neural network has connections that form directed cycles, allowing it to maintain state or memory over time."
What is the difference between LSTMs and GRUs in recurrent neural networks?,R9,"Both models are good at capturing sequential patterns, but GRUs are typically preferred when computational resources are limited.",0.9,"LSTMs (Long Short-Term Memory) and GRUs (Gated Recurrent Units) are both types of recurrent neural network architectures designed to capture long-range dependencies in sequential data, but they differ in their internal gating mechanisms and computational complexity."
What is the purpose of attention mechanisms in neural networks?,R9,"By using attention, models can improve accuracy by weighing key features more heavily than less important ones.",0.95,Attention mechanisms in neural networks enable the model to focus on relevant parts of the input data while ignoring irrelevant information. They have been particularly successful in natural language processing tasks such as machine translation and text summarization.
What is the difference between generative adversarial networks (GANs) and autoencoders in deep learning?,R9,"The main goal of GANs is to generate realistic data, whereas autoencoders focus on accurately reconstructing the original data.",0.95,"Generative adversarial networks (GANs) consist of two neural networks (generator and discriminator) that compete with each other to generate realistic data samples, while autoencoders consist of an encoder and decoder network that reconstruct input data."
What is the purpose of reinforcement learning in machine learning?,R9,It allows systems to learn behaviors dynamically based on feedback from their own actions.,0.95,Reinforcement learning is a machine learning paradigm where an agent learns to make decisions by interacting with an environment to maximize cumulative rewards. It is commonly used in settings where actions influence future states and the agent learns through trial and error.
What is the difference between Q-learning and policy gradient methods in reinforcement learning?,R9,"Q-learning selects actions by comparing values, but policy gradient methods sample actions based on probability distributions.",0.9,"Q-learning is a model-free reinforcement learning algorithm that learns the optimal action-selection policy by iteratively updating a Q-value function, while policy gradient methods directly parameterize the policy and update the policy parameters based on gradients of expected rewards."
What is the purpose of data preprocessing in exploratory data analysis (EDA)?,R9,"Preprocessing helps detect trends, relationships, and anomalies that might otherwise be hidden in raw data.",0.95,"Data preprocessing in EDA involves cleaning, transforming, and preparing raw data for analysis. It includes tasks such as handling missing values, removing outliers, and encoding categorical variables."
What are some common techniques for handling missing data in EDA?,R9,Advanced methods like regression imputation or using predictive models can be used for missing value handling.,0.9,"Common techniques for handling missing data in EDA include imputation (replacing missing values with estimates), deletion (removing rows or columns with missing values), and prediction (using models to estimate missing values)."
What is the difference between univariate and bivariate analysis in EDA?,R9,"Univariate analysis helps find patterns in one column, while bivariate analysis checks how two columns might be related.",0.9,"Univariate analysis examines the distribution and summary statistics of a single variable, while bivariate analysis explores the relationship between two variables."
What is the purpose of data visualization in EDA?,R9,It supports better decision-making by giving a visual context to the data that raw numbers can’t provide.,0.9,"Data visualization in EDA helps to explore and understand the structure, patterns, and relationships in the data. It provides insights that may not be apparent from raw data or summary statistics alone."
What are some common types of data visualization used in EDA?,R9,"Bar plots, histograms, and scatter plots are widely used to explore both categorical and numerical data.",0.9,"Common types of data visualization used in EDA include histograms, scatter plots, box plots, bar charts, line plots, and heatmaps."
What is the difference between continuous and categorical variables in EDA?,R9,"You can apply mean and standard deviation to continuous data, but not usually to categorical data.",0.9,"Continuous variables are numeric variables with an infinite number of possible values within a range, while categorical variables are qualitative variables with a finite number of distinct categories or levels."
What is the purpose of statistical summaries in EDA?,R9,"They provide insights into distribution and variation, allowing analysts to decide the next steps in EDA.",0.95,"Statistical summaries in EDA provide descriptive statistics such as mean, median, mode, variance, and percentiles to summarize the central tendency, dispersion, and shape of the data distribution."
What is the difference between central tendency and dispersion measures in EDA?,R9,"Central tendency is about average outcomes, while dispersion highlights the reliability or stability of those outcomes.",0.95,"Central tendency measures (e.g., mean, median, mode) describe the central or typical value of a dataset, while dispersion measures (e.g., variance, standard deviation) quantify the spread or variability of the data around the central value."
What is the purpose of hypothesis testing in EDA?,R9,It plays a role in validating insights gained from visualizations and summary statistics in EDA.,0.9,Hypothesis testing in EDA is used to evaluate whether observed differences or relationships in the data are statistically significant or due to random chance. It involves formulating null and alternative hypotheses and conducting statistical tests to make inferences about population parameters.
What is the difference between parametric and non-parametric tests in hypothesis testing?,R9,"Parametric tests rely on parameters like mean and standard deviation, while non-parametric tests use ranks and medians.",0.9,"Parametric tests assume that the data follows a specific probability distribution (e.g., normal distribution), while non-parametric tests make no such assumptions and are based on ranks or frequencies."
What is the purpose of correlation analysis in EDA?,R9,"It helps visualize relationships using heatmaps, making it easier to explore patterns during EDA.",0.9,Correlation analysis in EDA measures the strength and direction of the linear relationship between two continuous variables. It helps to identify patterns and associations in the data.
What is the difference between Pearson correlation and Spearman correlation?,R9,Spearman is better when the data is not normally distributed or has outliers.,0.9,"Pearson correlation measures the linear relationship between two continuous variables, while Spearman correlation measures the monotonic relationship between variables, allowing for non-linear associations."
What is the purpose of regression analysis in EDA?,R9,It provides coefficients that describe the strength and direction of the effect of predictors.,0.95,Regression analysis in EDA is used to model the relationship between one or more independent variables (predictors) and a dependent variable (outcome). It helps to understand how changes in predictors affect the outcome variable.
What are the assumptions of linear regression in EDA?,R9,"It assumes independence of observations, meaning errors should not be correlated.",0.95,"The assumptions of linear regression in EDA include linearity (relationship between variables is linear), independence (residuals are independent), homoscedasticity (residuals have constant variance), and normality (residuals are normally distributed)."
What is the difference between simple linear regression and multiple linear regression?,R9,"In simple regression, the line equation is Y = a + bX. In multiple regression, it becomes Y = a + b1X1 + b2X2 + ... + bnXn, involving multiple predictors.",0.95,"Simple linear regression models the relationship between one independent variable and a dependent variable, while multiple linear regression models the relationship between multiple independent variables and a dependent variable."
What is the purpose of logistic regression in EDA?,R9,"Logistic regression provides odds ratios, which help explain how much each feature contributes to the target outcome in classification problems during EDA.",0.9,Logistic regression in EDA is used to model the relationship between one or more independent variables (predictors) and a binary outcome variable (0 or 1). It is commonly used for classification tasks.
What are the assumptions of logistic regression in EDA?,R9,"There should be no extreme outliers in the data, as they can disproportionately affect the coefficients. This is why outlier detection is useful in EDA.",0.9,"The assumptions of logistic regression in EDA include linearity (relationship between predictors and log-odds is linear), independence (observations are independent), and absence of multicollinearity (predictors are not highly correlated)."
What is the purpose of outlier detection in EDA?,R9,"Sometimes, outliers represent actual rare phenomena, so detecting them helps avoid mistakenly treating valuable insights as noise.",0.95,"Outlier detection in EDA involves identifying observations that deviate significantly from the rest of the data. Outliers can distort statistical analyses and machine learning models, so detecting and handling them appropriately is important for accurate results."
What are some common techniques for detecting outliers in EDA?,R9,"Using the IQR method, outliers are detected by identifying points below Q1 - 1.5*IQR or above Q3 + 1.5*IQR. It’s a robust and widely used method.",0.95,"Common techniques for detecting outliers in EDA include graphical methods (e.g., scatter plots, box plots), statistical methods (e.g., z-score, modified z-score), and proximity-based methods (e.g., nearest neighbors)."
What is the purpose of box plots in EDA?,R9,Box plots provide a quick visual summary of central tendency and variability. This is especially useful when comparing features or distributions across groups.,0.9,"Box plots in EDA provide a visual summary of the distribution, central tendency, and spread of a dataset. They show the median, quartiles, and potential outliers of the data distribution."
What is the purpose of histograms in EDA?,R9,"Histograms allow you to quickly assess data distribution, especially for large datasets, by grouping values into bins or intervals.",0.95,Histograms in EDA display the frequency distribution of a continuous variable by dividing the data into intervals (bins) and plotting the number of observations in each interval. They provide insights into the shape and spread of the data distribution.
What is the purpose of scatter plots in EDA?,R9,"By showing the spread of data points, scatter plots help analysts decide whether variables should be transformed or scaled before modeling.",0.9,"Scatter plots in EDA visualize the relationship between two continuous variables by plotting each data point as a point on a graph. They help to identify patterns, trends, and correlations in the data."
What is the purpose of bar charts in EDA?,R9,"Bar charts can be used to compare aggregated values, like average income across regions or count of events per category.",0.9,Bar charts in EDA represent categorical data by displaying the frequency or proportion of observations in each category as bars of varying heights. They are useful for comparing the distribution of categorical variables.
What is the purpose of line plots in EDA?,R9,"In EDA, line plots can help identify data inconsistencies, such as sudden spikes or drops, which may indicate errors or outliers.",0.9,Line plots in EDA visualize the trend or pattern of a variable over time or another continuous dimension by connecting data points with lines. They are commonly used for time series analysis and trend identification.
What is the purpose of heatmaps in EDA?,R9,"You can use heatmaps to detect clusters of features that behave similarly, which can be useful for dimensionality reduction or grouping.",0.9,Heatmaps in EDA visualize the magnitude of a variable or the strength of a relationship between variables using colors. They are useful for identifying patterns and correlations in large datasets.
What is the purpose of correlation matrices in EDA?,R9,Correlation matrices make it easier to understand the structure of the dataset by summarizing pairwise linear associations in one place.,0.9,Correlation matrices in EDA display the pairwise correlations between variables in a tabular format or heatmap. They help to identify patterns and associations among variables.
What is the difference between correlation and causation in EDA?,R9,Correlation can be positive or negative and is shown with a value from -1 to 1. Causation isn't measured with numbers the same way.,0.9,"Correlation measures the strength and direction of the linear relationship between two variables, while causation indicates that changes in one variable directly cause changes in another variable. Correlation does not imply causation."
What is the purpose of statistical tests in EDA?,R9,They help ensure that any insights or trends found during EDA are statistically valid and not just due to random noise in the data.,0.95,Statistical tests in EDA are used to assess the significance of observed differences or relationships in the data. They help to make inferences about population parameters and draw conclusions from sample data.
What is the difference between parametric and non-parametric tests in EDA?,R9,"Non-parametric tests are useful when dealing with ranked data or small sample sizes, while parametric tests are better with large, well-behaved datasets.",0.9,"Parametric tests assume that the data follows a specific probability distribution (e.g., normal distribution), while non-parametric tests make no such assumptions and are based on ranks or frequencies."
What is the purpose of ANOVA in EDA?,R9,"In EDA, ANOVA provides insight into whether a factor has a meaningful effect on a numeric outcome, which is useful before modeling.",0.9,Analysis of Variance (ANOVA) in EDA is used to compare the means of two or more groups to determine whether they are significantly different. It is commonly used for comparing means across multiple categories or treatments.
What is the difference between one-way ANOVA and two-way ANOVA in EDA?,R9,"One-way ANOVA assumes only one independent variable influences the dependent variable, while two-way ANOVA tests if two independent variables and their interaction affect it.",0.9,"One-way ANOVA compares the means of two or more groups based on a single categorical factor, while two-way ANOVA considers the effects of two categorical factors (main effects and interaction) on the outcome variable."
What is the purpose of chi-squared test in EDA?,R9,It compares observed frequencies with expected frequencies to test hypotheses about categorical data relationships.,0.9,The chi-squared test in EDA is used to assess the independence between two categorical variables by comparing the observed frequencies to the expected frequencies under the null hypothesis of independence.
What is the purpose of t-tests in EDA?,R9,T-tests compare the variances of two groups to check if the spread of data is similar.,0.3,T-tests in EDA are used to compare the means of two independent samples or the mean of a sample to a known value. They help to determine whether observed differences in means are statistically significant.
What is the difference between independent samples t-test and paired samples t-test in EDA?,R9,Both tests can be used interchangeably as they serve the same purpose of comparing group means in EDA.,0.1,"Independent samples t-test compares the means of two independent groups, while paired samples t-test compares the means of two related groups (e.g., before and after treatment) based on matched pairs of observations."
What is the purpose of non-parametric tests in EDA?,R9,They replace parametric tests in EDA because they always give more accurate results regardless of data type.,0.2,"Non-parametric tests in EDA are used when the data does not meet the assumptions of parametric tests (e.g., normal distribution, homogeneity of variance). They provide robust alternatives for analyzing non-normally distributed or skewed data."
What is the difference between Wilcoxon signed-rank test and Mann-Whitney U test in EDA?,R9,"Wilcoxon signed-rank test works for small sample sizes in paired data, and Mann-Whitney U test works well for independent samples of any size.",0.9,"Wilcoxon signed-rank test is a non-parametric test used to compare the medians of two related groups, while Mann-Whitney U test compares the medians of two independent groups."
What is the purpose of exploratory factor analysis (EFA) in EDA?,R9,The main purpose of EFA is to clean missing values from datasets by identifying correlated groups.,0.2,Exploratory factor analysis (EFA) in EDA is used to identify underlying factors or latent variables that explain the common variance among observed variables. It helps to uncover the underlying structure of a dataset.
What is the difference between EFA and principal component analysis (PCA) in EDA?,R9,"PCA is a supervised method using target variables, while EFA is unsupervised and does not use outcome variables.",0.1,"EFA and PCA are both dimensionality reduction techniques used in EDA, but EFA aims to identify underlying factors that explain the common variance among observed variables, while PCA aims to capture the maximum variance in the data with orthogonal principal components."
What is the purpose of cluster analysis in EDA?,R9,Cluster analysis is used to test hypotheses about the relationships between variables.,0.2,Cluster analysis in EDA is used to identify groups or clusters of similar observations based on their characteristics or features. It helps to uncover natural groupings in the data and identify patterns or relationships.
What are some common algorithms used for cluster analysis in EDA?,R9,DBSCAN requires setting parameters like epsilon and minimum points to identify clusters and noise.,0.9,"Common algorithms used for cluster analysis in EDA include K-means clustering, hierarchical clustering, DBSCAN (Density-Based Spatial Clustering of Applications with Noise), and Gaussian mixture models."
What is the difference between K-means clustering and hierarchical clustering in EDA?,R9,"K-means uses centroids to define clusters, whereas hierarchical clustering uses distance measures between points or clusters.",0.95,"K-means clustering partitions the data into K clusters by minimizing the within-cluster sum of squares, while hierarchical clustering builds a tree-like hierarchy of clusters by recursively merging or splitting clusters based on proximity."
What is the purpose of dendrogram in hierarchical clustering in EDA?,R9,A dendrogram is a tool to convert categorical data into numerical format for clustering.,0.1,A dendrogram in hierarchical clustering visualizes the hierarchical relationships between clusters by displaying the merging order and distance between clusters. It helps to identify the optimal number of clusters and interpret the clustering results.
What is the purpose of silhouette analysis in cluster validation in EDA?,R9,The silhouette method replaces dimensionality reduction techniques like PCA in EDA.,0.1,"Silhouette analysis in cluster validation assesses the quality of clustering results by calculating silhouette coefficients for each data point, which measure the cohesion within clusters and separation between clusters. Higher silhouette coefficients indicate better-defined clusters."
What is the purpose of dimensionality reduction techniques in EDA?,R9,Dimensionality reduction replaces data normalization in the preprocessing pipeline.,0.1,Dimensionality reduction techniques in EDA are used to reduce the number of features or variables in a dataset while preserving the most important information. They help to overcome the curse of dimensionality and improve computational efficiency.
What are some common dimensionality reduction techniques used in EDA?,R9,"PCA assumes linear relationships among features, while t-SNE handles nonlinear relationships better.",0.9,"Common dimensionality reduction techniques used in EDA include principal component analysis (PCA), factor analysis, t-distributed stochastic neighbor embedding (t-SNE), and linear discriminant analysis (LDA)."
What is the purpose of PCA in EDA?,R9,PCA helps in feature extraction by combining correlated variables into fewer components without losing much information.,0.9,Principal Component Analysis (PCA) in EDA is used to reduce the dimensionality of a dataset by transforming the original features into a new set of orthogonal principal components. It helps to identify the underlying structure and patterns in high-dimensional data.
What is the difference between PCA and factor analysis in EDA?,R9,"PCA can be used with categorical data, but factor analysis only works with numerical data.",0.2,"PCA and factor analysis are both dimensionality reduction techniques used in EDA, but PCA aims to capture the maximum variance in the data with orthogonal principal components, while factor analysis aims to identify underlying factors that explain the common variance among observed variables."
What is the purpose of t-SNE in EDA?,R9,It transforms categorical data into numerical vectors for easier analysis.,0.1,t-Distributed Stochastic Neighbor Embedding (t-SNE) in EDA is a technique used for visualizing high-dimensional data in a lower-dimensional space while preserving the local structure and relationships between data points. It is commonly used for visualizing clusters and patterns in complex datasets.
What is the difference between linear discriminant analysis (LDA) and PCA in EDA?,R9,"PCA requires knowing the target variable, but LDA does not.",0.1,"Linear Discriminant Analysis (LDA) and PCA are both dimensionality reduction techniques used in EDA, but LDA aims to maximize the separation between classes or groups while reducing dimensionality, whereas PCA focuses on capturing the maximum variance in the data regardless of class labels."
What is the purpose of feature selection in EDA?,R9,It transforms categorical data into numerical data to prepare for modeling.,0.2,"Feature selection in EDA involves selecting the most relevant features from a dataset to improve model performance, reduce overfitting, and enhance interpretability. It helps to identify the subset of features that contribute the most to predicting the target variable."
What are some common feature selection techniques used in EDA?,R9,Feature selection involves normalizing features before using them in machine learning models.,0.1,"Common feature selection techniques used in EDA include filter methods (e.g., correlation analysis, information gain), wrapper methods (e.g., recursive feature elimination, forward/backward selection), and embedded methods (e.g., LASSO, decision trees)."
What is the purpose of ensemble methods in EDA?,R9,They help in choosing the best single model by comparing performances.,0.3,"Ensemble methods in EDA combine multiple models or algorithms to improve predictive performance, robustness, and generalization. They help to reduce overfitting and capture diverse patterns in the data."
What are some common ensemble methods used in EDA?,R9,Ensemble methods replace data normalization and scaling steps during preprocessing.,0.1,"Common ensemble methods used in EDA include bagging (Bootstrap Aggregating), boosting (e.g., AdaBoost, Gradient Boosting), and stacking (meta-learning). They combine predictions from multiple base models to make more accurate and reliable predictions."
What is the purpose of model evaluation in EDA?,R9,It is used to detect outliers in the dataset by measuring model residuals.,0.2,Model evaluation in EDA involves assessing the performance of predictive models using various metrics and techniques. It helps to determine the effectiveness of the models in capturing patterns and making accurate predictions on unseen data.
What are some common metrics used for model evaluation in EDA?,R9,ROC curve is a visual metric but doesn’t provide quantitative evaluation.,0.3,"Common metrics used for model evaluation in EDA include accuracy, precision, recall, F1 score, ROC-AUC score, mean squared error (MSE), and R-squared (R2) score. They provide insights into different aspects of model performance."
What is the difference between cross-validation and holdout validation in model evaluation in EDA?,R9,Cross-validation is a data augmentation technique used before training models.,0.1,"Cross-validation in model evaluation splits the data into multiple subsets (folds) for training and testing, while holdout validation splits the data into two disjoint sets (training and testing). Cross-validation provides more reliable estimates of model performance but is computationally more expensive."
What is the purpose of hyperparameter tuning in model evaluation in EDA?,R9,Hyperparameter tuning is the process of changing the dataset size to improve learning.,0.1,Hyperparameter tuning in model evaluation involves optimizing the hyperparameters of machine learning algorithms to improve model performance and generalization. It helps to find the best combination of hyperparameters for a given dataset and problem.
What are some common techniques for hyperparameter tuning in EDA?,R9,AutoML tools automate hyperparameter tuning using advanced optimization algorithms.,0.9,"Common techniques for hyperparameter tuning in EDA include grid search, random search, Bayesian optimization, and genetic algorithms. They systematically search the hyperparameter space to find the optimal values for model performance."
What is the purpose of model interpretation in EDA?,R9,Its main role is to measure model performance metrics like accuracy and F1 score before choosing the best algorithm.,0.2,"Model interpretation in EDA involves understanding and explaining how predictive models make decisions and predictions. It helps to gain insights into the underlying patterns, relationships, and features that contribute to the model's predictions."
What are some common techniques for model interpretation in EDA?,R9,Global Surrogate Models can be used to approximate a black-box model using a simpler interpretable model like a decision tree.,0.9,"Common techniques for model interpretation in EDA include feature importance analysis (e.g., permutation importance, SHAP values), partial dependence plots, individual conditional expectation (ICE) plots, and model-agnostic interpretation methods (e.g., LIME, SHAP)."
What is the purpose of model deployment in EDA?,R9,Model deployment is unrelated to EDA; it is only used in big data pipelines to stream live data to machine learning models.,0.2,Model deployment in EDA involves deploying predictive models into production environments to make real-time predictions on new data. It allows organizations to leverage the insights and predictions generated from data analysis for decision-making and business applications.
What are some common techniques for model deployment in EDA?,R9,Model deployment in EDA is mostly manual and only involves exporting plots and charts to be reused later in the training phase.,0.3,"Common techniques for model deployment in EDA include deploying models as web services or APIs, embedding models into applications or dashboards, and integrating models with existing business processes or workflows. They enable seamless integration of predictive models into operational systems and workflows."
What are the common types of regularization used in machine learning?,R9,Batch Normalization and Min-Max Scaling are both regularization techniques used to improve model generalization and reduce noise.,0.2,"Common types of regularization used in machine learning include L1 regularization (Lasso), L2 regularization (Ridge), and elastic net regularization (combination of L1 and L2 penalties)."
What is the bias-variance trade-off in machine learning?,R9,"Bias and variance are the same thing. Reducing one automatically reduces the other, so there's no real trade-off.",0.1,"The bias-variance trade-off refers to the balance between bias (error due to overly simplistic assumptions) and variance (error due to sensitivity to fluctuations in the training data) in machine learning models. Reducing bias typically increases variance, and vice versa."
What are the common types of cross-validation techniques used in machine learning?,R9,"Cross-validation methods like bootstrap, k-fold, and LOOCV help reduce model bias and give better generalization performance.",0.9,"Common types of cross-validation techniques used in machine learning include k-fold cross-validation, stratified k-fold cross-validation, leave-one-out cross-validation, and nested cross-validation."
What is the purpose of hyperparameter tuning in machine learning?,R9,Tuning helps automate data cleaning tasks by identifying which features are noisy or redundant.,0.2,Hyperparameter tuning in machine learning involves optimizing the hyperparameters of models to improve their performance and generalization. It helps to find the best combination of hyperparameters for a given dataset and problem.
What are some common techniques for hyperparameter tuning in machine learning?,R9,Automated machine learning (AutoML) tools often handle hyperparameter tuning using advanced strategies like Bayesian or evolutionary search.,0.95,"Common techniques for hyperparameter tuning in machine learning include grid search, random search, Bayesian optimization, genetic algorithms, and gradient-based optimization methods. They systematically search the hyperparameter space to find optimal values."
What is the difference between supervised and unsupervised learning in machine learning?,R9,"Supervised learning is like teaching with answers provided, while unsupervised learning is like giving a puzzle with no final picture.",0.9,"Supervised learning involves learning from labeled data with input-output pairs, where the model predicts the output variable based on input features. Unsupervised learning involves learning from unlabeled data to discover hidden patterns or structures in the data."
What are some common dimensionality reduction techniques used in machine learning?,R9,Feature selection techniques like Recursive Feature Elimination (RFE) are also considered dimensionality reduction methods.,0.9,"Common dimensionality reduction techniques used in machine learning include principal component analysis (PCA), linear discriminant analysis (LDA), t-distributed stochastic neighbor embedding (t-SNE), and autoencoders."
What is the purpose of model interpretation in machine learning?,R9,Understanding model behavior helps detect bias and improve fairness in predictions.,0.95,"Model interpretation in machine learning involves understanding and explaining how predictive models make decisions and predictions. It helps to gain insights into the underlying patterns, relationships, and features that contribute to the model's predictions."
What are some common techniques for model interpretation in machine learning?,R9,"Using SHAP values allows us to see the exact impact of each feature on individual predictions, making it highly interpretable.",1.0,"Common techniques for model interpretation in machine learning include feature importance analysis (e.g., permutation importance, SHAP values), partial dependence plots, individual conditional expectation (ICE) plots, and model-agnostic interpretation methods (e.g., LIME, SHAP)."
What is the purpose of model deployment in machine learning?,R9,We deploy models to visualize training loss and accuracy curves after the model has been trained.,0.2,Model deployment in machine learning involves deploying predictive models into production environments to make real-time predictions on new data. It allows organizations to leverage the insights and predictions generated from data analysis for decision-making and business applications.
What are some common techniques for model deployment in machine learning?,R9,CI/CD pipelines are used to automate the deployment process and ensure reliable updates to models in production.,0.9,"Common techniques for model deployment in machine learning include deploying models as web services or APIs, embedding models into applications or dashboards, and integrating models with existing business processes or workflows. They enable seamless integration of predictive models into operational systems and workflows."
What is the purpose of transfer learning in machine learning?,R9,"With transfer learning, we can achieve good results with fewer computing resources by leveraging knowledge from previously trained models.",0.9,"Transfer learning in machine learning is a technique where a model trained on one task is reused as the starting point for a model on a related task. It enables the transfer of knowledge and features learned from one domain to another, reducing the need for large labeled datasets."
What are some common applications of transfer learning in machine learning?,R9,Transfer learning allows robotics systems to adapt motion control or object recognition using knowledge from other trained environments.,0.9,"Common applications of transfer learning in machine learning include image classification, natural language processing (NLP), and speech recognition. Pre-trained models such as BERT, ResNet, and GPT are often used as starting points for transfer learning tasks."
What is the purpose of feature engineering in machine learning?,R9,We perform feature engineering to increase training time and make the model more complex.,0.3,"Feature engineering involves selecting, transforming, or creating new features from the raw data to improve the performance of machine learning models by making them more expressive, informative, and suitable for the task at hand."
What are some common techniques for feature engineering?,R9,The best technique is to randomly shuffle feature values to increase model generalization.,0.2,"Common techniques include one-hot encoding for categorical variables, scaling numerical features, creating interaction terms, binning or discretizing continuous variables, and extracting features from text or images using techniques like TF-IDF or CNNs."
What is the difference between batch processing and real-time processing?,R9,"Batch jobs run at set times and can take longer, whereas real-time processing requires fast computation with low latency.",0.95,"Batch processing involves processing data in large, discrete batches or groups, typically on a scheduled or periodic basis, while real-time processing involves handling data as soon as it is generated or received, often requiring low-latency and immediate responses."
What is transfer learning in NLP?,R9,"Pre-trained models used in transfer learning already have linguistic knowledge, which helps boost performance on downstream tasks.",0.95,"Transfer learning in NLP involves leveraging pre-trained language models, such as BERT or GPT, trained on large corpora of text data, and fine-tuning them on specific tasks or domains with smaller datasets to achieve better performance and faster convergence."
What is named entity recognition (NER)?,R9,NER helps in structuring unstructured text by identifying specific named elements relevant to the context.,0.95,"Named entity recognition is a subtask of information extraction that involves identifying and classifying named entities (e.g., persons, organizations, locations) mentioned in text documents, which is useful for tasks such as entity linking and relation extraction."
What are some challenges in working with unstructured data?,R9,Lack of clear labeling and the need for specialized tools to extract insights are key challenges in working with unstructured data.,0.95,"Challenges include extracting meaningful information from unstructured text, handling noisy or inconsistent data, dealing with large volumes of data, and ensuring privacy and security when working with sensitive information."
What is collaborative filtering in recommendation systems?,R9,"It's different from content-based filtering, which uses item features; collaborative filtering relies on user behavior.",0.95,"Collaborative filtering is a technique used in recommendation systems to generate personalized recommendations by analyzing similarities and patterns in users' behavior or preferences, often based on user-item interactions or user-user similarity."
What are some evaluation metrics used in recommendation systems?,R9,MAP and MRR are used to evaluate ranked lists of recommendations where the order matters.,0.95,"Evaluation metrics include precision, recall, F1 score, Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and ranking-based metrics like Mean Average Precision (MAP) and Normalized Discounted Cumulative Gain (NDCG)."
What is the Markov Decision Process (MDP)?,R9,MDPs are used in reinforcement learning to help agents make sequences of decisions in uncertain environments.,0.95,"The Markov Decision Process is a mathematical framework used to model sequential decision-making problems, where an agent takes actions in an environment to maximize cumulative rewards, while satisfying the Markov property (future states depend only on the current state and action)."
What is policy gradient in reinforcement learning?,R9,It works by estimating the gradient of the reward with respect to the policy parameters and updating them accordingly.,0.95,"Policy gradient methods are a class of reinforcement learning algorithms that directly optimize the policy (the agent's strategy or behavior) by estimating gradients of expected rewards with respect to the policy parameters, typically using techniques like stochastic gradient ascent."
What is data augmentation in deep learning?,R9,"Data augmentation helps simulate real-world variations, making the model more adaptable during testing.",0.95,"Data augmentation is a technique used to artificially increase the size and diversity of training datasets by applying transformations such as rotation, translation, scaling, cropping, or flipping to input data, which can improve model generalization and robustness."
What is sequence-to-sequence learning?,R9,"It uses RNNs, LSTMs, or Transformers to learn the mapping between input and output sequences over time.",0.95,"Sequence-to-sequence learning is a type of model architecture used for tasks involving input and output sequences of variable lengths, such as machine translation, summarization, and speech recognition, typically implemented using recurrent neural networks (RNNs) or transformers."
What is attention mechanism in deep learning?,R9,Self-attention is a type of attention mechanism where the model relates different positions of a single sequence to compute representations.,0.95,"Attention mechanism is a mechanism used in neural networks to selectively focus on relevant parts of the input data while processing sequences, enabling the model to learn to weigh different input elements dynamically and attend to the most informative parts."
What are some common optimization algorithms used in deep learning?,R9,SGD with momentum helps models escape local minima by adding a fraction of the previous update to the current one.,0.95,"Common optimization algorithms include stochastic gradient descent (SGD), Adam, RMSprop, and Adagrad, each with different update rules and learning rate adaptation strategies to optimize the model parameters efficiently."
What is semi-supervised learning?,R9,It is often used in situations like web classification or text mining where labels are scarce.,0.95,"Semi-supervised learning is a machine learning paradigm that combines labeled and unlabeled data to improve model performance, often by leveraging the inherent structure or relationships in the data to semi-supervised learning algorithms."
What is self-supervised learning?,R9,"It helps the model learn general patterns by solving pretext tasks, which makes it easier to adapt to new tasks later.",0.95,"Self-supervised learning is a form of unsupervised learning where the model learns to predict or generate certain properties or features of the input data itself, without requiring external labels or annotations, often used as a pretraining step for downstream tasks."
What is the Bellman equation in reinforcement learning?,R9,"It describes how the agent should always choose the action that leads to the highest immediate reward, ignoring future rewards.",0.3,The Bellman equation is a fundamental equation in dynamic programming and reinforcement learning that expresses the value of a state or state-action pair in terms of the expected immediate reward and the value of the next state or next state-action pair.
What is the difference between on-policy and off-policy learning in reinforcement learning?,R9,"Off-policy can learn from historical data or demonstrations, making it more flexible than on-policy.",0.95,"On-policy learning involves learning the value or policy while following the current policy, while off-policy learning involves learning the value or policy while following a different behavior policy, often leading to more efficient exploration and better sample efficiency."
What are the main components of the data science process?,R9,"It includes collecting data, cleaning it, training machine learning models, and making predictions.",0.95,"The main components include data collection, data cleaning and preprocessing, exploratory data analysis, modeling, evaluation, and deployment."
What is overfitting in machine learning?,R9,"It is the case where the model fits noise and outliers, resulting in bad generalization.",1.0,Overfitting occurs when a model learns the details and noise in the training data to the extent that it negatively impacts its performance on unseen data.
Explain the curse of dimensionality.,R9,It causes algorithms to run faster because there are more dimensions to separate the data.,0.2,"The curse of dimensionality refers to the increased difficulty of analyzing and processing data as the number of features or dimensions increases, leading to sparsity and computational challenges."
What is the purpose of cross-validation?,R9,Cross-validation automatically selects the best features to use in the model.,0.3,"Cross-validation is used to assess the generalization performance of a model by partitioning the data into multiple subsets, training the model on some subsets, and evaluating it on the remaining subsets."
How does regularization prevent overfitting in neural networks?,R9,It adds random numbers to the weights so they don't become too big and overfit.,0.3,"Regularization techniques such as L1 and L2 regularization add penalty terms to the neural network's loss function, which discourages overly complex weight configurations and helps prevent overfitting."
What is the purpose of activation functions in neural networks?,R9,The main purpose is to compress the output values between 0 and 1 always.,0.4,"Activation functions introduce nonlinearity to neural networks, allowing them to learn complex patterns and relationships in the data by transforming the input signal from each neuron into an output signal."
What is the purpose of dimensionality reduction techniques?,R9,The purpose is to randomly remove features to prevent bias.,0.2,"Dimensionality reduction techniques are used to reduce the number of features or dimensions in a dataset while preserving its important information, which can help improve the performance and efficiency of machine learning algorithms."
Explain the difference between PCA and t-SNE.,R9,t-SNE can sometimes create misleading clusters because it focuses on local relationships.,0.9,"PCA (Principal Component Analysis) is a linear dimensionality reduction technique that seeks to maximize variance, while t-SNE (t-Distributed Stochastic Neighbor Embedding) is a nonlinear technique that focuses on preserving local relationships between data points."
What is K-means clustering?,R9,K-means always finds the global optimal clustering solution.,0.3,"K-means clustering is an unsupervised learning algorithm used to partition a dataset into K clusters based on similarities in the data points' features, with the goal of minimizing the within-cluster sum of squares."
What is the elbow method used for in K-means clustering?,R9,It determines the cluster labels after running K-means.,0.3,"The elbow method is used to determine the optimal number of clusters (K) in K-means clustering by plotting the within-cluster sum of squares against the number of clusters and selecting the point where the rate of decrease sharply changes (the ""elbow"" point)."
What is outlier detection?,R9,The process filters out the most common values to focus on rare data.,0.3,"Outlier detection is the process of identifying data points or observations that deviate significantly from the rest of the dataset, which may indicate errors, anomalies, or interesting patterns."
What are the advantages of deep learning over traditional machine learning algorithms?,R9,"Deep learning models benefit from end-to-end learning, meaning they can learn the entire pipeline from input to output directly, which simplifies model design.",1.0,"Deep learning models, particularly neural networks, can automatically learn hierarchical representations of data, handle large and complex datasets, and often achieve state-of-the-art performance in tasks such as image recognition and natural language processing."
What is backpropagation?,R9,"Backpropagation doesn’t require gradients or derivatives, it just changes weights using fixed values for all neurons.",0.1,Backpropagation is a supervised learning algorithm used to train neural networks by iteratively adjusting the weights of connections between neurons in the network based on the error between predicted and actual outputs.
How do you evaluate the performance of a regression model?,R9,"Another way to evaluate regression models is to visualize residual plots and check if the errors are randomly distributed, which suggests a good fit.",1.0,"Performance metrics for regression models include Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), R-squared (coefficient of determination), and others."
What is the purpose of the A/B test in data science?,R9,A/B testing is mostly used in supervised learning to find out which class label is more frequent.,0.1,"A/B testing is a statistical hypothesis testing method used to compare two or more versions of a product, webpage, or other elements to determine which one performs better in terms of predefined metrics."
What is Bayesian inference?,R9,It’s a rule for calculating the frequency of an event based on how often it happened in the past. It doesn’t change with new data.,0.1,"Bayesian inference is a statistical approach that uses Bayes' theorem to update the probability of a hypothesis or belief based on new evidence or observations, incorporating prior knowledge and uncertainty into the analysis."
What is the purpose of hypothesis testing?,R9,It helps in validating assumptions or claims made about a dataset by checking how likely the sample result would occur if the null hypothesis were true.,1.0,"Hypothesis testing is used to make inferences or decisions about a population parameter based on sample data, by testing a null hypothesis against an alternative hypothesis using statistical methods."
What is regularization in linear regression?,R9,The idea of regularization is to balance the bias-variance tradeoff by penalizing large weights in the linear model.,1.0,"Regularization in linear regression involves adding a penalty term to the ordinary least squares (OLS) loss function to prevent overfitting, with common regularization techniques including Ridge regression (L2 regularization) and Lasso regression (L1 regularization)."
What are the assumptions of linear regression?,R9,It’s assumed that outliers do not affect the model at all because linear regression automatically removes them.,0.2,"The assumptions of linear regression include linearity between the independent and dependent variables, homoscedasticity (constant variance of errors), independence of errors, and normality of error terms."
What is the difference between Type I and Type II errors?,R9,"The difference is that Type I is caused by small sample sizes, while Type II is caused by large sample sizes.",0.3,"Type I error occurs when a true null hypothesis is rejected (false positive), while Type II error occurs when a false null hypothesis is not rejected (false negative)."
What is the p-value in hypothesis testing?,R9,The p-value is always greater than 0.05 in a valid hypothesis test.,0.1,"The p-value is the probability of observing a test statistic as extreme as, or more extreme than, the one observed, assuming that the null hypothesis is true. It is used to assess the strength of evidence against the null hypothesis."
What is the bias-variance tradeoff?,R9,"It means that as you try to decrease bias in your model, variance might increase, and vice versa.",1.0,"The bias-variance tradeoff refers to the compromise between bias (error due to overly simplistic assumptions) and variance (error due to sensitivity to fluctuations in the training data) in machine learning models, where reducing one typically increases the other."
What is the difference between variance and standard deviation?,R9,"Standard deviation is used in machine learning, but variance is only used in physics.",0.1,"Variance measures the average squared deviation from the mean of a set of values, while standard deviation measures the average deviation from the mean, providing a measure of the dispersion or spread of the data."
What is the difference between a decision tree and a random forest?,R9,"A random forest uses deep trees only, while decision trees can be shallow or deep depending on parameters.",0.3,"A decision tree is a simple, interpretable tree-like structure that recursively splits the data based on features, while a random forest is an ensemble of decision trees that aggregates their predictions to improve performance and reduce overfitting."
What is batch normalization in neural networks?,R9,"It normalizes activations across the mini-batch, then applies learned scaling and shifting parameters.",1.0,"Batch normalization is a technique used to normalize the activations of each layer in a neural network by adjusting and scaling them to have zero mean and unit variance, which can accelerate training and improve model performance."
What is the purpose of a confusion matrix in classification?,R9,A confusion matrix is a scoring function used to regularize the model.,0.2,"A confusion matrix is a table that summarizes the performance of a classification model by comparing predicted class labels with actual class labels, showing the counts of true positives, true negatives, false positives, and false negatives."
What is the Kullback-Leibler (KL) divergence?,R9,KL divergence helps in variational inference by measuring the gap between the true and approximate distributions.,1.0,"The Kullback-Leibler divergence is a measure of the difference between two probability distributions, used in information theory and statistics to quantify the amount of information lost when one distribution is used to approximate another."
What is the difference between batch normalization and layer normalization in neural networks?,R9,Batch normalization is always faster than layer normalization regardless of the data type or model architecture.,0.3,"Batch normalization normalizes the activations of each layer across the batch dimension, while layer normalization normalizes the activations across the feature dimension, making it more suitable for recurrent neural networks and other architectures with variable-length sequences."
What is unsupervised learning?,R9,It helps in finding hidden patterns or groupings in datasets without supervision.,1.0,"Unsupervised learning is a type of machine learning where the model is trained on unlabeled data, and it seeks to find patterns or structures in the data without explicit supervision."
What are the main types of unsupervised learning techniques?,R9,Naive Bayes and Random Forest are popular unsupervised learning algorithms.,0.1,"The main types include clustering, dimensionality reduction, and association rule learning."
What are the common clustering algorithms?,R9,Gaussian Mixture Models assume data is generated from a mixture of several Gaussian distributions and assign probabilities to cluster membership.,1.0,"Common clustering algorithms include K-means clustering, hierarchical clustering, DBSCAN, and Gaussian mixture models (GMM)."
What is hierarchical clustering?,R9,"Hierarchical clustering produces a dendrogram, which helps identify the number of natural clusters in the data.",1.0,Hierarchical clustering is an algorithm that creates a hierarchy of clusters by recursively merging or splitting clusters based on their similarity or dissimilarity.
What is DBSCAN clustering?,R9,"It’s useful for detecting clusters in spatial and high-dimensional data, especially when the density varies.",1.0,"DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a density-based clustering algorithm that groups together data points that are closely packed, while marking outliers as noise."
What are the common dimensionality reduction techniques?,R9,Feature selection methods like backward elimination and L1 regularization can also reduce dimensions.,1.0,"Common techniques include Principal Component Analysis (PCA), t-Distributed Stochastic Neighbor Embedding (t-SNE), and Singular Value Decomposition (SVD)."
What is PCA (Principal Component Analysis)?,R9,PCA increases the number of dimensions to ensure more detailed feature extraction.,0.1,PCA is a linear dimensionality reduction technique that transforms the original features into a lower-dimensional space while preserving the maximum variance in the data.
What is t-SNE (t-Distributed Stochastic Neighbor Embedding)?,R9,t-SNE is only used for numerical data and can't work on text or images.,0.2,"t-SNE is a nonlinear dimensionality reduction technique that focuses on preserving local relationships between data points in a lower-dimensional space, often used for visualizing high-dimensional data."
What is Singular Value Decomposition (SVD)?,R9,It’s similar to PCA in the sense that both are used for dimensionality reduction.,1.0,"SVD is a matrix factorization technique used to decompose a matrix into the product of three matrices, which can be used for dimensionality reduction, data compression, and matrix approximation."
What is association rule learning?,R9,It predicts continuous values based on past associations found in data.,0.2,"Association rule learning is a type of unsupervised learning used to discover interesting associations or relationships between variables in large datasets, commonly used in market basket analysis and recommendation systems."
What are the common association rule learning algorithms?,R9,These algorithms are commonly applied in market basket analysis to find product purchase patterns.,1.0,Common algorithms include Apriori and FP-Growth.
What is the Apriori algorithm?,R9,Apriori is mainly used in supervised learning models like classification and regression.,0.1,"The Apriori algorithm is a classic algorithm for mining frequent itemsets and generating association rules, based on the principle of the Apriori property and candidate generation."
What is FP-Growth?,R9,FP-Growth compresses the dataset using neural networks before applying association rule learning.,0.1,"FP-Growth (Frequent Pattern Growth) is an efficient algorithm for mining frequent itemsets and generating association rules without candidate generation, using a compact data structure called FP-tree."
What is anomaly detection?,R9,"It’s used in many fields like finance, cybersecurity, and health monitoring.",1.0,"Anomaly detection, also known as outlier detection, is the process of identifying data points or observations that deviate significantly from the rest of the dataset, which may indicate errors, anomalies, or interesting patterns."
What are the common anomaly detection techniques?,R9,Anomaly detection is usually done using supervised learning techniques only.,0.2,"Common techniques include statistical methods, density-based methods, distance-based methods, and machine learning-based methods such as Isolation Forest and One-Class SVM."
What is Isolation Forest?,R9,It’s similar to k-means clustering but with added randomness to detect outliers.,0.2,"Isolation Forest is an unsupervised learning algorithm for anomaly detection that isolates anomalies by recursively partitioning the dataset into subsets using random splits, making anomalies easier to isolate than normal data points."
What is One-Class SVM?,R9,One-Class SVM is less effective when normal and anomalous data overlap significantly.,0.9,One-Class Support Vector Machine (SVM) is a machine learning algorithm for anomaly detection that learns a boundary around normal data points in the feature space and identifies anomalies as data points lying outside this boundary.
What is the purpose of density estimation?,R9,It is mainly used for supervised learning to improve model predictions.,0.2,"Density estimation is the process of estimating the probability density function of a random variable from a set of data points, often used in clustering, anomaly detection, and generative modeling."
What are the common density estimation techniques?,R9,"Parametric methods assume a specific form for the data distribution, while non-parametric methods do not.",1.0,"Common techniques include histogram-based methods, kernel density estimation (KDE), Gaussian mixture models (GMM), and Parzen window estimation."
What is the Gaussian Mixture Model (GMM)?,R9,It estimates cluster membership probabilities rather than hard assignments.,1.0,"Gaussian Mixture Model (GMM) is a probabilistic model used for density estimation and clustering, assuming that the data is generated from a mixture of several Gaussian distributions."
What is the Expectation-Maximization (EM) algorithm?,R9,The algorithm is slow because it requires computing gradients for every data point in each iteration.,0.3,"The Expectation-Maximization (EM) algorithm is an iterative optimization algorithm used to estimate the parameters of probabilistic models with latent variables, such as Gaussian mixture models (GMM), by iteratively computing the expected values of the latent variables and maximizing the likelihood function."
What is the difference between generative and discriminative models?,R9,Generative models simulate data samples but cannot classify data points.,0.2,"Generative models learn the joint probability distribution of the input features and the output labels, while discriminative models learn the conditional probability distribution of the output labels given the input features."
What is the purpose of generative modeling?,R9,The purpose is to compress data by reducing its dimensionality.,0.3,"Generative modeling is used to model the underlying structure of the data and generate new samples from the learned distribution, often used in tasks such as image generation, text generation, and data augmentation."
What is data science?,R10,"Data science is an interdisciplinary field that extracts meaningful insights from data by using statistical analysis, machine learning models, and domain-specific techniques. It plays a vital role in solving real-world problems by enabling data-driven decision-making.",1.0,"Data science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from data."
What are the key steps in the data science process?,R10,"Data science has several steps like understanding the business problem, data collection, data cleaning, data exploration, model selection, model training, testing, and finally deployment and maintenance. Each step helps ensure reliable and impactful insights are generated.",0.95,"The key steps typically include problem definition, data collection, data preparation, exploratory data analysis, modeling, evaluation, and deployment."
What is the difference between supervised and unsupervised learning?,R10,"Supervised learning is for prediction problems and unsupervised is for problems where you just want to explore the data. Supervised needs answers; unsupervised doesn’t. I think reinforcement learning is kind of like unsupervised, but I’m not sure.",0.7,"Supervised learning involves training a model on labeled data, where the algorithm learns the relationship between input features and target labels. Unsupervised learning deals with unlabeled data and aims to find hidden patterns or structures within the data."
Explain the bias-variance tradeoff.,R10,"I believe bias is like a mistake in the coding and variance is a difference in data. When they are both balanced, we get the most accurate results. If not balanced, the model either memorizes or forgets everything.",0.4,The bias-variance tradeoff is the balance between the error introduced by bias (underfitting) and the error introduced by variance (overfitting) when building a machine learning model.
What is feature engineering?,R10,"I think feature engineering is similar to data visualization. We draw graphs and plots to see which features are most important. After that, we can remove the useless ones and use only the features that look good in plots.",0.35,"Feature engineering is the process of selecting, transforming, and creating features from raw data to improve model performance."
How do you handle missing data in a dataset?,R10,"I usually open the CSV file and manually type in values that look reasonable. This way, I’m sure there are no blanks left and I don’t need to use any coding or statistical techniques.",0.25,"Missing data can be handled by imputation techniques such as mean, median, or mode imputation, using predictive models to estimate missing values, or by deleting rows or columns with missing values, depending on the dataset size and nature."
What is cross-validation?,R10,"The idea behind cross-validation is to divide the dataset into equal chunks, use some for training and others for testing, and then rotate them. This ensures the model is tested fairly and gives a more accurate measure of how it will perform.",0.95,Cross-validation is a technique used to assess the performance of a predictive model by splitting the data into multiple subsets.
Explain the concept of overfitting.,R10,"If your training error is high and test error is low, it’s called overfitting. It means your model is not learning properly and is overcompensating for the test data. You should train more to reduce it.",0.25,"Overfitting occurs when a model learns the training data too well, capturing noise or random fluctuations in the data instead of the underlying pattern."
What are some common algorithms used in supervised learning?,R10,"Supervised learning is done by using algorithms such as KNN and K-means, both of which look at the nearest points in the dataset to make predictions. They are useful in identifying groups or predicting values.",0.4,"Common algorithms include linear regression, logistic regression, decision trees, random forests, support vector machines, and neural networks."
What is dimensionality reduction?,R10,"In my understanding, dimensionality reduction means training a model on only a few features instead of using all of them. It’s done to save time and avoid confusion during the learning process.",0.6,Dimensionality reduction is the process of reducing the number of input variables in a dataset by transforming them into a lower-dimensional space while preserving important information.
What is regularization in machine learning?,R10,"I believe regularization is about regularly updating the learning rate during training. If the learning rate is too high or too low, the model fails. Regularization keeps it consistent and under control.",0.3,Regularization is a technique used to prevent overfitting by adding a penalty term to the model's objective function.
What is ensemble learning?,R10,Ensemble learning just means using one model at a time in a sequence. Each model gives its answer and then you take the last one as the final result. It's mainly used to reduce processing time.,0.3,Ensemble learning is a technique that combines multiple machine learning models to improve performance and robustness.
What is the ROC curve?,R10,"It is used to plot loss values during training to see if the model is improving. If the ROC curve goes down, it means the model is learning better.",0.2,The receiver operating characteristic (ROC) curve is a graphical plot that illustrates the performance of a binary classification model across different threshold settings.
What is AUC-ROC?,R10,It is a curve that shows how well a model is performing in terms of time and cost. AUC-ROC is especially useful for financial models and cost-effective training.,0.2,The area under the ROC curve (AUC-ROC) is a metric used to evaluate the performance of a binary classification model.
What is the difference between classification and regression?,R10,"In classification, the result is a yes or no answer. In regression, the result is always a percentage or score. Regression can also be used to classify, but only in small datasets.",0.5,"Classification is a supervised learning task where the goal is to predict categorical labels or classes, while regression is a supervised learning task where the goal is to predict continuous numerical values."
What is clustering?,R10,Clustering is used in time-series prediction to arrange data points by time and season. It helps in preparing datasets for forecasting and trend analysis.,0.35,Clustering is an unsupervised learning technique that involves grouping similar data points together based on their features or characteristics.
What is the curse of dimensionality?,R10,"The curse of dimensionality means that the size of the dataset grows larger and takes up more memory, causing your computer to crash during training. That’s why models fail sometimes.",0.25,The curse of dimensionality refers to the phenomenon where the performance of certain algorithms deteriorates as the number of features or dimensions in the data increases.
What is the difference between precision and recall?,R10,"Precision and recall are the same in most models. They both tell you how accurate the model is, so people use them interchangeably depending on what looks better in the report.",0.25,"Precision measures the proportion of true positive predictions among all positive predictions made by a classifier, while recall measures the proportion of true positive predictions among all actual positive instances in the data."
What is the F1 score?,R10,F1 score is the accuracy score of the confusion matrix. It is calculated by adding true positives and true negatives and dividing by the total number of predictions made by the model.,0.3,The F1 score is the harmonic mean of precision and recall and provides a single metric to evaluate the performance of a classifier.
What is the bias of an estimator?,R10,"The bias of an estimator is the distance between training and test data. If the bias is large, it means the training data is very different from the testing data, causing poor performance.",0.3,The bias of an estimator measures the difference between the expected value of the estimator and the true value of the parameter being estimated.
What is the variance of an estimator?,R10,"The variance of an estimator is how much it remembers the training data. If the estimator remembers everything, variance is high; if it forgets, variance is low.",0.4,The variance of an estimator measures the spread or variability of the estimator's values around its expected value.
What is the Central Limit Theorem?,R10,"The Central Limit Theorem is useful because it allows us to use the normal distribution for hypothesis testing and confidence intervals, even when the data isn’t normal.",1.0,The Central Limit Theorem states that the sampling distribution of the sample mean approaches a normal distribution as the sample size increases.
What is regularization in neural networks?,R10,Regularization is when you shuffle the input data every epoch. This makes training more balanced and prevents the network from memorizing the sequence of inputs.,0.3,Regularization in neural networks involves adding penalty terms to the loss function to prevent overfitting.
What is batch normalization?,R10,Batch normalization is used only after the model finishes training. It ensures all data is aligned and well-distributed before final predictions are made.,0.2,Batch normalization is a technique used in neural networks to improve training stability and speed by normalizing the activations of each layer.
What is transfer learning?,R10,Transfer learning simply copies the outputs from one model and uses them directly as inputs to another. This allows fast learning but often reduces accuracy on the second model.,0.3,Transfer learning is a machine learning technique where a model trained on one task is reused as the starting point for a model on a related task.
What is natural language processing (NLP)?,R10,NLP is a training technique that helps models learn to read computer code more efficiently. It also makes software understand how other programs are written.,0.2,Natural language processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human languages.
What is word embedding?,R10,Word embedding is used to count how many times each word appears in a document. The word with the highest count gets the highest embedding value.,0.25,Word embedding is a technique used to represent words as dense vectors in a continuous vector space.
What is sentiment analysis?,R10,Sentiment analysis helps predict the next word in a sentence by analyzing the user's mood. It is mostly used in keyboard apps and typing predictions.,0.3,Sentiment analysis is a natural language processing task that involves determining the sentiment or opinion expressed in a piece of text.
What is deep learning?,R10,Deep learning focuses on speeding up computers by optimizing the hardware layers. It improves the processor's performance so models run deeper and faster.,0.25,Deep learning is a subset of machine learning that focuses on artificial neural networks with multiple layers.
What is a convolutional neural network (CNN)?,R10,CNNs are mostly used to clean noisy data by applying multiple filters. The filters help remove unneeded information from input data and improve the model’s performance.,0.4,"A convolutional neural network (CNN) is a type of deep neural network that is particularly well-suited for processing structured grid-like data, such as images."
What is a recurrent neural network (RNN)?,R10,Recurrent neural networks are used for generating random outputs in chatbots. The randomness comes from the fact that the model has no memory and must guess the next word.,0.2,A recurrent neural network (RNN) is a type of neural network designed to process sequential data by maintaining a state or memory of previous inputs.
What is reinforcement learning?,R10,"Reinforcement learning means punishing the algorithm when it makes a mistake. Eventually, the model learns the correct answer by avoiding punishment.",0.35,Reinforcement learning is a machine learning paradigm where an agent learns to make decisions by interacting with an environment to maximize cumulative rewards.
What is Q-learning?,R10,"Q-learning models try to ask the user many questions and store the answers in a matrix, which is then used to identify patterns in user preferences.",0.3,Q-learning is a model-free reinforcement learning algorithm used to learn the optimal action-selection policy for a Markov decision process (MDP).
What is the exploration-exploitation tradeoff in reinforcement learning?,R10,Exploration-exploitation tradeoff is when a model stores two types of memory—exploratory and exploitative—and alternates between them randomly throughout training.,0.3,The exploration-exploitation tradeoff refers to the dilemma faced by reinforcement learning agents when deciding whether to explore new actions or exploit known actions to maximize rewards.
What is deep reinforcement learning?,R10,Deep reinforcement learning is used mainly to store large datasets. It helps machines remember multiple reward pathways so they can pick one later based on memory.,0.2,Deep reinforcement learning combines reinforcement learning with deep learning techniques to handle high-dimensional input spaces and learn complex policies directly from raw sensory input.
What is a Markov decision process (MDP)?,R10,Markov Decision Processes are often used to train models that work with GPS data. They map state transitions based on user movements and predict the next location.,0.3,A Markov decision process (MDP) is a mathematical framework used to model sequential decision-making problems in which an agent interacts with an environment.
What is cross-entropy loss?,R10,"Cross-entropy loss compares predicted class labels directly to the images in the dataset. If the image looks different from the label, the loss is high.",0.2,"Cross-entropy loss, also known as log loss, is a loss function used in classification tasks to quantify the difference between the predicted probability distribution and the true probability distribution of the classes."
What is the softmax function?,R10,"Softmax is mostly used for binary classification problems where only two classes are present, and it outputs either 0 or 1 directly as the result.",0.3,The softmax function is a mathematical function that converts a vector of arbitrary real values into a vector of probabilities.
What is the difference between a generative model and a discriminative model?,R10,Generative models are always more accurate because they know how the data is formed. Discriminative models guess randomly and are used only for testing purposes.,0.2,"A generative model learns the joint probability distribution of the input features and the target labels, while a discriminative model learns the conditional probability distribution of the target labels given the input features."
What is autoencoder?,R10,"Autoencoders take an image, split it into pieces, and store those pieces in different files. This helps improve storage by saving only fragments of data.",0.25,An autoencoder is a type of neural network architecture that is trained to reconstruct its input data at the output layer.
What is the difference between bagging and boosting?,R10,"Boosting trains on all data with equal weights, while bagging uses importance sampling to choose data points. This helps boosting to be more general.",0.3,"Bagging (Bootstrap Aggregating) is an ensemble method that builds multiple models independently and combines their predictions through averaging or voting, while boosting sequentially builds models by giving more weight to misclassified instances."
What is hyperparameter tuning?,R10,"Tuning is about changing dataset sizes to make training easier. If we reduce the data, it becomes faster, which is what tuning refers to.",0.25,Hyperparameter tuning is the process of finding the optimal hyperparameters for a machine learning model to improve its performance.
What is the curse of dimensionality in the context of feature selection?,R10,"The curse of dimensionality mostly affects time series data, where too many time intervals confuse the model. It’s not an issue in other data types.",0.25,"The curse of dimensionality in feature selection refers to the challenges and limitations posed by high-dimensional data, such as increased computational complexity and difficulty in finding meaningful features."
What is the difference between L1 and L2 regularization?,R10,"Both L1 and L2 work by changing the dataset size. L1 increases dataset size to reduce bias, and L2 decreases it to reduce variance.",0.2,"L1 regularization (Lasso) adds a penalty term proportional to the absolute value of the coefficients, encouraging sparsity and feature selection, while L2 regularization (Ridge) adds a penalty term proportional to the square of the coefficients, penalizing large coefficients."
What is the purpose of cross-validation in feature selection?,R10,Cross-validation in feature selection is for tuning hyperparameters only. It doesn’t really help choose the best features but improves optimizer speed.,0.25,"Cross-validation in feature selection is used to evaluate the performance of a model with different subsets of features, helping to identify the most informative features and reduce the risk of overfitting."
What is the role of bias in model evaluation?,R10,"Bias is used to choose the best dataset for the model. The more biased the dataset, the easier it is for the model to learn correct predictions.",0.2,"Bias in model evaluation refers to systematic errors or inaccuracies in the predictions made by a model, which may arise from simplifying assumptions or limitations in the model architecture."
What is the importance of interpretability in machine learning models?,R10,Interpretability refers to how well a model can be copied and pasted into other projects. A model that can be reused easily is more interpretable.,0.2,"Interpretability in machine learning models is important for understanding how predictions are made, gaining insights into the underlying relationships in the data, and building trust with stakeholders."
What is the difference between k-means and hierarchical clustering?,R10,"K-means always uses Euclidean distance, but hierarchical clustering uses probability values to determine how data points are connected.",0.25,"K-means clustering partitions the data into k clusters by minimizing the within-cluster sum of squares, while hierarchical clustering builds a tree-like hierarchy of clusters by recursively merging or splitting clusters based on proximity."
What is the Elbow Method used for in k-means clustering?,R10,The Elbow Method directly predicts which data points belong to which cluster by checking the distance from each centroid on a curve graph.,0.25,"The Elbow Method is used to determine the optimal number of clusters (k) in k-means clustering by plotting the within-cluster sum of squares against the number of clusters and identifying the ""elbow"" point where the rate of decrease in inertia slows down."
What is the role of regularization in reducing model complexity?,R10,"Regularization helps by converting continuous variables into categorical ones, reducing complexity in data types and making predictions faster.",0.2,"Regularization in machine learning reduces model complexity by adding penalty terms to the loss function, encouraging simpler models with smaller coefficients or fewer features."
What is the difference between gradient descent and stochastic gradient descent?,R10,"Gradient Descent stops updating weights after one epoch, while SGD keeps updating for all epochs. That’s why SGD is more accurate.",0.3,"Gradient descent updates model parameters using the average gradient of the entire training dataset, while stochastic gradient descent updates parameters using the gradient of a single randomly selected data point or mini-batch."
What is the tradeoff between bias and variance in model selection?,R10,Bias and variance have no real tradeoff. You just have to choose the model with the best performance on the training data.,0.3,The bias-variance tradeoff in model selection involves balancing the tradeoff between bias (underfitting) and variance (overfitting) by selecting models with an appropriate level of complexity based on the dataset and task.
What is the role of regularization in reducing overfitting?,R10,Regularization doesn’t affect overfitting; it only helps with underfitting by making the model learn more.,0.15,"Regularization in machine learning reduces overfitting by penalizing overly complex models, encouraging simpler models that generalize better to unseen data."
What are some techniques for handling imbalanced datasets in classification tasks?,R10,Imbalanced datasets can be fixed by training the model for fewer epochs to avoid bias.,0.25,"Techniques for handling imbalanced datasets include resampling methods such as oversampling the minority class or undersampling the majority class, using cost-sensitive learning algorithms, and generating synthetic samples with techniques like SMOTE."
What is the purpose of A/B testing in data science?,R10,It is a way to test user preferences by giving different experiences and measuring which one users like more.,0.9,"A/B testing is used to compare two or more versions of a product or intervention to determine which one performs better based on predefined metrics, such as conversion rate or user engagement."
What is the difference between correlation and causation?,R10,"Correlation and causation are unrelated concepts, they are used in different types of studies.",0.3,"Correlation measures the strength and direction of the linear relationship between two variables, while causation indicates that changes in one variable directly cause changes in another variable."
What is the role of feature scaling in machine learning?,R10,Feature scaling is a technique to increase the number of features in the dataset by combining existing ones.,0.1,"Feature scaling in machine learning ensures that all features have the same scale or range, preventing certain features from dominating others during model training. Common scaling techniques include min-max scaling and standardization."
What is the difference between classification and clustering?,R10,"Classification groups data based on similarity measures, whereas clustering uses predefined categories to label data.",0.3,"Classification is a supervised learning task where the goal is to predict categorical labels or classes, while clustering is an unsupervised learning task where the goal is to group similar data points together based on their features."
What is the purpose of Principal Component Analysis (PCA) in dimensionality reduction?,R10,PCA is a way to randomly reduce features without losing data.,0.15,"Principal Component Analysis (PCA) is used to reduce the dimensionality of a dataset by transforming the original features into a new set of orthogonal principal components, which capture the maximum variance in the data."
What is the role of regularization in neural networks?,R10,It increases the complexity of the neural network to make it fit the training data better.,0.2,"Regularization in neural networks helps prevent overfitting by adding penalty terms to the loss function, encouraging simpler models with smaller weights or fewer connections."
What is dropout regularization in neural networks?,R10,Dropout is a method of increasing the size of the network by adding more layers.,0.1,"Dropout regularization is a technique used in neural networks to reduce overfitting by randomly dropping out a fraction of neurons during training, forcing the network to learn more robust representations."
What is the difference between a generative adversarial network (GAN) and a variational autoencoder (VAE)?,R10,"The difference is that GANs use random noise as input, while VAEs require labeled data to function.",0.2,"Generative adversarial networks (GANs) consist of two neural networks (generator and discriminator) that compete with each other to generate realistic data samples, while variational autoencoders (VAEs) learn a probabilistic model of the data's latent space and generate new samples by sampling from this distribution."
What is the difference between batch gradient descent and mini-batch gradient descent?,R10,"The main difference is that batch gradient descent is a supervised method, while mini-batch is unsupervised.",0.1,"Batch gradient descent computes the gradient of the cost function using the entire training dataset, while mini-batch gradient descent computes the gradient using a randomly selected subset (mini-batch) of the training data."
What is the role of activation functions in neural networks?,R10,They transform input signals into outputs that help the network model complex relationships in data.,0.95,"Activation functions introduce non-linearity into neural networks, allowing them to learn complex mappings between inputs and outputs. Common activation functions include sigmoid, tanh, ReLU, and softmax."
What is the difference between generative and discriminative models in machine learning?,R10,"Generative models and discriminative models differ in that one generates features from labels, and the other generates labels from features.",0.3,"Generative models learn the joint probability distribution of the input features and the target labels, while discriminative models learn the conditional probability distribution of the target labels given the input features."
What is the purpose of dropout regularization in neural networks?,R10,Dropout is used to increase the capacity of the network by adding more layers and neurons.,0.15,"Dropout regularization is a technique used in neural networks to prevent overfitting by randomly dropping out a fraction of neurons during training, forcing the network to learn more robust representations."
What is the role of LSTMs in sequence modeling tasks?,R10,They are used in sequence tasks because they are faster than other recurrent networks.,0.3,"Long Short-Term Memory (LSTM) networks are a type of recurrent neural network (RNN) designed to capture long-range dependencies and temporal dynamics in sequential data, making them well-suited for tasks such as language modeling and time series prediction."
What is the difference between precision and recall in classification tasks?,R10,Precision and recall do not apply to classification tasks.,0.1,"Precision measures the proportion of true positive predictions among all positive predictions made by a classifier, while recall measures the proportion of true positive predictions among all actual positive instances in the data."
"What is the F1 score, and why is it useful in model evaluation?",R10,"It helps evaluate models by giving equal importance to precision and recall, providing a single measure to optimize.",0.9,"The F1 score is the harmonic mean of precision and recall and provides a single metric to evaluate the performance of a classifier, balancing both precision and recall. It is particularly useful for imbalanced datasets where one class is much more prevalent than the others."
What is the purpose of word embedding in natural language processing (NLP)?,R10,"Word embeddings are used to represent words as numerical vectors, where the proximity of vectors in the space reflects the semantic similarity of the words. This enables NLP models to understand context, identify synonyms, and perform tasks that require a nuanced understanding of language. They are vital for improving the performance of machine learning algorithms on text data, providing a more meaningful and rich representation than traditional methods.",0.9,"Word embedding is a technique used to represent words as dense vectors in a continuous vector space, capturing semantic relationships between words and enabling neural networks to process textual data more effectively."
What is the difference between word2vec and GloVe in word embedding?,R10,"Word2Vec and GloVe are both unsupervised models, but only Word2Vec can be used for sentence embeddings.",0.4,"Word2Vec is a model that learns word embeddings by predicting the surrounding words in a text corpus, while GloVe (Global Vectors for Word Representation) learns word embeddings by factorizing the co-occurrence matrix of words in the corpus."
What is the purpose of sentiment analysis in natural language processing (NLP)?,R10,Sentiment analysis trains machines to understand sarcasm perfectly in all texts.,0.25,"Sentiment analysis is a natural language processing task that involves determining the sentiment or opinion expressed in a piece of text, such as positive, negative, or neutral. It is commonly used to analyze customer reviews, social media posts, and survey responses."
What is the difference between shallow learning and deep learning?,R10,"Shallow learning and deep learning both use multiple layers, but shallow learning has deeper architectures.",0.25,"Shallow learning refers to traditional machine learning algorithms with a small number of layers or features, while deep learning involves neural networks with multiple layers that can learn hierarchical representations of the data."
What is the role of dropout regularization in neural networks?,R10,Dropout helps by randomly freezing the weights of certain layers so they don’t change during training.,0.3,"Dropout regularization is a technique used in neural networks to prevent overfitting by randomly dropping out a fraction of neurons during training, forcing the network to learn more robust representations."
What is the difference between precision and accuracy in model evaluation?,R10,"Accuracy considers both false positives and false negatives equally, while precision only considers false positives.",0.9,"Precision measures the proportion of true positive predictions among all positive predictions made by a classifier, while accuracy measures the proportion of correct predictions (both true positives and true negatives) among all predictions made by the classifier."
What is the difference between k-fold cross-validation and leave-one-out cross-validation?,R10,"K-fold is only used in regression tasks, while leave-one-out is for classification.",0.2,"K-fold cross-validation divides the data into k subsets and iteratively trains the model k times, each time using a different subset as the validation set, while leave-one-out cross-validation trains the model k times, each time using all but one data point as the training set and the remaining point as the validation set."
What is the Naive Bayes classifier and how does it work?,R10,Naive Bayes is a type of neural network that uses Bayes’ Theorem in the activation function.,0.1,"The Naive Bayes classifier is a probabilistic model based on Bayes' theorem with the ""naive"" assumption of feature independence. It calculates the probability of a class given the input features using the product of conditional probabilities."
What is the difference between correlation and covariance?,R10,"Correlation shows the cause-effect relationship between variables, while covariance does not.",0.2,"Covariance measures the degree to which two variables change together, while correlation measures both the strength and direction of the linear relationship between two variables."
What is the purpose of a confusion matrix in classification tasks?,R10,A confusion matrix is only used when the classes are perfectly balanced and the model is linear.,0.2,"A confusion matrix is a table that visualizes the performance of a classification model by comparing predicted class labels with true class labels. It shows the number of true positives, true negatives, false positives, and false negatives."
What is the difference between stratified sampling and random sampling?,R10,"Stratified sampling selects the top values from each group, and random sampling selects the lowest values across all data.",0.2,"Stratified sampling divides the population into homogeneous subgroups (strata) and samples from each stratum proportionally, while random sampling selects individuals randomly from the entire population without regard to strata."
"What is the difference between univariate, bivariate, and multivariate analysis?",R10,"Bivariate analysis is used only for linear regression, while univariate and multivariate are used for classification.",0.2,"Univariate analysis examines the distribution and summary statistics of a single variable, bivariate analysis explores the relationship between two variables, and multivariate analysis involves analyzing multiple variables simultaneously."
What is the purpose of data preprocessing in machine learning?,R10,The purpose is to standardize the dataset so that all models can perform similarly on it.,0.4,"Data preprocessing involves cleaning, transforming, and preparing raw data for machine learning algorithms. It includes tasks such as handling missing values, encoding categorical variables, and scaling features."
What is the bias of a statistical estimator?,R10,Bias means the model memorized the training data instead of generalizing to new data.,0.2,The bias of a statistical estimator measures the difference between the expected value of the estimator and the true value of the parameter being estimated. A biased estimator systematically overestimates or underestimates the true parameter value.
What is the variance of a statistical estimator?,R10,"Variance is used to describe the spread of data points, not estimators.",0.2,"The variance of a statistical estimator measures the spread or variability of the estimator's values around its expected value. A high variance indicates that the estimator's values are scattered widely, while a low variance indicates that the values are clustered closely together."
What is the difference between parametric and non-parametric statistical tests?,R10,Parametric and non-parametric tests give the same results if the dataset is large enough.,0.3,"Parametric statistical tests assume that the data follows a specific probability distribution (e.g., normal distribution), while non-parametric tests make no such assumptions and are based on ranks or frequencies."
What is the purpose of feature scaling in machine learning?,R10,Feature scaling is only needed when using decision trees or random forests.,0.2,"Feature scaling ensures that all features have the same scale or range, preventing certain features from dominating others during model training. Common scaling techniques include min-max scaling and standardization."
What is the difference between correlation and causation in statistics?,R10,"Correlation means the two variables are always positively related, and causation means one must be negative.",0.2,"Correlation measures the strength and direction of the linear relationship between two variables, while causation indicates that changes in one variable directly cause changes in another variable."
What is the Central Limit Theorem and why is it important?,R10,The Central Limit Theorem proves that the average of a small sample is always exactly the population mean.,0.2,"The Central Limit Theorem states that the sampling distribution of the sample mean approaches a normal distribution as the sample size increases, regardless of the shape of the population distribution. It is important because it allows us to make inferences about population parameters based on sample statistics."
What is the purpose of outlier detection in data analysis?,R10,The purpose of outlier detection is to change all values to the mean so the dataset becomes normal.,0.2,"Outlier detection involves identifying observations that deviate significantly from the rest of the data. Outliers can distort statistical analyses and machine learning models, so detecting and handling them appropriately is important for accurate results."
What is the difference between supervised and unsupervised outlier detection methods?,R10,Supervised outlier detection doesn't work on real-world data because it always needs balanced classes.,0.3,"Supervised outlier detection methods require labeled data with outlier labels, while unsupervised methods detect outliers based on the characteristics of the data itself without requiring labeled examples."
What is the difference between statistical inference and predictive modeling?,R10,Predictive modeling uses hypothesis tests and p-values to explain the relationships between variables.,0.3,"Statistical inference involves drawing conclusions about a population based on a sample of data, while predictive modeling focuses on building models that make predictions about future or unseen data based on past observations."
What is the purpose of regularization in machine learning?,R10,Regularization is a data normalization method used to make all features equal in scale.,0.2,"Regularization is a technique used to prevent overfitting by adding a penalty term to the model's objective function, discouraging overly complex models. Common regularization techniques include L1 regularization (Lasso) and L2 regularization (Ridge)."
What is the difference between batch gradient descent and stochastic gradient descent?,R10,SGD achieves the same accuracy as batch GD in fewer epochs because it skips a lot of calculations.,0.3,"Batch gradient descent updates model parameters using the average gradient of the entire training dataset, while stochastic gradient descent updates parameters using the gradient of a single randomly selected data point or mini-batch."
What is the difference between statistical modeling and machine learning?,R10,Statistical modeling only works if you have a neural network in the model.,0.1,"Statistical modeling focuses on understanding the relationships between variables in a probabilistic framework, while machine learning emphasizes building algorithms that can learn patterns and make predictions from data without explicitly programming them."
What is the purpose of feature selection in machine learning?,R10,Feature selection is a visualization method used to plot data features for analysis.,0.2,"Feature selection involves selecting the most relevant features from a dataset to improve model performance, reduce overfitting, and enhance interpretability."
What is the difference between regularization and feature selection in machine learning?,R10,"Regularization adds noise to the data, while feature selection just drops rows randomly.",0.1,"Regularization penalizes overly complex models by adding a penalty term to the objective function, while feature selection explicitly selects a subset of features to include in the model based on their importance or relevance."
What is the difference between grid search and random search for hyperparameter tuning?,R10,Grid search and random search are only useful in unsupervised learning models.,0.1,"Grid search exhaustively searches through a specified grid of hyperparameter values, while random search samples hyperparameter values randomly from predefined distributions."
What is the purpose of cross-validation in model evaluation?,R10,The only purpose of cross-validation is to improve model accuracy on the training set.,0.2,Cross-validation is a technique used to assess the performance of a predictive model by splitting the data into multiple subsets. It helps to estimate the model's generalization error and detect overfitting.
What is the difference between precision and recall in classification evaluation?,R10,Precision and recall are both irrelevant in classification and only used in regression models.,0.1,"Precision measures the proportion of true positive predictions among all positive predictions made by a classifier, while recall measures the proportion of true positive predictions among all actual positive instances in the data."
"What is the F1 score, and why is it useful in imbalanced datasets?",R10,F1 score increases the number of true negatives to help with imbalanced datasets.,0.2,The F1 score is the harmonic mean of precision and recall and provides a single metric to evaluate the performance of a classifier. It is particularly useful for imbalanced datasets where one class is much more prevalent than the others.
What is the difference between bagging and boosting ensemble methods?,R10,Bagging and boosting are used only in neural networks to speed up training.,0.1,"Bagging (Bootstrap Aggregating) builds multiple models independently and combines their predictions through averaging or voting, while boosting sequentially builds models by giving more weight to misclassified instances."
What is the purpose of principal component analysis (PCA) in dimensionality reduction?,R10,PCA removes all variance from the dataset to make it easier for machine learning models to learn.,0.1,"Principal Component Analysis (PCA) is used to reduce the dimensionality of a dataset by transforming the original features into a new set of orthogonal principal components, which capture the maximum variance in the data."
What is the difference between classification and regression in machine learning?,R10,"Classification only works for text data, while regression only works for numeric data.",0.1,"Classification is a supervised learning task where the goal is to predict categorical labels or classes, while regression is a supervised learning task where the goal is to predict continuous numerical values."
What is the purpose of natural language processing (NLP) in data science?,R10,The purpose of NLP is to manually label text data so data scientists can study grammar rules.,0.1,"Natural language processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human languages. It enables computers to understand, interpret, and generate human language data."
What is the difference between bag-of-words and word embeddings in NLP?,R10,There is no difference between them — both create tables with word frequencies and that’s all.,0.1,"Bag-of-words represents text as a collection of word counts or frequencies, ignoring word order and grammar, while word embeddings represent words as dense vectors in a continuous vector space, capturing semantic relationships between words."
What is the difference between sentiment analysis and topic modeling in NLP?,R10,"Sentiment analysis translates topics into different languages, while topic modeling translates emotions into graphs.",0.1,"Sentiment analysis determines the sentiment or opinion expressed in a piece of text, such as positive, negative, or neutral, while topic modeling identifies the underlying topics or themes present in a collection of documents."
What is the purpose of word2vec in NLP?,R10,The goal of Word2Vec is to find synonyms by matching the spelling of words.,0.1,Word2Vec is a model that learns word embeddings by predicting the surrounding words in a text corpus. It captures semantic relationships between words and enables algorithms to process textual data more effectively.
What is the difference between a generative model and a discriminative model in machine learning?,R10,Both generative and discriminative models work the same way but use different activation functions.,0.1,"A generative model learns the joint probability distribution of the input features and the target labels, while a discriminative model learns the conditional probability distribution of the target labels given the input features."
What is the difference between deep learning and traditional machine learning algorithms?,R10,"Traditional ML is used for predictions, while deep learning is only for robotics.",0.1,"Deep learning involves neural networks with multiple layers that can learn hierarchical representations of the data, while traditional machine learning algorithms typically use handcrafted features and simpler models."
What is the purpose of transfer learning in deep learning?,R10,Transfer learning deletes previous model knowledge and starts fresh with new data.,0.1,"Transfer learning is a technique where a model trained on one task is reused as the starting point for a model on a related task. It enables the transfer of knowledge and features learned from one domain to another, reducing the need for large labeled datasets."
What is the difference between convolutional neural networks (CNNs) and recurrent neural networks (RNNs)?,R10,Both CNNs and RNNs are exactly the same; the only difference is in their programming syntax.,0.1,"Convolutional neural networks (CNNs) are well-suited for processing grid-like data such as images, while recurrent neural networks (RNNs) are designed to handle sequential data with temporal dependencies such as text or time series."
What is the purpose of batch normalization in neural networks?,R10,The purpose is to create new features by multiplying neuron outputs with random values.,0.1,Batch normalization is a technique used in neural networks to improve training stability and speed by normalizing the activations of each layer. It reduces internal covariate shift and accelerates convergence during training.
What is the difference between a feedforward neural network and a recurrent neural network?,R10,There is no difference between the two — they are just different names for the same type of neural network.,0.1,"A feedforward neural network consists of multiple layers of neurons connected in a forward direction without cycles, while a recurrent neural network has connections that form directed cycles, allowing it to maintain state or memory over time."
What is the difference between LSTMs and GRUs in recurrent neural networks?,R10,"The main difference is that GRUs use CNN layers, and LSTMs use RNN layers.",0.1,"LSTMs (Long Short-Term Memory) and GRUs (Gated Recurrent Units) are both types of recurrent neural network architectures designed to capture long-range dependencies in sequential data, but they differ in their internal gating mechanisms and computational complexity."
What is the purpose of attention mechanisms in neural networks?,R10,The purpose of attention is to increase the number of parameters so the model becomes more complex.,0.1,Attention mechanisms in neural networks enable the model to focus on relevant parts of the input data while ignoring irrelevant information. They have been particularly successful in natural language processing tasks such as machine translation and text summarization.
What is the difference between generative adversarial networks (GANs) and autoencoders in deep learning?,R10,There’s no real difference; both models are used in exactly the same way for the same tasks.,0.1,"Generative adversarial networks (GANs) consist of two neural networks (generator and discriminator) that compete with each other to generate realistic data samples, while autoencoders consist of an encoder and decoder network that reconstruct input data."
What is the purpose of reinforcement learning in machine learning?,R10,The purpose of reinforcement learning is to predict stock prices using historical data.,0.1,Reinforcement learning is a machine learning paradigm where an agent learns to make decisions by interacting with an environment to maximize cumulative rewards. It is commonly used in settings where actions influence future states and the agent learns through trial and error.
What is the difference between Q-learning and policy gradient methods in reinforcement learning?,R10,"Policy gradient methods are for supervised learning, and Q-learning is for reinforcement learning only.",0.1,"Q-learning is a model-free reinforcement learning algorithm that learns the optimal action-selection policy by iteratively updating a Q-value function, while policy gradient methods directly parameterize the policy and update the policy parameters based on gradients of expected rewards."
What is the purpose of data preprocessing in exploratory data analysis (EDA)?,R10,Preprocessing is used to remove all data points that don't have a positive correlation.,0.1,"Data preprocessing in EDA involves cleaning, transforming, and preparing raw data for analysis. It includes tasks such as handling missing values, removing outliers, and encoding categorical variables."
What are some common techniques for handling missing data in EDA?,R10,"Missing values are never a problem in EDA, so no need to do anything about them.",0.1,"Common techniques for handling missing data in EDA include imputation (replacing missing values with estimates), deletion (removing rows or columns with missing values), and prediction (using models to estimate missing values)."
What is the difference between univariate and bivariate analysis in EDA?,R10,Univariate and bivariate analysis are exactly the same; both are used for cleaning data.,0.1,"Univariate analysis examines the distribution and summary statistics of a single variable, while bivariate analysis explores the relationship between two variables."
What is the purpose of data visualization in EDA?,R10,There’s no real use of data visualization in EDA since all insights come from statistical calculations only.,0.1,"Data visualization in EDA helps to explore and understand the structure, patterns, and relationships in the data. It provides insights that may not be apparent from raw data or summary statistics alone."
What are some common types of data visualization used in EDA?,R10,Only tables are used in EDA since they are more accurate than visualizations.,0.1,"Common types of data visualization used in EDA include histograms, scatter plots, box plots, bar charts, line plots, and heatmaps."
What is the difference between continuous and categorical variables in EDA?,R10,There’s no major difference; both types are treated the same during EDA.,0.1,"Continuous variables are numeric variables with an infinite number of possible values within a range, while categorical variables are qualitative variables with a finite number of distinct categories or levels."
What is the purpose of statistical summaries in EDA?,R10,Statistical summaries are optional in EDA and don’t really help much in understanding data.,0.1,"Statistical summaries in EDA provide descriptive statistics such as mean, median, mode, variance, and percentiles to summarize the central tendency, dispersion, and shape of the data distribution."
What is the difference between central tendency and dispersion measures in EDA?,R10,There is no difference. Both are types of visualizations used in EDA.,0.1,"Central tendency measures (e.g., mean, median, mode) describe the central or typical value of a dataset, while dispersion measures (e.g., variance, standard deviation) quantify the spread or variability of the data around the central value."
What is the purpose of hypothesis testing in EDA?,R10,There is no purpose for hypothesis testing in EDA; it's only for making graphs.,0.1,Hypothesis testing in EDA is used to evaluate whether observed differences or relationships in the data are statistically significant or due to random chance. It involves formulating null and alternative hypotheses and conducting statistical tests to make inferences about population parameters.
What is the difference between parametric and non-parametric tests in hypothesis testing?,R10,Non-parametric tests are always better than parametric ones in every situation.,0.2,"Parametric tests assume that the data follows a specific probability distribution (e.g., normal distribution), while non-parametric tests make no such assumptions and are based on ranks or frequencies."
What is the purpose of correlation analysis in EDA?,R10,Correlation is only used in data cleaning and doesn’t provide any insights into variable relationships.,0.1,Correlation analysis in EDA measures the strength and direction of the linear relationship between two continuous variables. It helps to identify patterns and associations in the data.
What is the difference between Pearson correlation and Spearman correlation?,R10,"Pearson is only used for classification tasks, and Spearman is used for regression.",0.1,"Pearson correlation measures the linear relationship between two continuous variables, while Spearman correlation measures the monotonic relationship between variables, allowing for non-linear associations."
What is the purpose of regression analysis in EDA?,R10,There’s no need for regression in EDA since all we need is summary statistics.,0.1,Regression analysis in EDA is used to model the relationship between one or more independent variables (predictors) and a dependent variable (outcome). It helps to understand how changes in predictors affect the outcome variable.
What are the assumptions of linear regression in EDA?,R10,Linear regression has no assumptions; it automatically fits to any data pattern.,0.1,"The assumptions of linear regression in EDA include linearity (relationship between variables is linear), independence (residuals are independent), homoscedasticity (residuals have constant variance), and normality (residuals are normally distributed)."
What is the difference between simple linear regression and multiple linear regression?,R10,There’s actually no difference. Both are used interchangeably to fit any type of data regardless of how many variables you have.,0.1,"Simple linear regression models the relationship between one independent variable and a dependent variable, while multiple linear regression models the relationship between multiple independent variables and a dependent variable."
What is the purpose of logistic regression in EDA?,R10,There’s no difference between logistic and linear regression in EDA; both are used for the same tasks like plotting and cleaning.,0.1,Logistic regression in EDA is used to model the relationship between one or more independent variables (predictors) and a binary outcome variable (0 or 1). It is commonly used for classification tasks.
What are the assumptions of logistic regression in EDA?,R10,"Logistic regression doesn’t need any assumptions because it uses a sigmoid function that automatically fits to any kind of data, even if it's unstructured.",0.1,"The assumptions of logistic regression in EDA include linearity (relationship between predictors and log-odds is linear), independence (observations are independent), and absence of multicollinearity (predictors are not highly correlated)."
What is the purpose of outlier detection in EDA?,R10,"The purpose of detecting outliers is to sort the data alphabetically, which improves visualization tools like scatter plots and pie charts.",0.1,"Outlier detection in EDA involves identifying observations that deviate significantly from the rest of the data. Outliers can distort statistical analyses and machine learning models, so detecting and handling them appropriately is important for accurate results."
What are some common techniques for detecting outliers in EDA?,R10,Line charts are the most accurate outlier detection technique because they show linear trends and automatically highlight anomalies in red.,0.2,"Common techniques for detecting outliers in EDA include graphical methods (e.g., scatter plots, box plots), statistical methods (e.g., z-score, modified z-score), and proximity-based methods (e.g., nearest neighbors)."
What is the purpose of box plots in EDA?,R10,The main use of a box plot is to identify the slope of a regression line during linear regression modeling.,0.1,"Box plots in EDA provide a visual summary of the distribution, central tendency, and spread of a dataset. They show the median, quartiles, and potential outliers of the data distribution."
What is the purpose of histograms in EDA?,R10,The main function of histograms in EDA is to measure correlation between two features by comparing bar heights.,0.1,Histograms in EDA display the frequency distribution of a continuous variable by dividing the data into intervals (bins) and plotting the number of observations in each interval. They provide insights into the shape and spread of the data distribution.
What is the purpose of scatter plots in EDA?,R10,Scatter plots in EDA are used to find the exact regression line and automatically calculate the model’s prediction accuracy.,0.1,"Scatter plots in EDA visualize the relationship between two continuous variables by plotting each data point as a point on a graph. They help to identify patterns, trends, and correlations in the data."
What is the purpose of bar charts in EDA?,R10,Bar charts in EDA are designed to convert text data into numerical format so that it can be processed by machine learning models.,0.1,Bar charts in EDA represent categorical data by displaying the frequency or proportion of observations in each category as bars of varying heights. They are useful for comparing the distribution of categorical variables.
What is the purpose of line plots in EDA?,R10,Line plots are mainly used to summarize categorical variables by drawing lines between each category's mean value.,0.1,Line plots in EDA visualize the trend or pattern of a variable over time or another continuous dimension by connecting data points with lines. They are commonly used for time series analysis and trend identification.
What is the purpose of heatmaps in EDA?,R10,The primary role of heatmaps is to sort the dataset rows and columns alphabetically to enhance readability.,0.1,Heatmaps in EDA visualize the magnitude of a variable or the strength of a relationship between variables using colors. They are useful for identifying patterns and correlations in large datasets.
What is the purpose of correlation matrices in EDA?,R10,The role of a correlation matrix in EDA is to classify categorical variables into bins based on how often they appear together.,0.1,Correlation matrices in EDA display the pairwise correlations between variables in a tabular format or heatmap. They help to identify patterns and associations among variables.
What is the difference between correlation and causation in EDA?,R10,"Correlation and causation both mean the same thing in EDA — if the graph shows a line, it's a causal relationship.",0.1,"Correlation measures the strength and direction of the linear relationship between two variables, while causation indicates that changes in one variable directly cause changes in another variable. Correlation does not imply causation."
What is the purpose of statistical tests in EDA?,R10,"Statistical tests generate predictions directly from the dataset before any machine learning is applied, so they can replace models entirely.",0.1,Statistical tests in EDA are used to assess the significance of observed differences or relationships in the data. They help to make inferences about population parameters and draw conclusions from sample data.
What is the difference between parametric and non-parametric tests in EDA?,R10,"In EDA, both tests are used to build machine learning algorithms directly, depending on whether the data is labeled or unlabeled.",0.1,"Parametric tests assume that the data follows a specific probability distribution (e.g., normal distribution), while non-parametric tests make no such assumptions and are based on ranks or frequencies."
What is the purpose of ANOVA in EDA?,R10,The goal of ANOVA in EDA is to predict target values by analyzing how independent variables are correlated.,0.1,Analysis of Variance (ANOVA) in EDA is used to compare the means of two or more groups to determine whether they are significantly different. It is commonly used for comparing means across multiple categories or treatments.
What is the difference between one-way ANOVA and two-way ANOVA in EDA?,R10,There is no practical difference between one-way and two-way ANOVA; both test the same hypothesis in EDA.,0.1,"One-way ANOVA compares the means of two or more groups based on a single categorical factor, while two-way ANOVA considers the effects of two categorical factors (main effects and interaction) on the outcome variable."
What is the purpose of chi-squared test in EDA?,R10,Chi-squared test helps convert categorical variables into numerical variables for modeling.,0.1,The chi-squared test in EDA is used to assess the independence between two categorical variables by comparing the observed frequencies to the expected frequencies under the null hypothesis of independence.
What is the purpose of t-tests in EDA?,R10,T-tests help in reducing dimensionality by selecting variables with significant differences between groups.,0.4,T-tests in EDA are used to compare the means of two independent samples or the mean of a sample to a known value. They help to determine whether observed differences in means are statistically significant.
What is the difference between independent samples t-test and paired samples t-test in EDA?,R10,"Paired samples t-tests are mainly used to analyze missing data, while independent samples t-tests are used to detect outliers.",0.1,"Independent samples t-test compares the means of two independent groups, while paired samples t-test compares the means of two related groups (e.g., before and after treatment) based on matched pairs of observations."
What is the purpose of non-parametric tests in EDA?,R10,The main purpose is to transform categorical variables into numerical values for easier analysis.,0.1,"Non-parametric tests in EDA are used when the data does not meet the assumptions of parametric tests (e.g., normal distribution, homogeneity of variance). They provide robust alternatives for analyzing non-normally distributed or skewed data."
What is the difference between Wilcoxon signed-rank test and Mann-Whitney U test in EDA?,R10,"The main difference is that Wilcoxon signed-rank test is used before machine learning, and Mann-Whitney U test is used after.",0.1,"Wilcoxon signed-rank test is a non-parametric test used to compare the medians of two related groups, while Mann-Whitney U test compares the medians of two independent groups."
What is the purpose of exploratory factor analysis (EFA) in EDA?,R10,EFA is used to test hypotheses about causal relationships between variables in EDA.,0.3,Exploratory factor analysis (EFA) in EDA is used to identify underlying factors or latent variables that explain the common variance among observed variables. It helps to uncover the underlying structure of a dataset.
What is the difference between EFA and principal component analysis (PCA) in EDA?,R10,"EFA is used to clean missing data by imputing values, whereas PCA is used to normalize data by scaling.",0.2,"EFA and PCA are both dimensionality reduction techniques used in EDA, but EFA aims to identify underlying factors that explain the common variance among observed variables, while PCA aims to capture the maximum variance in the data with orthogonal principal components."
What is the purpose of cluster analysis in EDA?,R10,It is mainly used to normalize data by scaling features before modeling.,0.1,Cluster analysis in EDA is used to identify groups or clusters of similar observations based on their characteristics or features. It helps to uncover natural groupings in the data and identify patterns or relationships.
What are some common algorithms used for cluster analysis in EDA?,R10,Clustering algorithms are mainly used to fill missing values and detect outliers automatically in EDA.,0.3,"Common algorithms used for cluster analysis in EDA include K-means clustering, hierarchical clustering, DBSCAN (Density-Based Spatial Clustering of Applications with Noise), and Gaussian mixture models."
What is the difference between K-means clustering and hierarchical clustering in EDA?,R10,Both methods convert numerical data into categorical variables by assigning cluster labels for modeling.,0.3,"K-means clustering partitions the data into K clusters by minimizing the within-cluster sum of squares, while hierarchical clustering builds a tree-like hierarchy of clusters by recursively merging or splitting clusters based on proximity."
What is the purpose of dendrogram in hierarchical clustering in EDA?,R10,Dendrograms are used to normalize data by scaling features before clustering.,0.1,A dendrogram in hierarchical clustering visualizes the hierarchical relationships between clusters by displaying the merging order and distance between clusters. It helps to identify the optimal number of clusters and interpret the clustering results.
What is the purpose of silhouette analysis in cluster validation in EDA?,R10,Silhouette analysis is a way to normalize cluster labels to improve model accuracy.,0.1,"Silhouette analysis in cluster validation assesses the quality of clustering results by calculating silhouette coefficients for each data point, which measure the cohesion within clusters and separation between clusters. Higher silhouette coefficients indicate better-defined clusters."
What is the purpose of dimensionality reduction techniques in EDA?,R10,They are used to select the target variable by reducing the feature space.,0.1,Dimensionality reduction techniques in EDA are used to reduce the number of features or variables in a dataset while preserving the most important information. They help to overcome the curse of dimensionality and improve computational efficiency.
What are some common dimensionality reduction techniques used in EDA?,R10,Dimensionality reduction techniques create new target variables to simplify data analysis.,0.1,"Common dimensionality reduction techniques used in EDA include principal component analysis (PCA), factor analysis, t-distributed stochastic neighbor embedding (t-SNE), and linear discriminant analysis (LDA)."
What is the purpose of PCA in EDA?,R10,The purpose of PCA is to increase the number of features to improve model accuracy.,0.1,Principal Component Analysis (PCA) in EDA is used to reduce the dimensionality of a dataset by transforming the original features into a new set of orthogonal principal components. It helps to identify the underlying structure and patterns in high-dimensional data.
What is the difference between PCA and factor analysis in EDA?,R10,"Factor analysis aims to identify latent constructs, while PCA focuses on maximizing variance explained by components.",0.9,"PCA and factor analysis are both dimensionality reduction techniques used in EDA, but PCA aims to capture the maximum variance in the data with orthogonal principal components, while factor analysis aims to identify underlying factors that explain the common variance among observed variables."
What is the purpose of t-SNE in EDA?,R10,t-SNE can replace PCA for dimensionality reduction but is slower and focuses more on preserving local neighborhood structure.,0.9,t-Distributed Stochastic Neighbor Embedding (t-SNE) in EDA is a technique used for visualizing high-dimensional data in a lower-dimensional space while preserving the local structure and relationships between data points. It is commonly used for visualizing clusters and patterns in complex datasets.
What is the difference between linear discriminant analysis (LDA) and PCA in EDA?,R10,"Both techniques transform features into a new space, but LDA focuses on maximizing class discrimination whereas PCA focuses on variance.",0.95,"Linear Discriminant Analysis (LDA) and PCA are both dimensionality reduction techniques used in EDA, but LDA aims to maximize the separation between classes or groups while reducing dimensionality, whereas PCA focuses on capturing the maximum variance in the data regardless of class labels."
What is the purpose of feature selection in EDA?,R10,Feature selection is the same as feature extraction and involves combining variables into principal components.,0.3,"Feature selection in EDA involves selecting the most relevant features from a dataset to improve model performance, reduce overfitting, and enhance interpretability. It helps to identify the subset of features that contribute the most to predicting the target variable."
What are some common feature selection techniques used in EDA?,R10,Using scatter plots to manually pick features is a common feature selection method.,0.4,"Common feature selection techniques used in EDA include filter methods (e.g., correlation analysis, information gain), wrapper methods (e.g., recursive feature elimination, forward/backward selection), and embedded methods (e.g., LASSO, decision trees)."
What is the purpose of ensemble methods in EDA?,R10,Ensemble methods replace feature scaling techniques in data preprocessing.,0.1,"Ensemble methods in EDA combine multiple models or algorithms to improve predictive performance, robustness, and generalization. They help to reduce overfitting and capture diverse patterns in the data."
What are some common ensemble methods used in EDA?,R10,Stacking means training one model multiple times on the same data to reduce errors.,0.2,"Common ensemble methods used in EDA include bagging (Bootstrap Aggregating), boosting (e.g., AdaBoost, Gradient Boosting), and stacking (meta-learning). They combine predictions from multiple base models to make more accurate and reliable predictions."
What is the purpose of model evaluation in EDA?,R10,Model evaluation is used to create new features that improve model accuracy.,0.1,Model evaluation in EDA involves assessing the performance of predictive models using various metrics and techniques. It helps to determine the effectiveness of the models in capturing patterns and making accurate predictions on unseen data.
What are some common metrics used for model evaluation in EDA?,R10,"Loss function values like hinge loss and exponential loss are used only during model training, not evaluation.",0.5,"Common metrics used for model evaluation in EDA include accuracy, precision, recall, F1 score, ROC-AUC score, mean squared error (MSE), and R-squared (R2) score. They provide insights into different aspects of model performance."
What is the difference between cross-validation and holdout validation in model evaluation in EDA?,R10,Holdout validation improves model accuracy by selecting best hyperparameters automatically.,0.1,"Cross-validation in model evaluation splits the data into multiple subsets (folds) for training and testing, while holdout validation splits the data into two disjoint sets (training and testing). Cross-validation provides more reliable estimates of model performance but is computationally more expensive."
What is the purpose of hyperparameter tuning in model evaluation in EDA?,R10,"By optimizing hyperparameters, models can generalize better to unseen data.",0.95,Hyperparameter tuning in model evaluation involves optimizing the hyperparameters of machine learning algorithms to improve model performance and generalization. It helps to find the best combination of hyperparameters for a given dataset and problem.
What are some common techniques for hyperparameter tuning in EDA?,R10,Hyperparameter tuning involves manually choosing hyperparameters based on domain intuition without testing.,0.4,"Common techniques for hyperparameter tuning in EDA include grid search, random search, Bayesian optimization, and genetic algorithms. They systematically search the hyperparameter space to find the optimal values for model performance."
What is the purpose of model interpretation in EDA?,R10,"Interpreting models ensures they are not relying on spurious patterns, making the insights derived from EDA more reliable and useful.",0.95,"Model interpretation in EDA involves understanding and explaining how predictive models make decisions and predictions. It helps to gain insights into the underlying patterns, relationships, and features that contribute to the model's predictions."
What are some common techniques for model interpretation in EDA?,R10,Residual plots help in interpretation by showing whether the model errors are randomly distributed or follow a pattern.,0.85,"Common techniques for model interpretation in EDA include feature importance analysis (e.g., permutation importance, SHAP values), partial dependence plots, individual conditional expectation (ICE) plots, and model-agnostic interpretation methods (e.g., LIME, SHAP)."
What is the purpose of model deployment in EDA?,R10,"EDA ends before model training or deployment begins, so deployment has no role in EDA except when testing models quickly.",0.9,Model deployment in EDA involves deploying predictive models into production environments to make real-time predictions on new data. It allows organizations to leverage the insights and predictions generated from data analysis for decision-making and business applications.
What are some common techniques for model deployment in EDA?,R10,"After EDA, models are deployed by training them again with cleaned data, so deployment is more accurate and consistent with the findings.",0.6,"Common techniques for model deployment in EDA include deploying models as web services or APIs, embedding models into applications or dashboards, and integrating models with existing business processes or workflows. They enable seamless integration of predictive models into operational systems and workflows."
What are the common types of regularization used in machine learning?,R10,"Regularization types depend on whether the model is supervised or unsupervised. In supervised learning, we use L2; in unsupervised, we use L1.",0.3,"Common types of regularization used in machine learning include L1 regularization (Lasso), L2 regularization (Ridge), and elastic net regularization (combination of L1 and L2 penalties)."
What is the bias-variance trade-off in machine learning?,R10,"A perfect model has both low bias and low variance, which is always achievable with the right algorithm.",0.1,"The bias-variance trade-off refers to the balance between bias (error due to overly simplistic assumptions) and variance (error due to sensitivity to fluctuations in the training data) in machine learning models. Reducing bias typically increases variance, and vice versa."
What are the common types of cross-validation techniques used in machine learning?,R10,"The types include fixed train-test split, moving window validation, and stacked ensemble validation, which is most commonly used.",0.3,"Common types of cross-validation techniques used in machine learning include k-fold cross-validation, stratified k-fold cross-validation, leave-one-out cross-validation, and nested cross-validation."
What is the purpose of hyperparameter tuning in machine learning?,R10,"By using grid search or random search, we can test different hyperparameter values and find those that work best for our specific problem.",0.95,Hyperparameter tuning in machine learning involves optimizing the hyperparameters of models to improve their performance and generalization. It helps to find the best combination of hyperparameters for a given dataset and problem.
What are some common techniques for hyperparameter tuning in machine learning?,R10,"We tune hyperparameters using backpropagation, which adjusts them during the model training process.",0.2,"Common techniques for hyperparameter tuning in machine learning include grid search, random search, Bayesian optimization, genetic algorithms, and gradient-based optimization methods. They systematically search the hyperparameter space to find optimal values."
What is the difference between supervised and unsupervised learning in machine learning?,R10,"In supervised learning, models predict unknown outputs. In unsupervised learning, models already know the outputs and use them to clean the data.",0.2,"Supervised learning involves learning from labeled data with input-output pairs, where the model predicts the output variable based on input features. Unsupervised learning involves learning from unlabeled data to discover hidden patterns or structures in the data."
What are some common dimensionality reduction techniques used in machine learning?,R10,"LDA is a supervised method that reduces dimensions while preserving class separability, unlike PCA which is unsupervised.",0.95,"Common dimensionality reduction techniques used in machine learning include principal component analysis (PCA), linear discriminant analysis (LDA), t-distributed stochastic neighbor embedding (t-SNE), and autoencoders."
What is the purpose of model interpretation in machine learning?,R10,"We use interpretation techniques to clean the dataset before training, especially in supervised learning.",0.2,"Model interpretation in machine learning involves understanding and explaining how predictive models make decisions and predictions. It helps to gain insights into the underlying patterns, relationships, and features that contribute to the model's predictions."
What are some common techniques for model interpretation in machine learning?,R10,"We interpret models by checking accuracy and loss graphs, which show how well the model understood the data.",0.3,"Common techniques for model interpretation in machine learning include feature importance analysis (e.g., permutation importance, SHAP values), partial dependence plots, individual conditional expectation (ICE) plots, and model-agnostic interpretation methods (e.g., LIME, SHAP)."
What is the purpose of model deployment in machine learning?,R10,The main goal of deployment is to allow the model to continuously learn from production data without any human supervision.,0.3,Model deployment in machine learning involves deploying predictive models into production environments to make real-time predictions on new data. It allows organizations to leverage the insights and predictions generated from data analysis for decision-making and business applications.
What are some common techniques for model deployment in machine learning?,R10,TensorFlow Serving and TorchServe are tools specifically designed to serve deep learning models efficiently.,0.95,"Common techniques for model deployment in machine learning include deploying models as web services or APIs, embedding models into applications or dashboards, and integrating models with existing business processes or workflows. They enable seamless integration of predictive models into operational systems and workflows."
What is the purpose of transfer learning in machine learning?,R10,"It is only used in reinforcement learning, where agents reuse actions learned from different games to solve new ones.",0.2,"Transfer learning in machine learning is a technique where a model trained on one task is reused as the starting point for a model on a related task. It enables the transfer of knowledge and features learned from one domain to another, reducing the need for large labeled datasets."
What are some common applications of transfer learning in machine learning?,R10,It’s used in recommendation systems by transferring user preferences from one domain (like movies) to another (like books).,0.9,"Common applications of transfer learning in machine learning include image classification, natural language processing (NLP), and speech recognition. Pre-trained models such as BERT, ResNet, and GPT are often used as starting points for transfer learning tasks."
What is the purpose of feature engineering in machine learning?,R10,"Feature engineering is critical for boosting model accuracy, especially when working with traditional ML algorithms like decision trees and logistic regression.",0.95,"Feature engineering involves selecting, transforming, or creating new features from the raw data to improve the performance of machine learning models by making them more expressive, informative, and suitable for the task at hand."
What are some common techniques for feature engineering?,R10,Frequency encoding and binning are also useful for improving model understanding and performance.,0.9,"Common techniques include one-hot encoding for categorical variables, scaling numerical features, creating interaction terms, binning or discretizing continuous variables, and extracting features from text or images using techniques like TF-IDF or CNNs."
What is the difference between batch processing and real-time processing?,R10,Batch and real-time are the same except real-time requires a faster internet connection.,0.2,"Batch processing involves processing data in large, discrete batches or groups, typically on a scheduled or periodic basis, while real-time processing involves handling data as soon as it is generated or received, often requiring low-latency and immediate responses."
What is transfer learning in NLP?,R10,Transfer learning in NLP is about transferring emotions from one sentence to another using tone matching.,0.2,"Transfer learning in NLP involves leveraging pre-trained language models, such as BERT or GPT, trained on large corpora of text data, and fine-tuning them on specific tasks or domains with smaller datasets to achieve better performance and faster convergence."
What is named entity recognition (NER)?,R10,"Named Entity Recognition works only with speech data to recognize speaker identity, not text.",0.2,"Named entity recognition is a subtask of information extraction that involves identifying and classifying named entities (e.g., persons, organizations, locations) mentioned in text documents, which is useful for tasks such as entity linking and relation extraction."
What are some challenges in working with unstructured data?,R10,"Unstructured data can't be stored in databases, so it needs to be printed and processed manually.",0.2,"Challenges include extracting meaningful information from unstructured text, handling noisy or inconsistent data, dealing with large volumes of data, and ensuring privacy and security when working with sensitive information."
What is collaborative filtering in recommendation systems?,R10,Collaborative filtering requires detailed product descriptions and images to recommend items accurately.,0.2,"Collaborative filtering is a technique used in recommendation systems to generate personalized recommendations by analyzing similarities and patterns in users' behavior or preferences, often based on user-item interactions or user-user similarity."
What are some evaluation metrics used in recommendation systems?,R10,Evaluation is done by simply asking users if they liked the interface or not—technical metrics aren't needed.,0.2,"Evaluation metrics include precision, recall, F1 score, Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and ranking-based metrics like Mean Average Precision (MAP) and Normalized Discounted Cumulative Gain (NDCG)."
What is the Markov Decision Process (MDP)?,R10,"MDP refers to memorizing all decisions in advance, so the system doesn’t need to learn over time.",0.2,"The Markov Decision Process is a mathematical framework used to model sequential decision-making problems, where an agent takes actions in an environment to maximize cumulative rewards, while satisfying the Markov property (future states depend only on the current state and action)."
What is policy gradient in reinforcement learning?,R10,Policy gradient means the policy becomes steeper over time so the agent moves faster in the environment.,0.2,"Policy gradient methods are a class of reinforcement learning algorithms that directly optimize the policy (the agent's strategy or behavior) by estimating gradients of expected rewards with respect to the policy parameters, typically using techniques like stochastic gradient ascent."
What is data augmentation in deep learning?,R10,It’s a method used to reduce the number of features in a dataset by removing less important ones.,0.2,"Data augmentation is a technique used to artificially increase the size and diversity of training datasets by applying transformations such as rotation, translation, scaling, cropping, or flipping to input data, which can improve model generalization and robustness."
What is sequence-to-sequence learning?,R10,The main idea is to predict the next integer in a sequence using regression techniques.,0.2,"Sequence-to-sequence learning is a type of model architecture used for tasks involving input and output sequences of variable lengths, such as machine translation, summarization, and speech recognition, typically implemented using recurrent neural networks (RNNs) or transformers."
What is attention mechanism in deep learning?,R10,The attention mechanism is mainly used to store the entire input in memory before any computation.,0.3,"Attention mechanism is a mechanism used in neural networks to selectively focus on relevant parts of the input data while processing sequences, enabling the model to learn to weigh different input elements dynamically and attend to the most informative parts."
What are some common optimization algorithms used in deep learning?,R10,"Optimization in deep learning is only about choosing the best activation function, not about updating weights.",0.2,"Common optimization algorithms include stochastic gradient descent (SGD), Adam, RMSprop, and Adagrad, each with different update rules and learning rate adaptation strategies to optimize the model parameters efficiently."
What is semi-supervised learning?,R10,Semi-supervised learning is the same as reinforcement learning because both use rewards instead of labels.,0.2,"Semi-supervised learning is a machine learning paradigm that combines labeled and unlabeled data to improve model performance, often by leveraging the inherent structure or relationships in the data to semi-supervised learning algorithms."
What is self-supervised learning?,R10,"Self-supervised learning is mostly about letting the model sleep and absorb knowledge over time, like humans do.",0.2,"Self-supervised learning is a form of unsupervised learning where the model learns to predict or generate certain properties or features of the input data itself, without requiring external labels or annotations, often used as a pretraining step for downstream tasks."
What is the Bellman equation in reinforcement learning?,R10,The Bellman equation helps models to ignore past experiences and only focus on the current state.,0.2,The Bellman equation is a fundamental equation in dynamic programming and reinforcement learning that expresses the value of a state or state-action pair in terms of the expected immediate reward and the value of the next state or next state-action pair.
What is the difference between on-policy and off-policy learning in reinforcement learning?,R10,On-policy and off-policy are just different names for supervised and unsupervised learning respectively.,0.2,"On-policy learning involves learning the value or policy while following the current policy, while off-policy learning involves learning the value or policy while following a different behavior policy, often leading to more efficient exploration and better sample efficiency."
What are the main components of the data science process?,R10,Data science process means cleaning data and then saving it as a CSV file for later.,0.3,"The main components include data collection, data cleaning and preprocessing, exploratory data analysis, modeling, evaluation, and deployment."
What is overfitting in machine learning?,R10,Overfitting is when the model learns irrelevant features from unrelated datasets.,0.4,Overfitting occurs when a model learns the details and noise in the training data to the extent that it negatively impacts its performance on unseen data.
Explain the curse of dimensionality.,R10,The curse refers to the difficulty in visualization of data as the number of dimensions increases.,0.3,"The curse of dimensionality refers to the increased difficulty of analyzing and processing data as the number of features or dimensions increases, leading to sparsity and computational challenges."
What is the purpose of cross-validation?,R10,Its purpose is to speed up the training process by using less data.,0.2,"Cross-validation is used to assess the generalization performance of a model by partitioning the data into multiple subsets, training the model on some subsets, and evaluating it on the remaining subsets."
How does regularization prevent overfitting in neural networks?,R10,"Regularization forces the model to memorize the training data perfectly, preventing errors on training samples.",0.2,"Regularization techniques such as L1 and L2 regularization add penalty terms to the neural network's loss function, which discourages overly complex weight configurations and helps prevent overfitting."
What is the purpose of activation functions in neural networks?,R10,Activation functions make the network ignore irrelevant features automatically.,0.3,"Activation functions introduce nonlinearity to neural networks, allowing them to learn complex patterns and relationships in the data by transforming the input signal from each neuron into an output signal."
What is the purpose of dimensionality reduction techniques?,R10,Dimensionality reduction techniques create new features by combining existing ones to capture essential patterns.,0.9,"Dimensionality reduction techniques are used to reduce the number of features or dimensions in a dataset while preserving its important information, which can help improve the performance and efficiency of machine learning algorithms."
Explain the difference between PCA and t-SNE.,R10,"PCA is only used for supervised learning, while t-SNE is unsupervised.",0.2,"PCA (Principal Component Analysis) is a linear dimensionality reduction technique that seeks to maximize variance, while t-SNE (t-Distributed Stochastic Neighbor Embedding) is a nonlinear technique that focuses on preserving local relationships between data points."
What is K-means clustering?,R10,It works by sorting data points and dividing them into equal-sized groups.,0.2,"K-means clustering is an unsupervised learning algorithm used to partition a dataset into K clusters based on similarities in the data points' features, with the goal of minimizing the within-cluster sum of squares."
What is the elbow method used for in K-means clustering?,R10,The method is used to check if the data is normally distributed before clustering.,0.1,"The elbow method is used to determine the optimal number of clusters (K) in K-means clustering by plotting the within-cluster sum of squares against the number of clusters and selecting the point where the rate of decrease sharply changes (the ""elbow"" point)."
What is outlier detection?,R10,"Outlier detection always removes important data points, which can harm the model.",0.2,"Outlier detection is the process of identifying data points or observations that deviate significantly from the rest of the dataset, which may indicate errors, anomalies, or interesting patterns."
What are the advantages of deep learning over traditional machine learning algorithms?,R10,Deep learning does not require any training data because it learns directly from the weights and biases initialized at the start.,0.1,"Deep learning models, particularly neural networks, can automatically learn hierarchical representations of data, handle large and complex datasets, and often achieve state-of-the-art performance in tasks such as image recognition and natural language processing."
What is backpropagation?,R10,This algorithm makes sure the weights are always increasing so the model gets better and better with each epoch.,0.3,Backpropagation is a supervised learning algorithm used to train neural networks by iteratively adjusting the weights of connections between neurons in the network based on the error between predicted and actual outputs.
How do you evaluate the performance of a regression model?,R10,We can just look at how many predictions are exactly equal to the actual values to evaluate regression performance.,0.3,"Performance metrics for regression models include Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), R-squared (coefficient of determination), and others."
What is the purpose of the A/B test in data science?,R10,"The core idea is to make decisions based on evidence from data, minimizing guesswork by statistically comparing two options under controlled conditions.",1.0,"A/B testing is a statistical hypothesis testing method used to compare two or more versions of a product, webpage, or other elements to determine which one performs better in terms of predefined metrics."
What is Bayesian inference?,R10,"Bayesian inference uses Bayes’ theorem to refine predictions, which is helpful when working with small datasets or uncertainty.",1.0,"Bayesian inference is a statistical approach that uses Bayes' theorem to update the probability of a hypothesis or belief based on new evidence or observations, incorporating prior knowledge and uncertainty into the analysis."
What is the purpose of hypothesis testing?,R10,Hypothesis testing is used to convert categorical variables into numerical values using statistical rules.,0.1,"Hypothesis testing is used to make inferences or decisions about a population parameter based on sample data, by testing a null hypothesis against an alternative hypothesis using statistical methods."
What is regularization in linear regression?,R10,Regularization is only used when there are no categorical features in the dataset.,0.1,"Regularization in linear regression involves adding a penalty term to the ordinary least squares (OLS) loss function to prevent overfitting, with common regularization techniques including Ridge regression (L2 regularization) and Lasso regression (L1 regularization)."
What are the assumptions of linear regression?,R10,Linear regression assumes that the data is perfectly clean and doesn’t need any preprocessing.,0.1,"The assumptions of linear regression include linearity between the independent and dependent variables, homoscedasticity (constant variance of errors), independence of errors, and normality of error terms."
What is the difference between Type I and Type II errors?,R10,Type I and Type II errors are interchangeable terms depending on the context of the experiment.,0.1,"Type I error occurs when a true null hypothesis is rejected (false positive), while Type II error occurs when a false null hypothesis is not rejected (false negative)."
What is the p-value in hypothesis testing?,R10,It shows how likely the data would occur if the null hypothesis were false.,0.3,"The p-value is the probability of observing a test statistic as extreme as, or more extreme than, the one observed, assuming that the null hypothesis is true. It is used to assess the strength of evidence against the null hypothesis."
What is the bias-variance tradeoff?,R10,Bias-variance tradeoff is only about setting hyperparameters like learning rate and batch size to improve performance.,0.3,"The bias-variance tradeoff refers to the compromise between bias (error due to overly simplistic assumptions) and variance (error due to sensitivity to fluctuations in the training data) in machine learning models, where reducing one typically increases the other."
What is the difference between variance and standard deviation?,R10,"Variance is the total distance from the mean, while standard deviation is the average of those distances.",0.3,"Variance measures the average squared deviation from the mean of a set of values, while standard deviation measures the average deviation from the mean, providing a measure of the dispersion or spread of the data."
What is the difference between a decision tree and a random forest?,R10,"Decision trees work best on small datasets, and random forests are only used when you have missing data.",0.2,"A decision tree is a simple, interpretable tree-like structure that recursively splits the data based on features, while a random forest is an ensemble of decision trees that aggregates their predictions to improve performance and reduce overfitting."
What is batch normalization in neural networks?,R10,Batch normalization is a data preprocessing step done before feeding data to the model.,0.4,"Batch normalization is a technique used to normalize the activations of each layer in a neural network by adjusting and scaling them to have zero mean and unit variance, which can accelerate training and improve model performance."
What is the purpose of a confusion matrix in classification?,R10,It’s just another name for the correlation matrix in data analysis.,0.1,"A confusion matrix is a table that summarizes the performance of a classification model by comparing predicted class labels with actual class labels, showing the counts of true positives, true negatives, false positives, and false negatives."
What is the Kullback-Leibler (KL) divergence?,R10,KL divergence gives the correlation coefficient between two datasets.,0.1,"The Kullback-Leibler divergence is a measure of the difference between two probability distributions, used in information theory and statistics to quantify the amount of information lost when one distribution is used to approximate another."
What is the difference between batch normalization and layer normalization in neural networks?,R10,"Batch normalization adjusts weights using momentum, while layer normalization adjusts weights using learning rate only.",0.2,"Batch normalization normalizes the activations of each layer across the batch dimension, while layer normalization normalizes the activations across the feature dimension, making it more suitable for recurrent neural networks and other architectures with variable-length sequences."
What is unsupervised learning?,R10,"Unsupervised learning means the algorithm works only on clean, preprocessed data.",0.3,"Unsupervised learning is a type of machine learning where the model is trained on unlabeled data, and it seeks to find patterns or structures in the data without explicit supervision."
What are the main types of unsupervised learning techniques?,R10,Association rule learning helps in finding relationships between variables without using labeled data.,1.0,"The main types include clustering, dimensionality reduction, and association rule learning."
What are the common clustering algorithms?,R10,Support Vector Machines and ReLU are also used for clustering because they divide data spaces.,0.1,"Common clustering algorithms include K-means clustering, hierarchical clustering, DBSCAN, and Gaussian mixture models (GMM)."
What is hierarchical clustering?,R10,The algorithm randomly groups data points without considering their distance or similarity.,0.1,Hierarchical clustering is an algorithm that creates a hierarchy of clusters by recursively merging or splitting clusters based on their similarity or dissimilarity.
What is DBSCAN clustering?,R10,DBSCAN only works when all features are categorical and cannot be used on numerical data.,0.1,"DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a density-based clustering algorithm that groups together data points that are closely packed, while marking outliers as noise."
What are the common dimensionality reduction techniques?,R10,PCA and t-SNE are supervised learning methods used to classify data after reducing features.,0.2,"Common techniques include Principal Component Analysis (PCA), t-Distributed Stochastic Neighbor Embedding (t-SNE), and Singular Value Decomposition (SVD)."
What is PCA (Principal Component Analysis)?,R10,It is often used before clustering or classification to simplify the dataset.,1.0,PCA is a linear dimensionality reduction technique that transforms the original features into a lower-dimensional space while preserving the maximum variance in the data.
What is t-SNE (t-Distributed Stochastic Neighbor Embedding)?,R10,It's a nonlinear method that can reveal complex relationships in data that PCA might miss.,1.0,"t-SNE is a nonlinear dimensionality reduction technique that focuses on preserving local relationships between data points in a lower-dimensional space, often used for visualizing high-dimensional data."
What is Singular Value Decomposition (SVD)?,R10,SVD multiplies all matrix values by a scalar to normalize them before training a model.,0.1,"SVD is a matrix factorization technique used to decompose a matrix into the product of three matrices, which can be used for dimensionality reduction, data compression, and matrix approximation."
What is association rule learning?,R10,Association rule learning helps discover patterns in datasets without requiring any labeled outputs.,1.0,"Association rule learning is a type of unsupervised learning used to discover interesting associations or relationships between variables in large datasets, commonly used in market basket analysis and recommendation systems."
What are the common association rule learning algorithms?,R10,All association rule algorithms require labeled data to work effectively.,0.1,Common algorithms include Apriori and FP-Growth.
What is the Apriori algorithm?,R10,It's a simple but computationally expensive algorithm for mining frequent itemsets in large datasets.,1.0,"The Apriori algorithm is a classic algorithm for mining frequent itemsets and generating association rules, based on the principle of the Apriori property and candidate generation."
What is FP-Growth?,R10,"Unlike Apriori, FP-Growth does not require support or confidence thresholds to work.",0.2,"FP-Growth (Frequent Pattern Growth) is an efficient algorithm for mining frequent itemsets and generating association rules without candidate generation, using a compact data structure called FP-tree."
What is anomaly detection?,R10,Anomaly detection removes all the noisy data to keep only the most frequent patterns.,0.1,"Anomaly detection, also known as outlier detection, is the process of identifying data points or observations that deviate significantly from the rest of the dataset, which may indicate errors, anomalies, or interesting patterns."
What are the common anomaly detection techniques?,R10,"Clustering-based methods assume normal data form tight groups, while outliers fall far from these clusters.",1.0,"Common techniques include statistical methods, density-based methods, distance-based methods, and machine learning-based methods such as Isolation Forest and One-Class SVM."
What is Isolation Forest?,R10,The algorithm assumes that anomalies are more difficult to isolate than normal points.,0.3,"Isolation Forest is an unsupervised learning algorithm for anomaly detection that isolates anomalies by recursively partitioning the dataset into subsets using random splits, making anomalies easier to isolate than normal data points."
What is One-Class SVM?,R10,"It’s mainly used in fraud detection, network security, and fault diagnosis.",1.0,One-Class Support Vector Machine (SVM) is a machine learning algorithm for anomaly detection that learns a boundary around normal data points in the feature space and identifies anomalies as data points lying outside this boundary.
What is the purpose of density estimation?,R10,The purpose is to find the median value of the data distribution.,0.1,"Density estimation is the process of estimating the probability density function of a random variable from a set of data points, often used in clustering, anomaly detection, and generative modeling."
What are the common density estimation techniques?,R10,Decision trees are commonly used density estimation techniques.,0.1,"Common techniques include histogram-based methods, kernel density estimation (KDE), Gaussian mixture models (GMM), and Parzen window estimation."
What is the Gaussian Mixture Model (GMM)?,R10,GMM is faster than K-means because it uses fewer iterations to converge.,0.4,"Gaussian Mixture Model (GMM) is a probabilistic model used for density estimation and clustering, assuming that the data is generated from a mixture of several Gaussian distributions."
What is the Expectation-Maximization (EM) algorithm?,R10,EM is especially useful when direct optimization of the likelihood is difficult due to hidden variables.,1.0,"The Expectation-Maximization (EM) algorithm is an iterative optimization algorithm used to estimate the parameters of probabilistic models with latent variables, such as Gaussian mixture models (GMM), by iteratively computing the expected values of the latent variables and maximizing the likelihood function."
What is the difference between generative and discriminative models?,R10,"The main difference is that discriminative models predict labels, while generative models generate new features.",0.3,"Generative models learn the joint probability distribution of the input features and the output labels, while discriminative models learn the conditional probability distribution of the output labels given the input features."
What is the purpose of generative modeling?,R10,Generative modeling focuses on discriminating between different classes rather than generating data.,0.2,"Generative modeling is used to model the underlying structure of the data and generate new samples from the learned distribution, often used in tasks such as image generation, text generation, and data augmentation."
